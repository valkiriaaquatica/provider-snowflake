name: snowflakedb/snowflake
resources:
    snowflake_account:
        subCategory: Stable
        description: The account resource allows you to create and manage Snowflake accounts. For more information, check account documentation https://docs.snowflake.com/en/user-guide/organizations-manage-accounts.
        name: snowflake_account
        title: snowflake_account Resource - terraform-provider-snowflake
        examples:
            - name: minimal
              manifest: |-
                {
                  "admin_name": "${var.admin_name}",
                  "admin_password": "${var.admin_password}",
                  "edition": "STANDARD",
                  "email": "${var.email}",
                  "grace_period_in_days": 3,
                  "name": "ACCOUNT_NAME"
                }
              references:
                admin_name: var.admin_name
                admin_password: var.admin_password
                email: var.email
            - name: complete
              manifest: |-
                {
                  "admin_name": "${var.admin_name}",
                  "admin_rsa_public_key": "\u003cpublic_key\u003e",
                  "admin_user_type": "SERVICE",
                  "comment": "some comment",
                  "edition": "STANDARD",
                  "email": "${var.email}",
                  "grace_period_in_days": 3,
                  "is_org_admin": "true",
                  "name": "ACCOUNT_NAME",
                  "region": "AWS_US_WEST_2",
                  "region_group": "PUBLIC"
                }
              references:
                admin_name: var.admin_name
                email: var.email
            - name: complete
              manifest: |-
                {
                  "admin_name": "${var.admin_name}",
                  "admin_password": "${var.admin_password}",
                  "admin_user_type": "PERSON",
                  "comment": "some comment",
                  "edition": "STANDARD",
                  "email": "${var.email}",
                  "first_name": "${var.first_name}",
                  "grace_period_in_days": 3,
                  "is_org_admin": "true",
                  "last_name": "${var.last_name}",
                  "must_change_password": "false",
                  "name": "ACCOUNT_NAME",
                  "region": "AWS_US_WEST_2",
                  "region_group": "PUBLIC"
                }
              references:
                admin_name: var.admin_name
                admin_password: var.admin_password
                email: var.email
                first_name: var.first_name
                last_name: var.last_name
        argumentDocs:
            account_locator: (String)
            account_locator_url: (String)
            account_name: (String)
            account_old_url_last_used: (String)
            account_old_url_saved_on: (String)
            account_url: (String)
            admin_name: (String, Sensitive) Login name of the initial administrative user of the account. A new user is created in the new account with this name and password and granted the ACCOUNTADMIN role in the account. A login name can be any string consisting of letters, numbers, and underscores. Login names are always case-insensitive. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            admin_password: (String, Sensitive) Password for the initial administrative user of the account. Either admin_password or admin_rsa_public_key has to be specified. This field cannot be used whenever admin_user_type is set to SERVICE. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            admin_rsa_public_key: (String) Assigns a public key to the initial administrative user of the account. Either admin_password or admin_rsa_public_key has to be specified. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            admin_user_type: '(String) Used for setting the type of the first user that is assigned the ACCOUNTADMIN role during account creation. Valid options are: PERSON | SERVICE | LEGACY_SERVICE External changes for this field won''t be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".'
            comment: (String) Specifies a comment for the account.
            consumption_billing_entity: (String) Determines which billing entity is responsible for the account's consumption-based billing.
            consumption_billing_entity_name: (String)
            create: (String)
            created_on: (String)
            delete: (String)
            dropped_on: (String)
            edition: '(String) Snowflake Edition of the account. See more about Snowflake Editions in the official documentation. Valid options are: STANDARD | ENTERPRISE | BUSINESS_CRITICAL'
            email: (String, Sensitive) Email address of the initial administrative user of the account. This email address is used to send any notifications about the account. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            first_name: (String, Sensitive) First name of the initial administrative user of the account. This field cannot be used whenever admin_user_type is set to SERVICE. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            grace_period_in_days: (Number) Specifies the number of days during which the account can be restored (“undropped”). The minimum is 3 days and the maximum is 90 days.
            id: (String) The ID of this resource.
            is_events_account: (Boolean)
            is_org_admin: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Sets an account property that determines whether the ORGADMIN role is enabled in the account. Only an organization administrator (i.e. user with the ORGADMIN role) can set the property.'
            is_organization_account: (Boolean)
            last_name: (String, Sensitive) Last name of the initial administrative user of the account. This field cannot be used whenever admin_user_type is set to SERVICE. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            managed_accounts: (Number)
            marketplace_consumer_billing_entity_name: (String)
            marketplace_provider_billing_entity_name: (String)
            moved_on: (String)
            moved_to_organization: (String)
            must_change_password: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies whether the new user created to administer the account is forced to change their password upon first login into the account. This field cannot be used whenever admin_user_type is set to SERVICE. External changes for this field won''t be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".'
            name: (String) Specifies the identifier (i.e. name) for the account. It must be unique within an organization, regardless of which Snowflake Region the account is in and must start with an alphabetic character and cannot contain spaces or special characters except for underscores (_). Note that if the account name includes underscores, features that do not accept account names with underscores (e.g. Okta SSO or SCIM) can reference a version of the account name that substitutes hyphens (-) for the underscores.
            old_account_url: (String)
            organization_name: (String)
            organization_old_url: (String)
            organization_old_url_last_used: (String)
            organization_old_url_saved_on: (String)
            organization_url_expiration_on: (String)
            read: (String)
            region: (String) Snowflake Region ID of the region where the account is created. If no value is provided, Snowflake creates the account in the same Snowflake Region as the current account (i.e. the account in which the CREATE ACCOUNT statement is executed.)
            region_group: (String) ID of the region group where the account is created. To retrieve the region group ID for existing accounts in your organization, execute the SHOW REGIONS command. For information about when you might need to specify region group, see Region groups.
            restored_on: (String)
            scheduled_deletion_time: (String)
            show_output: (List of Object) Outputs the result of SHOW ACCOUNTS for the given account. (see below for nested schema)
            snowflake_region: (String)
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
        importStatements:
            - terraform import snowflake_account.example '"<organization_name>"."<account_name>"'
    snowflake_account_authentication_policy_attachment:
        subCategory: Preview
        description: Specifies the authentication policy to use for the current account. To set the authentication policy of a different account, use a provider alias.
        name: snowflake_account_authentication_policy_attachment
        title: snowflake_account_authentication_policy_attachment Resource - terraform-provider-snowflake
        examples:
            - name: attachment
              manifest: |-
                {
                  "authentication_policy": "${snowflake_authentication_policy.default.fully_qualified_name}"
                }
              references:
                authentication_policy: snowflake_authentication_policy.default.fully_qualified_name
              dependencies:
                snowflake_authentication_policy.default: |-
                    {
                      "database": "prod",
                      "name": "default_policy",
                      "schema": "security"
                    }
        argumentDocs:
            authentication_policy: (String) Qualified name ("db"."schema"."policy_name") of the authentication policy to apply to the current account.
            create: (String)
            delete: (String)
            id: (String) The ID of this resource.
            read: (String)
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
        importStatements: []
    snowflake_account_parameter:
        subCategory: Stable
        description: Resource used to manage current account parameters. For more information, check parameters documentation https://docs.snowflake.com/en/sql-reference/parameters.
        name: snowflake_account_parameter
        title: snowflake_account_parameter Resource - terraform-provider-snowflake
        examples:
            - name: p
              manifest: |-
                {
                  "key": "ALLOW_ID_TOKEN",
                  "value": "true"
                }
            - name: p2
              manifest: |-
                {
                  "key": "CLIENT_ENCRYPTION_KEY_SIZE",
                  "value": "256"
                }
        argumentDocs:
            create: (String)
            delete: (String)
            id: (String) The ID of this resource.
            key: '(String) Name of account parameter. Valid values are (case-insensitive): ALLOW_CLIENT_MFA_CACHING | ALLOW_ID_TOKEN | CLIENT_ENCRYPTION_KEY_SIZE | CORTEX_ENABLED_CROSS_REGION | DISABLE_USER_PRIVILEGE_GRANTS | ENABLE_IDENTIFIER_FIRST_LOGIN | ENABLE_INTERNAL_STAGES_PRIVATELINK | ENABLE_TRI_SECRET_AND_REKEY_OPT_OUT_FOR_IMAGE_REPOSITORY | ENABLE_TRI_SECRET_AND_REKEY_OPT_OUT_FOR_SPCS_BLOCK_STORAGE | ENABLE_UNHANDLED_EXCEPTIONS_REPORTING | ENFORCE_NETWORK_RULES_FOR_INTERNAL_STAGES | EVENT_TABLE | EXTERNAL_OAUTH_ADD_PRIVILEGED_ROLES_TO_BLOCKED_LIST | INITIAL_REPLICATION_SIZE_LIMIT_IN_TB | MIN_DATA_RETENTION_TIME_IN_DAYS | NETWORK_POLICY | OAUTH_ADD_PRIVILEGED_ROLES_TO_BLOCKED_LIST | PERIODIC_DATA_REKEYING | PREVENT_LOAD_FROM_INLINE_URL | PREVENT_UNLOAD_TO_INLINE_URL | REQUIRE_STORAGE_INTEGRATION_FOR_STAGE_CREATION | REQUIRE_STORAGE_INTEGRATION_FOR_STAGE_OPERATION | SSO_LOGIN_PAGE | ABORT_DETACHED_QUERY | ACTIVE_PYTHON_PROFILER | AUTOCOMMIT | BINARY_INPUT_FORMAT | BINARY_OUTPUT_FORMAT | CLIENT_ENABLE_LOG_INFO_STATEMENT_PARAMETERS | CLIENT_MEMORY_LIMIT | CLIENT_METADATA_REQUEST_USE_CONNECTION_CTX | CLIENT_METADATA_USE_SESSION_DATABASE | CLIENT_PREFETCH_THREADS | CLIENT_RESULT_CHUNK_SIZE | CLIENT_SESSION_KEEP_ALIVE | CLIENT_SESSION_KEEP_ALIVE_HEARTBEAT_FREQUENCY | CLIENT_TIMESTAMP_TYPE_MAPPING | ENABLE_UNLOAD_PHYSICAL_TYPE_OPTIMIZATION | CLIENT_RESULT_COLUMN_CASE_INSENSITIVE | CSV_TIMESTAMP_FORMAT | DATE_INPUT_FORMAT | DATE_OUTPUT_FORMAT | ERROR_ON_NONDETERMINISTIC_MERGE | ERROR_ON_NONDETERMINISTIC_UPDATE | GEOGRAPHY_OUTPUT_FORMAT | GEOMETRY_OUTPUT_FORMAT | HYBRID_TABLE_LOCK_TIMEOUT | JDBC_TREAT_DECIMAL_AS_INT | JDBC_TREAT_TIMESTAMP_NTZ_AS_UTC | JDBC_USE_SESSION_TIMEZONE | JSON_INDENT | JS_TREAT_INTEGER_AS_BIGINT | LOCK_TIMEOUT | MULTI_STATEMENT_COUNT | NOORDER_SEQUENCE_AS_DEFAULT | ODBC_TREAT_DECIMAL_AS_INT | PYTHON_PROFILER_MODULES | PYTHON_PROFILER_TARGET_STAGE | QUERY_TAG | QUOTED_IDENTIFIERS_IGNORE_CASE | ROWS_PER_RESULTSET | S3_STAGE_VPCE_DNS_NAME | SEARCH_PATH | SIMULATED_DATA_SHARING_CONSUMER | STATEMENT_TIMEOUT_IN_SECONDS | STRICT_JSON_OUTPUT | TIME_INPUT_FORMAT | TIME_OUTPUT_FORMAT | TIMESTAMP_DAY_IS_ALWAYS_24H | TIMESTAMP_INPUT_FORMAT | TIMESTAMP_LTZ_OUTPUT_FORMAT | TIMESTAMP_NTZ_OUTPUT_FORMAT | TIMESTAMP_OUTPUT_FORMAT | TIMESTAMP_TYPE_MAPPING | TIMESTAMP_TZ_OUTPUT_FORMAT | TIMEZONE | TRANSACTION_ABORT_ON_ERROR | TRANSACTION_DEFAULT_ISOLATION_LEVEL | TWO_DIGIT_CENTURY_START | UNSUPPORTED_DDL_ACTION | USE_CACHED_RESULT | WEEK_OF_YEAR_POLICY | WEEK_START | CATALOG | DATA_RETENTION_TIME_IN_DAYS | DEFAULT_DDL_COLLATION | EXTERNAL_VOLUME | LOG_LEVEL | MAX_CONCURRENCY_LEVEL | MAX_DATA_EXTENSION_TIME_IN_DAYS | PIPE_EXECUTION_PAUSED | PREVENT_UNLOAD_TO_INTERNAL_STAGES | REPLACE_INVALID_CHARACTERS | STATEMENT_QUEUED_TIMEOUT_IN_SECONDS | STORAGE_SERIALIZATION_POLICY | SHARE_RESTRICTIONS | SUSPEND_TASK_AFTER_NUM_FAILURES | TRACE_LEVEL | USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE | USER_TASK_TIMEOUT_MS | TASK_AUTO_RETRY_ATTEMPTS | USER_TASK_MINIMUM_TRIGGER_INTERVAL_IN_SECONDS | METRIC_LEVEL | ENABLE_CONSOLE_OUTPUT | ENABLE_UNREDACTED_QUERY_SYNTAX_ERROR | ENABLE_PERSONAL_DATABASE. Deprecated parameters are not supported in the provider.'
            read: (String)
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
            value: (String) Value of account parameter, as a string. Constraints are the same as those for the parameters in Snowflake documentation. The parameter values are validated in Snowflake.
        importStatements:
            - terraform import snowflake_account_parameter.p '<parameter_name>'
    snowflake_account_password_policy_attachment:
        subCategory: Preview
        description: Specifies the password policy to use for the current account. To set the password policy of a different account, use a provider alias.
        name: snowflake_account_password_policy_attachment
        title: snowflake_account_password_policy_attachment Resource - terraform-provider-snowflake
        examples:
            - name: attachment
              manifest: |-
                {
                  "password_policy": "${snowflake_password_policy.default.fully_qualified_name}"
                }
              references:
                password_policy: snowflake_password_policy.default.fully_qualified_name
              dependencies:
                snowflake_password_policy.default: |-
                    {
                      "database": "prod",
                      "name": "default_policy",
                      "schema": "security"
                    }
        argumentDocs:
            create: (String)
            delete: (String)
            id: (String) The ID of this resource.
            password_policy: (String) Qualified name ("db"."schema"."policy_name") of the password policy to apply to the current account.
            read: (String)
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
        importStatements: []
    snowflake_account_role:
        subCategory: Stable
        description: The resource is used for role management, where roles can be assigned privileges and, in turn, granted to users and other roles. When granted to roles they can create hierarchies of privilege structures. For more details, refer to the official documentation https://docs.snowflake.com/en/user-guide/security-access-control-overview.
        name: snowflake_account_role
        title: snowflake_account_role Resource - terraform-provider-snowflake
        examples:
            - name: minimal
              manifest: |-
                {
                  "name": "role_name"
                }
            - name: complete
              manifest: |-
                {
                  "comment": "my account role",
                  "name": "role_name"
                }
        argumentDocs:
            assigned_to_users: (Number)
            comment: (String)
            create: (String)
            created_on: (String)
            delete: (String)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            granted_roles: (Number)
            granted_to_roles: (Number)
            id: (String) The ID of this resource.
            is_current: (Boolean)
            is_default: (Boolean)
            is_inherited: (Boolean)
            name: '(String) Identifier for the role; must be unique for your account. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            owner: (String)
            read: (String)
            show_output: (List of Object) Outputs the result of SHOW ROLES for the given role. (see below for nested schema)
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
        importStatements:
            - terraform import snowflake_account_role.example '"<account_role_name>"'
    snowflake_alert:
        subCategory: Preview
        name: snowflake_alert
        title: snowflake_alert Resource - terraform-provider-snowflake
        examples:
            - name: alert
              manifest: |-
                {
                  "action": "select 1 as c",
                  "alert_schedule": [
                    {
                      "interval": 10
                    }
                  ],
                  "comment": "my alert",
                  "condition": "select 1 as c",
                  "database": "database",
                  "enabled": true,
                  "name": "alert",
                  "schema": "schema",
                  "warehouse": "warehouse"
                }
        argumentDocs:
            action: (String) The SQL statement that should be executed if the condition returns one or more rows.
            alert_schedule: '(Block List, Max: 1) The schedule for periodically running an alert. (see below for nested schema)'
            comment: (String) Specifies a comment for the alert.
            condition: (String) The SQL statement that represents the condition for the alert. (SELECT, SHOW, CALL)
            create: (String)
            cron: '(Block List, Max: 1) Specifies the cron expression for the alert. The cron expression must be in the following format: "minute hour day-of-month month day-of-week". The following values are supported: minute: 0-59 hour: 0-23 day-of-month: 1-31 month: 1-12 day-of-week: 0-6 (0 is Sunday) (see below for nested schema)'
            database: (String) The database in which to create the alert.
            delete: (String)
            enabled: '(Boolean) (Default: false) Specifies if an alert should be ''started'' (enabled) after creation or should remain ''suspended'' (default).'
            expression: '(String) Specifies the cron expression for the alert. The cron expression must be in the following format: "minute hour day-of-month month day-of-week". The following values are supported: minute: 0-59 hour: 0-23 day-of-month: 1-31 month: 1-12 day-of-week: 0-6 (0 is Sunday)'
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            interval: (Number) Specifies the interval in minutes for the alert schedule. The interval must be greater than 0 and less than 1440 (24 hours).
            name: (String) Specifies the identifier for the alert; must be unique for the database and schema in which the alert is created.
            read: (String)
            schema: (String) The schema in which to create the alert.
            time_zone: (String) Specifies the time zone for alert refresh.
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
            warehouse: (String) The warehouse the alert will use.
        importStatements:
            - |-
              # format is database name | schema name | alert name
              terraform import snowflake_alert.example 'dbName|schemaName|alertName'
    snowflake_api_authentication_integration_with_authorization_code_grant:
        subCategory: Stable
        description: Resource used to manage api authentication security integration objects with authorization code grant. For more information, check security integrations documentation https://docs.snowflake.com/en/sql-reference/sql/create-security-integration-api-auth.
        name: snowflake_api_authentication_integration_with_authorization_code_grant
        title: snowflake_api_authentication_integration_with_authorization_code_grant Resource - terraform-provider-snowflake
        examples:
            - name: test
              manifest: |-
                {
                  "enabled": true,
                  "name": "test",
                  "oauth_client_id": "${var.oauth_client_id}",
                  "oauth_client_secret": "${var.oauth_client_secret}"
                }
              references:
                oauth_client_id: var.oauth_client_id
                oauth_client_secret: var.oauth_client_secret
            - name: test
              manifest: |-
                {
                  "comment": "comment",
                  "enabled": true,
                  "name": "test",
                  "oauth_access_token_validity": 42,
                  "oauth_allowed_scopes": [
                    "useraccount"
                  ],
                  "oauth_authorization_endpoint": "https://example.com",
                  "oauth_client_auth_method": "CLIENT_SECRET_POST",
                  "oauth_client_id": "${var.oauth_client_id}",
                  "oauth_client_secret": "${var.oauth_client_secret}",
                  "oauth_refresh_token_validity": 42,
                  "oauth_token_endpoint": "https://example.com"
                }
              references:
                oauth_client_id: var.oauth_client_id
                oauth_client_secret: var.oauth_client_secret
        argumentDocs:
            auth_type: (List of Object) (see below for nested schema)
            category: (String)
            comment: (String) Specifies a comment for the integration.
            create: (String)
            created_on: (String)
            default: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE SECURITY INTEGRATIONS for the given security integration. (see below for nested schema)
            enabled: (Boolean) Specifies whether this security integration is enabled or disabled.
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            integration_type: (String)
            name: '(String) Specifies the identifier (i.e. name) for the integration. This value must be unique in your account. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            oauth_access_token_validity: '(Number) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (-1)) Specifies the default lifetime of the OAuth access token (in seconds) issued by an OAuth server.'
            oauth_allowed_scopes: (Set of String) Specifies a list of scopes to use when making a request from the OAuth by a role with USAGE on the integration during the OAuth client credentials flow.
            oauth_authorization_endpoint: (String) Specifies the URL for authenticating to the external service. If removed from the config, the resource is recreated.
            oauth_client_auth_method: '(String) Specifies that POST is used as the authentication method to the external service. If removed from the config, the resource is recreated. Valid values are (case-insensitive): CLIENT_SECRET_POST.'
            oauth_client_id: (String, Sensitive) Specifies the client ID for the OAuth application in the external service.
            oauth_client_secret: (String, Sensitive) Specifies the client secret for the OAuth application in the ServiceNow instance from the previous step. The connector uses this to request an access token from the ServiceNow instance. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            oauth_grant: (List of Object) (see below for nested schema)
            oauth_refresh_token_validity: (Number) Specifies the value to determine the validity of the refresh token obtained from the OAuth server.
            oauth_token_endpoint: (String) Specifies the token endpoint used by the client to obtain an access token by presenting its authorization grant or refresh token. The token endpoint is used with every authorization grant except for the implicit grant type (since an access token is issued directly). If removed from the config, the resource is recreated.
            parent_integration: (List of Object) (see below for nested schema)
            read: (String)
            show_output: (List of Object) Outputs the result of SHOW SECURITY INTEGRATIONS for the given security integration. (see below for nested schema)
            timeouts: (Block, Optional) (see below for nested schema)
            type: (String)
            update: (String)
            value: (String)
        importStatements:
            - terraform import snowflake_api_authentication_integration_with_authorization_code_grant.example '"<integration_name>"'
    snowflake_api_authentication_integration_with_client_credentials:
        subCategory: Stable
        description: Resource used to manage api authentication security integration objects with client credentials. For more information, check security integrations documentation https://docs.snowflake.com/en/sql-reference/sql/create-security-integration-api-auth.
        name: snowflake_api_authentication_integration_with_client_credentials
        title: snowflake_api_authentication_integration_with_client_credentials Resource - terraform-provider-snowflake
        examples:
            - name: test
              manifest: |-
                {
                  "enabled": true,
                  "name": "test",
                  "oauth_client_id": "${var.oauth_client_id}",
                  "oauth_client_secret": "${var.oauth_client_secret}"
                }
              references:
                oauth_client_id: var.oauth_client_id
                oauth_client_secret: var.oauth_client_secret
            - name: test
              manifest: |-
                {
                  "comment": "comment",
                  "enabled": true,
                  "name": "test",
                  "oauth_access_token_validity": 42,
                  "oauth_allowed_scopes": [
                    "useraccount"
                  ],
                  "oauth_client_auth_method": "CLIENT_SECRET_POST",
                  "oauth_client_id": "${var.oauth_client_id}",
                  "oauth_client_secret": "${var.oauth_client_secret}",
                  "oauth_token_endpoint": "https://example.com"
                }
              references:
                oauth_client_id: var.oauth_client_id
                oauth_client_secret: var.oauth_client_secret
        argumentDocs:
            auth_type: (List of Object) (see below for nested schema)
            category: (String)
            comment: (String) Specifies a comment for the integration.
            create: (String)
            created_on: (String)
            default: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE SECURITY INTEGRATIONS for the given security integration. (see below for nested schema)
            enabled: (Boolean) Specifies whether this security integration is enabled or disabled.
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            integration_type: (String)
            name: '(String) Specifies the identifier (i.e. name) for the integration. This value must be unique in your account. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            oauth_access_token_validity: '(Number) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (-1)) Specifies the default lifetime of the OAuth access token (in seconds) issued by an OAuth server.'
            oauth_allowed_scopes: (Set of String) Specifies a list of scopes to use when making a request from the OAuth by a role with USAGE on the integration during the OAuth client credentials flow.
            oauth_authorization_endpoint: (List of Object) (see below for nested schema)
            oauth_client_auth_method: '(String) Specifies that POST is used as the authentication method to the external service. If removed from the config, the resource is recreated. Valid values are (case-insensitive): CLIENT_SECRET_POST.'
            oauth_client_id: (String, Sensitive) Specifies the client ID for the OAuth application in the external service.
            oauth_client_secret: (String, Sensitive) Specifies the client secret for the OAuth application in the ServiceNow instance from the previous step. The connector uses this to request an access token from the ServiceNow instance. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            oauth_grant: (List of Object) (see below for nested schema)
            oauth_refresh_token_validity: (Number) Specifies the value to determine the validity of the refresh token obtained from the OAuth server.
            oauth_token_endpoint: (String) Specifies the token endpoint used by the client to obtain an access token by presenting its authorization grant or refresh token. The token endpoint is used with every authorization grant except for the implicit grant type (since an access token is issued directly). If removed from the config, the resource is recreated.
            parent_integration: (List of Object) (see below for nested schema)
            read: (String)
            show_output: (List of Object) Outputs the result of SHOW SECURITY INTEGRATIONS for the given security integration. (see below for nested schema)
            timeouts: (Block, Optional) (see below for nested schema)
            type: (String)
            update: (String)
            value: (String)
        importStatements:
            - terraform import snowflake_api_authentication_integration_with_client_credentials.example '"<integration_name>"'
    snowflake_api_authentication_integration_with_jwt_bearer:
        subCategory: Stable
        description: Resource used to manage api authentication security integration objects with jwt bearer. For more information, check security integrations documentation https://docs.snowflake.com/en/sql-reference/sql/create-security-integration-api-auth.
        name: snowflake_api_authentication_integration_with_jwt_bearer
        title: snowflake_api_authentication_integration_with_jwt_bearer Resource - terraform-provider-snowflake
        examples:
            - name: test
              manifest: |-
                {
                  "enabled": true,
                  "name": "test",
                  "oauth_assertion_issuer": "issuer",
                  "oauth_client_id": "${var.oauth_client_id}",
                  "oauth_client_secret": "${var.oauth_client_secret}"
                }
              references:
                oauth_client_id: var.oauth_client_id
                oauth_client_secret: var.oauth_client_secret
            - name: test
              manifest: |-
                {
                  "comment": "comment",
                  "enabled": true,
                  "name": "test",
                  "oauth_access_token_validity": 42,
                  "oauth_assertion_issuer": "issuer",
                  "oauth_authorization_endpoint": "https://example.com",
                  "oauth_client_auth_method": "CLIENT_SECRET_POST",
                  "oauth_client_id": "${var.oauth_client_id}",
                  "oauth_client_secret": "${var.oauth_client_secret}",
                  "oauth_refresh_token_validity": 42,
                  "oauth_token_endpoint": "https://example.com"
                }
              references:
                oauth_client_id: var.oauth_client_id
                oauth_client_secret: var.oauth_client_secret
        argumentDocs:
            auth_type: (List of Object) (see below for nested schema)
            category: (String)
            comment: (String) Specifies a comment for the integration.
            create: (String)
            created_on: (String)
            default: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE SECURITY INTEGRATIONS for the given security integration. (see below for nested schema)
            enabled: (Boolean) Specifies whether this security integration is enabled or disabled.
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            integration_type: (String)
            name: '(String) Specifies the identifier (i.e. name) for the integration. This value must be unique in your account. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            oauth_access_token_validity: '(Number) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (-1)) Specifies the default lifetime of the OAuth access token (in seconds) issued by an OAuth server.'
            oauth_allowed_scopes: (List of Object) (see below for nested schema)
            oauth_assertion_issuer: (String)
            oauth_authorization_endpoint: (String) Specifies the URL for authenticating to the external service.
            oauth_client_auth_method: '(String) Specifies that POST is used as the authentication method to the external service. If removed from the config, the resource is recreated. Valid values are (case-insensitive): CLIENT_SECRET_POST.'
            oauth_client_id: (String, Sensitive) Specifies the client ID for the OAuth application in the external service.
            oauth_client_secret: (String, Sensitive) Specifies the client secret for the OAuth application in the ServiceNow instance from the previous step. The connector uses this to request an access token from the ServiceNow instance. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            oauth_grant: (List of Object) (see below for nested schema)
            oauth_refresh_token_validity: (Number) Specifies the value to determine the validity of the refresh token obtained from the OAuth server.
            oauth_token_endpoint: (String) Specifies the token endpoint used by the client to obtain an access token by presenting its authorization grant or refresh token. The token endpoint is used with every authorization grant except for the implicit grant type (since an access token is issued directly). If removed from the config, the resource is recreated.
            parent_integration: (List of Object) (see below for nested schema)
            read: (String)
            show_output: (List of Object) Outputs the result of SHOW SECURITY INTEGRATIONS for the given security integration. (see below for nested schema)
            timeouts: (Block, Optional) (see below for nested schema)
            type: (String)
            update: (String)
            value: (String)
        importStatements:
            - terraform import snowflake_api_authentication_integration_with_jwt_bearer.example '"<integration_name>"'
    snowflake_api_integration:
        subCategory: Preview
        name: snowflake_api_integration
        title: snowflake_api_integration Resource - terraform-provider-snowflake
        examples:
            - name: aws
              manifest: |-
                {
                  "api_allowed_prefixes": [
                    "https://123456.execute-api.us-west-2.amazonaws.com/prod/"
                  ],
                  "api_aws_role_arn": "arn:aws:iam::000000000001:/role/test",
                  "api_provider": "aws_api_gateway",
                  "enabled": true,
                  "name": "aws_integration"
                }
            - name: azure
              manifest: |-
                {
                  "api_allowed_prefixes": [
                    "https://apim-hello-world.azure-api.net/"
                  ],
                  "api_provider": "azure_api_management",
                  "azure_ad_application_id": "11111111-1111-1111-1111-111111111111",
                  "azure_tenant_id": "00000000-0000-0000-0000-000000000000",
                  "enabled": true,
                  "name": "azure_integration"
                }
            - name: gcp
              manifest: |-
                {
                  "api_allowed_prefixes": [
                    "https://gateway-id-123456.uc.gateway.dev/"
                  ],
                  "api_provider": "google_api_gateway",
                  "enabled": true,
                  "google_audience": "api-gateway-id-123456.apigateway.gcp-project.cloud.goog",
                  "name": "gcp_integration"
                }
        argumentDocs:
            api_allowed_prefixes: (List of String) Explicitly limits external functions that use the integration to reference one or more HTTPS proxy service endpoints and resources within those proxies.
            api_aws_external_id: (String) The external ID that Snowflake will use when assuming the AWS role.
            api_aws_iam_user_arn: (String) The Snowflake user that will attempt to assume the AWS role.
            api_aws_role_arn: '(String) (Default: ``) ARN of a cloud platform role.'
            api_blocked_prefixes: (List of String) Lists the endpoints and resources in the HTTPS proxy service that are not allowed to be called from Snowflake.
            api_gcp_service_account: (String) The service account used for communication with the Google API Gateway.
            api_key: (String, Sensitive) The API key (also called a “subscription key”).
            api_provider: (String) Specifies the HTTPS proxy service type.
            azure_ad_application_id: '(String) (Default: ``) The ''Application (client) id'' of the Azure AD app for your remote service.'
            azure_consent_url: (String)
            azure_multi_tenant_app_name: (String)
            azure_tenant_id: '(String) (Default: ``) Specifies the ID for your Office 365 tenant that all Azure API Management instances belong to.'
            comment: (String)
            create: (String)
            created_on: (String) Date and time when the API integration was created.
            delete: (String)
            enabled: '(Boolean) (Default: true) Specifies whether this API integration is enabled or disabled. If the API integration is disabled, any external function that relies on it will not work.'
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            google_audience: '(String) (Default: ``) The audience claim when generating the JWT (JSON Web Token) to authenticate to the Google API Gateway.'
            id: (String) The ID of this resource.
            name: (String) Specifies the name of the API integration. This name follows the rules for Object Identifiers. The name should be unique among api integrations in your account.
            read: (String)
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
        importStatements:
            - terraform import snowflake_api_integration.example name
    snowflake_authentication_policy:
        subCategory: Preview
        description: Resource used to manage authentication policy objects. For more information, check authentication policy documentation https://docs.snowflake.com/en/sql-reference/sql/create-authentication-policy.
        name: snowflake_authentication_policy
        title: snowflake_authentication_policy Resource - terraform-provider-snowflake
        examples:
            - name: basic
              manifest: |-
                {
                  "database": "database_name",
                  "name": "network_policy_name",
                  "schema": "schema_name"
                }
            - name: complete
              manifest: |-
                {
                  "authentication_methods": [
                    "ALL"
                  ],
                  "client_types": [
                    "ALL"
                  ],
                  "comment": "My authentication policy.",
                  "database": "database_name",
                  "mfa_authentication_methods": [
                    "SAML",
                    "PASSWORD"
                  ],
                  "mfa_enrollment": "OPTIONAL",
                  "name": "network_policy_name",
                  "schema": "schema_name",
                  "security_integrations": [
                    "ALL"
                  ]
                }
        argumentDocs:
            authentication_methods: '(Set of String) A list of authentication methods that are allowed during login. This parameter accepts one or more of the following values: ALL | SAML | PASSWORD | OAUTH | KEYPAIR'
            client_types: (Set of String) A list of clients that can authenticate with Snowflake. If a client tries to connect, and the client is not one of the valid CLIENT_TYPES, then the login attempt fails. Allowed values are ALL | SNOWFLAKE_UI | DRIVERS | SNOWSQL. The CLIENT_TYPES property of an authentication policy is a best effort method to block user logins based on specific clients. It should not be used as the sole control to establish a security boundary.
            comment: (String) Specifies a comment for the authentication policy.
            create: (String)
            created_on: (String)
            database: '(String) The database in which to create the authentication policy. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            database_name: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE AUTHENTICATION POLICY for the given policy. (see below for nested schema)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            mfa_authentication_methods: (Set of String) A list of authentication methods that enforce multi-factor authentication (MFA) during login. Authentication methods not listed in this parameter do not prompt for multi-factor authentication. Allowed values are ALL | SAML | PASSWORD.
            mfa_enrollment: '(String) (Default: OPTIONAL) Determines whether a user must enroll in multi-factor authentication. Allowed values are REQUIRED and OPTIONAL. When REQUIRED is specified, Enforces users to enroll in MFA. If this value is used, then the CLIENT_TYPES parameter must include SNOWFLAKE_UI, because Snowsight is the only place users can enroll in multi-factor authentication (MFA).'
            name: '(String) Specifies the identifier for the authentication policy. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            options: (String)
            owner: (String)
            owner_role_type: (String)
            read: (String)
            schema: '(String) The schema in which to create the authentication policy. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            security_integrations: (Set of String) A list of security integrations the authentication policy is associated with. This parameter has no effect when SAML or OAUTH are not in the AUTHENTICATION_METHODS list. All values in the SECURITY_INTEGRATIONS list must be compatible with the values in the AUTHENTICATION_METHODS list. For example, if SECURITY_INTEGRATIONS contains a SAML security integration, and AUTHENTICATION_METHODS contains OAUTH, then you cannot create the authentication policy. To allow all security integrations use ALL as parameter.
            show_output: (List of Object) Outputs the result of SHOW AUTHENTICATION POLICIES for the given policy. (see below for nested schema)
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
        importStatements:
            - terraform import snowflake_authentication_policy.example '"<database_name>"."<schema_name>"."<authentication_policy_name>"'
    snowflake_compute_pool:
        subCategory: Preview
        description: Resource used to manage compute pools. For more information, check compute pools documentation https://docs.snowflake.com/en/sql-reference/sql/create-compute-pool. A compute pool is a collection of one or more virtual machine (VM) nodes on which Snowflake runs your Snowpark Container Services services (including job services). See Working with compute pools https://docs.snowflake.com/en/developer-guide/snowpark-container-services/working-with-compute-pool developer guide for more details.
        name: snowflake_compute_pool
        title: snowflake_compute_pool Resource - terraform-provider-snowflake
        examples:
            - name: basic
              manifest: |-
                {
                  "instance_family": "CPU_X64_S",
                  "max_nodes": 2,
                  "min_nodes": 1,
                  "name": "COMPUTE_POOL"
                }
            - name: complete
              manifest: |-
                {
                  "auto_resume": "true",
                  "auto_suspend_secs": 1200,
                  "comment": "A compute pool.",
                  "for_application": "APPLICATION_NAME",
                  "initially_suspended": "true",
                  "instance_family": "CPU_X64_S",
                  "max_nodes": 2,
                  "min_nodes": 1,
                  "name": "COMPUTE_POOL"
                }
        argumentDocs:
            active_nodes: (Number)
            application: (String)
            auto_resume: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies whether to automatically resume a compute pool when a service or job is submitted to it. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            auto_suspend_secs: '(Number) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (-1)) Number of seconds of inactivity after which you want Snowflake to automatically suspend the compute pool.'
            comment: (String) Specifies a comment for the compute pool.
            create: (String)
            created_on: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE COMPUTE POOL for the given compute pool. (see below for nested schema)
            error_code: (String)
            for_application: (String) Specifies the Snowflake Native App name.
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            idle_nodes: (Number)
            initially_suspended: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies whether the compute pool is created initially in the suspended state. This field is used only when creating a compute pool. Changes on this field are ignored after creation.'
            instance_family: '(String) Identifies the type of machine you want to provision for the nodes in the compute pool. Valid values are (case-insensitive): CPU_X64_XS | CPU_X64_S | CPU_X64_M | CPU_X64_L | HIGHMEM_X64_S | HIGHMEM_X64_M | HIGHMEM_X64_L | HIGHMEM_X64_SL | GPU_NV_S | GPU_NV_M | GPU_NV_L | GPU_NV_XS | GPU_NV_SM | GPU_NV_2M | GPU_NV_3M | GPU_NV_SL.'
            is_exclusive: (Boolean)
            max_nodes: (Number) Specifies the maximum number of nodes for the compute pool.
            min_nodes: (Number) Specifies the minimum number of nodes for the compute pool.
            name: '(String) Specifies the identifier for the compute pool; must be unique for the account. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            num_jobs: (Number)
            num_services: (Number)
            owner: (String)
            read: (String)
            resumed_on: (String)
            show_output: (List of Object) Outputs the result of SHOW COMPUTE POOLS for the given compute pool. (see below for nested schema)
            state: (String)
            status_message: (String)
            target_nodes: (Number)
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
            updated_on: (String)
        importStatements:
            - terraform import snowflake_compute_pool.example '"<compute_pool_name>"'
    snowflake_cortex_search_service:
        subCategory: Preview
        name: snowflake_cortex_search_service
        title: snowflake_cortex_search_service Resource - terraform-provider-snowflake
        examples:
            - name: test
              manifest: |-
                {
                  "comment": "some comment",
                  "database": "${snowflake_database.test.name}",
                  "depends_on": [
                    "${snowflake_table.test}"
                  ],
                  "embedding_model": "snowflake-arctic-embed-m-v1.5",
                  "name": "some_name",
                  "on": "SOME_TEXT",
                  "query": "SELECT SOME_TEXT FROM \"some_database\".\"some_schema\".\"some_table\"",
                  "schema": "${snowflake_schema.test.name}",
                  "target_lag": "2 minutes",
                  "warehouse": "some_warehouse"
                }
              references:
                database: snowflake_database.test.name
                schema: snowflake_schema.test.name
              dependencies:
                snowflake_database.test: |-
                    {
                      "name": "some_database"
                    }
                snowflake_schema.test: |-
                    {
                      "database": "${snowflake_database.test.name}",
                      "name": "some_schema"
                    }
                snowflake_table.test: |-
                    {
                      "change_tracking": true,
                      "column": [
                        {
                          "name": "ID",
                          "type": "NUMBER(38,0)"
                        },
                        {
                          "name": "SOME_TEXT",
                          "type": "VARCHAR"
                        }
                      ],
                      "database": "${snowflake_database.test.name}",
                      "name": "some_table",
                      "schema": "${snowflake_schema.test.name}"
                    }
        argumentDocs:
            attribute_columns: (List of String)
            attributes: (Set of String) Specifies the list of columns in the base table to enable filtering on when issuing queries to the service.
            columns: (List of String)
            comment: (String) Specifies a comment for the Cortex search service.
            create: (String)
            created_on: (String) Creation date for the given Cortex search service.
            data_timestamp: (String)
            database: (String) The database in which to create the Cortex search service.
            database_name: (String)
            definition: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE CORTEX SEARCH SERVICE for the given cortex search service. (see below for nested schema)
            embedding_model: (String) Specifies the embedding model to use for the Cortex search service.
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            indexing_error: (String)
            indexing_state: (String)
            name: (String) Specifies the name of the Cortex search service. The name must be unique for the schema in which the service is created.
            "on": (String) Specifies the column to use as the search column for the Cortex search service; must be a text value.
            query: (String) Specifies the query to use to populate the Cortex search service.
            read: (String)
            schema: (String) The schema in which to create the Cortex search service.
            schema_name: (String)
            search_column: (String)
            service_query_url: (String)
            source_data_num_rows: (Number)
            target_lag: (String) Specifies the maximum target lag time for the Cortex search service.
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
            warehouse: (String) The warehouse in which to create the Cortex search service.
        importStatements:
            - terraform import snowflake_cortex_search_service.example 'dbName|schemaName|fileFormatName'
    snowflake_current_account:
        subCategory: Preview
        description: Resource used to manage the account you are currently connected to. This resource is used to set account parameters and other account-level settings. See ALTER ACCOUNT https://docs.snowflake.com/en/sql-reference/sql/alter-account documentation for more information on resource capabilities.
        name: snowflake_current_account
        title: snowflake_current_account Resource - terraform-provider-snowflake
        examples:
            - name: minimal
              manifest: '{}'
            - name: complete
              manifest: |-
                {
                  "abort_detached_query": true,
                  "allow_client_mfa_caching": true,
                  "allow_id_token": true,
                  "authentication_policy": "${snowflake_authentication_policy.example.fully_qualified_name}",
                  "autocommit": false,
                  "base_location_prefix": "STORAGE_BASE_URL/",
                  "binary_input_format": "BASE64",
                  "binary_output_format": "BASE64",
                  "catalog": "SNOWFLAKE",
                  "client_enable_log_info_statement_parameters": true,
                  "client_encryption_key_size": 256,
                  "client_memory_limit": 1540,
                  "client_metadata_request_use_connection_ctx": true,
                  "client_metadata_use_session_database": true,
                  "client_prefetch_threads": 5,
                  "client_result_chunk_size": 159,
                  "client_result_column_case_insensitive": true,
                  "client_session_keep_alive": true,
                  "client_session_keep_alive_heartbeat_frequency": 3599,
                  "client_timestamp_type_mapping": "TIMESTAMP_NTZ",
                  "cortex_enabled_cross_region": "ANY_REGION",
                  "cortex_models_allowlist": "All",
                  "csv_timestamp_format": "YYYY-MM-DD",
                  "data_retention_time_in_days": 2,
                  "date_input_format": "YYYY-MM-DD",
                  "date_output_format": "YYYY-MM-DD",
                  "default_ddl_collation": "en-cs",
                  "default_notebook_compute_pool_cpu": "CPU_X64_S",
                  "default_notebook_compute_pool_gpu": "GPU_NV_S",
                  "default_null_ordering": "FIRST",
                  "default_streamlit_notebook_warehouse": "${snowflake_warehouse.example.fully_qualified_name}",
                  "disable_ui_download_button": true,
                  "disable_user_privilege_grants": true,
                  "enable_automatic_sensitive_data_classification_log": false,
                  "enable_egress_cost_optimizer": false,
                  "enable_identifier_first_login": false,
                  "enable_tri_secret_and_rekey_opt_out_for_image_repository": true,
                  "enable_tri_secret_and_rekey_opt_out_for_spcs_block_storage": true,
                  "enable_unhandled_exceptions_reporting": false,
                  "enable_unload_physical_type_optimization": false,
                  "enable_unredacted_query_syntax_error": true,
                  "enable_unredacted_secure_object_error": true,
                  "enforce_network_rules_for_internal_stages": true,
                  "error_on_nondeterministic_merge": false,
                  "error_on_nondeterministic_update": true,
                  "event_table": "\"\u003cdatabase_name\u003e\".\"\u003cschema_name\u003e\".\"\u003cevent_table_name\u003e\"",
                  "external_oauth_add_privileged_roles_to_blocked_list": false,
                  "external_volume": "XWDVEAAT_A6FEE9D6_5D41_AB3D_EB0C_51DA5E5F0BE2",
                  "feature_policy": "\"\u003cdatabase_name\u003e\".\"\u003cschema_name\u003e\".\"\u003cfeature_policy_name\u003e\"",
                  "geography_output_format": "WKT",
                  "geometry_output_format": "WKT",
                  "hybrid_table_lock_timeout": 3599,
                  "initial_replication_size_limit_in_tb": "9.9",
                  "jdbc_treat_decimal_as_int": false,
                  "jdbc_treat_timestamp_ntz_as_utc": true,
                  "jdbc_use_session_timezone": false,
                  "js_treat_integer_as_bigint": true,
                  "json_indent": 4,
                  "listing_auto_fulfillment_replication_refresh_schedule": "2 minutes",
                  "lock_timeout": 43201,
                  "log_level": "INFO",
                  "max_concurrency_level": 7,
                  "max_data_extension_time_in_days": 13,
                  "metric_level": "ALL",
                  "min_data_retention_time_in_days": 1,
                  "multi_statement_count": 0,
                  "network_policy": "${snowflake_network_policy.example.fully_qualified_name}",
                  "noorder_sequence_as_default": false,
                  "oauth_add_privileged_roles_to_blocked_list": false,
                  "odbc_treat_decimal_as_int": true,
                  "packages_policy": "\"\u003cdatabase_name\u003e\".\"\u003cschema_name\u003e\".\"\u003cpackages_policy_name\u003e\"",
                  "password_policy": "${snowflake_password_policy.example.fully_qualified_name}",
                  "periodic_data_rekeying": false,
                  "pipe_execution_paused": true,
                  "prevent_unload_to_inline_url": true,
                  "prevent_unload_to_internal_stages": true,
                  "python_profiler_target_stage": "${snowflake_stage.example.fully_qualified_name}",
                  "query_tag": "test-query-tag",
                  "quoted_identifiers_ignore_case": true,
                  "replace_invalid_characters": true,
                  "require_storage_integration_for_stage_creation": true,
                  "require_storage_integration_for_stage_operation": true,
                  "resource_monitor": "${snowflake_resource_monitor.example.fully_qualified_name}",
                  "rows_per_resultset": 1000,
                  "search_path": "$current, $public",
                  "serverless_task_max_statement_size": "XLARGE",
                  "serverless_task_min_statement_size": "SMALL",
                  "session_policy": "\"\u003cdatabase_name\u003e\".\"\u003cschema_name\u003e\".\"\u003csession_policy_name\u003e\"",
                  "sso_login_page": true,
                  "statement_queued_timeout_in_seconds": 1,
                  "statement_timeout_in_seconds": 1,
                  "storage_serialization_policy": "OPTIMIZED",
                  "strict_json_output": true,
                  "suspend_task_after_num_failures": 3,
                  "task_auto_retry_attempts": 3,
                  "time_input_format": "YYYY-MM-DD",
                  "time_output_format": "YYYY-MM-DD",
                  "timestamp_day_is_always_24h": true,
                  "timestamp_input_format": "YYYY-MM-DD",
                  "timestamp_ltz_output_format": "YYYY-MM-DD",
                  "timestamp_ntz_output_format": "YYYY-MM-DD",
                  "timestamp_output_format": "YYYY-MM-DD",
                  "timestamp_type_mapping": "TIMESTAMP_LTZ",
                  "timestamp_tz_output_format": "YYYY-MM-DD",
                  "timezone": "Europe/London",
                  "trace_level": "PROPAGATE",
                  "transaction_abort_on_error": true,
                  "transaction_default_isolation_level": "READ COMMITTED",
                  "two_digit_century_start": 1971,
                  "unsupported_ddl_action": "FAIL",
                  "use_cached_result": false,
                  "user_task_managed_initial_warehouse_size": "SMALL",
                  "user_task_minimum_trigger_interval_in_seconds": 10,
                  "user_task_timeout_ms": 10,
                  "week_of_year_policy": 1,
                  "week_start": 1
                }
              references:
                authentication_policy: snowflake_authentication_policy.example.fully_qualified_name
                default_streamlit_notebook_warehouse: snowflake_warehouse.example.fully_qualified_name
                network_policy: snowflake_network_policy.example.fully_qualified_name
                password_policy: snowflake_password_policy.example.fully_qualified_name
                python_profiler_target_stage: snowflake_stage.example.fully_qualified_name
                resource_monitor: snowflake_resource_monitor.example.fully_qualified_name
        argumentDocs:
            abort_detached_query: (Boolean) Specifies the action that Snowflake performs for in-progress queries if connectivity is lost due to abrupt termination of a session (e.g. network outage, browser termination, service interruption). For more information, check ABORT_DETACHED_QUERY docs.
            active_python_profiler: '(String) Sets the profiler to use for the session when profiling Python handler code. Valid values are (case-insensitive): LINE | MEMORY. For more information, check ACTIVE_PYTHON_PROFILER docs.'
            allow_client_mfa_caching: (Boolean) Specifies whether an MFA token can be saved in the client-side operating system keystore to promote continuous, secure connectivity without users needing to respond to an MFA prompt at the start of each connection attempt to Snowflake. For details and the list of supported Snowflake-provided clients, see Using MFA token caching to minimize the number of prompts during authentication — optional. For more information, check ALLOW_CLIENT_MFA_CACHING docs.
            allow_id_token: (Boolean) Specifies whether a connection token can be saved in the client-side operating system keystore to promote continuous, secure connectivity without users needing to enter login credentials at the start of each connection attempt to Snowflake. For details and the list of supported Snowflake-provided clients, see Using connection caching to minimize the number of prompts for authentication — optional. For more information, check ALLOW_ID_TOKEN docs.
            authentication_policy: (String) Specifies authentication policy for the current account. For more information about this resource, see docs.
            autocommit: (Boolean) Specifies whether autocommit is enabled for the session. Autocommit determines whether a DML statement, when executed without an active transaction, is automatically committed after the statement successfully completes. For more information, see Transactions. For more information, check AUTOCOMMIT docs.
            base_location_prefix: (String) Specifies a prefix for Snowflake to use in the write path for Snowflake-managed Apache Iceberg™ tables. For more information, see data and metadata directories for Iceberg tables. For more information, check BASE_LOCATION_PREFIX docs.
            binary_input_format: '(String) The format of VARCHAR values passed as input to VARCHAR-to-BINARY conversion functions. For more information, see Binary input and output. Valid values are (case-insensitive): HEX | BASE64 | UTF8. For more information, check BINARY_INPUT_FORMAT docs.'
            binary_output_format: '(String) The format for VARCHAR values returned as output by BINARY-to-VARCHAR conversion functions. For more information, see Binary input and output. Valid values are (case-insensitive): HEX | BASE64. For more information, check BINARY_OUTPUT_FORMAT docs.'
            catalog: '(String) Specifies the catalog for Apache Iceberg™ tables. For more information, see the Iceberg table documentation. Due to technical limitations (read more here), avoid using the following characters: |, ., ". For more information, check CATALOG docs.'
            catalog_sync: '(String) Specifies the name of your catalog integration for Snowflake Open Catalog. Snowflake syncs tables that use the specified catalog integration with your Snowflake Open Catalog account. For more information, see Sync a Snowflake-managed table with Snowflake Open Catalog. Due to technical limitations (read more here), avoid using the following characters: |, ., ". For more information, check CATALOG_SYNC docs.'
            client_enable_log_info_statement_parameters: (Boolean) Enables users to log the data values bound to PreparedStatements (more details). For more information, check CLIENT_ENABLE_LOG_INFO_STATEMENT_PARAMETERS docs.
            client_encryption_key_size: (Number) Specifies the AES encryption key size, in bits, used by Snowflake to encrypt/decrypt files stored on internal stages (for loading/unloading data) when you use the SNOWFLAKE_FULL encryption type. For more information, check CLIENT_ENCRYPTION_KEY_SIZE docs.
            client_memory_limit: (Number) Parameter that specifies the maximum amount of memory the JDBC driver or ODBC driver should use for the result set from queries (in MB). For more information, check CLIENT_MEMORY_LIMIT docs.
            client_metadata_request_use_connection_ctx: (Boolean) For specific ODBC functions and JDBC methods, this parameter can change the default search scope from all databases/schemas to the current database/schema. The narrower search typically returns fewer rows and executes more quickly. For more information, check CLIENT_METADATA_REQUEST_USE_CONNECTION_CTX docs.
            client_metadata_use_session_database: (Boolean) For specific ODBC functions and JDBC methods, this parameter can change the default search scope from all databases to the current database. The narrower search typically returns fewer rows and executes more quickly (more details on the usage). For more information, check CLIENT_METADATA_USE_SESSION_DATABASE docs.
            client_prefetch_threads: (Number) Parameter that specifies the number of threads used by the client to pre-fetch large result sets. The driver will attempt to honor the parameter value, but defines the minimum and maximum values (depending on your system’s resources) to improve performance. For more information, check CLIENT_PREFETCH_THREADS docs.
            client_result_chunk_size: (Number) Parameter that specifies the maximum size of each set (or chunk) of query results to download (in MB). The JDBC driver downloads query results in chunks. For more information, check CLIENT_RESULT_CHUNK_SIZE docs.
            client_result_column_case_insensitive: (Boolean) Parameter that indicates whether to match column name case-insensitively in ResultSet.get* methods in JDBC. For more information, check CLIENT_RESULT_COLUMN_CASE_INSENSITIVE docs.
            client_session_keep_alive: (Boolean) Parameter that indicates whether to force a user to log in again after a period of inactivity in the session. For more information, check CLIENT_SESSION_KEEP_ALIVE docs.
            client_session_keep_alive_heartbeat_frequency: (Number) Number of seconds in-between client attempts to update the token for the session. For more information, check CLIENT_SESSION_KEEP_ALIVE_HEARTBEAT_FREQUENCY docs.
            client_timestamp_type_mapping: '(String) Specifies the TIMESTAMP_* variation to use when binding timestamp variables for JDBC or ODBC applications that use the bind API to load data. Valid values are (case-insensitive): TIMESTAMP_LTZ | TIMESTAMP_NTZ. For more information, check CLIENT_TIMESTAMP_TYPE_MAPPING docs.'
            cortex_enabled_cross_region: (String) Specifies the regions where an inference request may be processed in case the request cannot be processed in the region where request is originally placed. Specifying DISABLED disables cross-region inferencing. For examples and details, see Cross-region inference. For more information, check CORTEX_ENABLED_CROSS_REGION docs.
            cortex_models_allowlist: (String) Specifies the models that users in the account can access. Use this parameter to allowlist models for all users in the account. If you need to provide specific users with access beyond what you’ve specified in the allowlist, use role-based access control instead. For more information, see Model allowlist. For more information, check CORTEX_MODELS_ALLOWLIST docs.
            create: (String)
            csv_timestamp_format: (String) Specifies the format for TIMESTAMP values in CSV files downloaded from Snowsight. If this parameter is not set, TIMESTAMP_LTZ_OUTPUT_FORMAT will be used for TIMESTAMP_LTZ values, TIMESTAMP_TZ_OUTPUT_FORMAT will be used for TIMESTAMP_TZ and TIMESTAMP_NTZ_OUTPUT_FORMAT for TIMESTAMP_NTZ values. For more information, see Date and time input and output formats or Download your query results. For more information, check CSV_TIMESTAMP_FORMAT docs.
            data_retention_time_in_days: (Number) Number of days for which Snowflake retains historical data for performing Time Travel actions (SELECT, CLONE, UNDROP) on the object. A value of 0 effectively disables Time Travel for the specified database, schema, or table. For more information, see Understanding & using Time Travel. For more information, check DATA_RETENTION_TIME_IN_DAYS docs.
            date_input_format: (String) Specifies the input format for the DATE data type. For more information, see Date and time input and output formats. For more information, check DATE_INPUT_FORMAT docs.
            date_output_format: (String) Specifies the display format for the DATE data type. For more information, see Date and time input and output formats. For more information, check DATE_OUTPUT_FORMAT docs.
            default_ddl_collation: '(String) Sets the default collation used for the following DDL operations: CREATE TABLE, ALTER TABLE … ADD COLUMN. Setting this parameter forces all subsequently-created columns in the affected objects (table, schema, database, or account) to have the specified collation as the default, unless the collation for the column is explicitly defined in the DDL. For more information, check DEFAULT_DDL_COLLATION docs.'
            default_notebook_compute_pool_cpu: '(String) Sets the preferred CPU compute pool used for Notebooks on CPU Container Runtime. Due to technical limitations (read more here), avoid using the following characters: |, ., ". For more information, check DEFAULT_NOTEBOOK_COMPUTE_POOL_CPU docs.'
            default_notebook_compute_pool_gpu: '(String) Sets the preferred GPU compute pool used for Notebooks on GPU Container Runtime. Due to technical limitations (read more here), avoid using the following characters: |, ., ". For more information, check DEFAULT_NOTEBOOK_COMPUTE_POOL_GPU docs.'
            default_null_ordering: '(String) Specifies the default ordering of NULL values in a result set (more details). Valid values are (case-insensitive): FIRST | LAST. For more information, check DEFAULT_NULL_ORDERING docs.'
            default_streamlit_notebook_warehouse: '(String) Specifies the name of the default warehouse to use when creating a notebook. Due to technical limitations (read more here), avoid using the following characters: |, ., ". For more information, check DEFAULT_STREAMLIT_NOTEBOOK_WAREHOUSE docs.'
            delete: (String)
            disable_ui_download_button: (Boolean) Controls whether users in an account see a button to download data in Snowsight or the Classic Console, such as a table returned from running a query in a worksheet. If the button to download is hidden in Snowsight or the Classic Console, users can still download or export data using third-party software. For more information, check DISABLE_UI_DOWNLOAD_BUTTON docs.
            disable_user_privilege_grants: (Boolean) Controls whether users in an account can grant privileges directly to other users. Disabling user privilege grants (that is, setting DISABLE_USER_PRIVILEGE_GRANTS to TRUE) does not affect existing grants to users. Existing grants to users continue to confer privileges to those users. For more information, see GRANT . For more information, check DISABLE_USER_PRIVILEGE_GRANTS docs.
            enable_automatic_sensitive_data_classification_log: (Boolean) Controls whether events from automatic sensitive data classification are logged in the user event table. For more information, check ENABLE_AUTOMATIC_SENSITIVE_DATA_CLASSIFICATION_LOG docs.
            enable_egress_cost_optimizer: (Boolean) Enables or disables the Listing Cross-cloud auto-fulfillment Egress cost optimizer. For more information, check ENABLE_EGRESS_COST_OPTIMIZER docs.
            enable_identifier_first_login: (Boolean) Determines the login flow for users. When enabled, Snowflake prompts users for their username or email address before presenting authentication methods. For details, see Identifier-first login. For more information, check ENABLE_IDENTIFIER_FIRST_LOGIN docs.
            enable_internal_stages_privatelink: (Boolean) Specifies whether the SYSTEM$GET_PRIVATELINK_CONFIG function returns the private-internal-stages key in the query result. The corresponding value in the query result is used during the configuration process for private connectivity to internal stages. For more information, check ENABLE_INTERNAL_STAGES_PRIVATELINK docs.
            enable_tri_secret_and_rekey_opt_out_for_image_repository: (Boolean) Specifies choice for the image repository to opt out of Tri-Secret Secure and Periodic rekeying. For more information, check ENABLE_TRI_SECRET_AND_REKEY_OPT_OUT_FOR_IMAGE_REPOSITORY docs.
            enable_tri_secret_and_rekey_opt_out_for_spcs_block_storage: (Boolean) Specifies the choice for the Snowpark Container Services block storage volume to opt out of Tri-Secret Secure and Periodic rekeying. For more information, check ENABLE_TRI_SECRET_AND_REKEY_OPT_OUT_FOR_SPCS_BLOCK_STORAGE docs.
            enable_unhandled_exceptions_reporting: (Boolean) Specifies whether Snowflake may capture – in an event table – log messages or trace event data for unhandled exceptions in procedure or UDF handler code. For more information, see Capturing messages from unhandled exceptions. For more information, check ENABLE_UNHANDLED_EXCEPTIONS_REPORTING docs.
            enable_unload_physical_type_optimization: (Boolean) Specifies whether to set the schema for unloaded Parquet files based on the logical column data types (i.e. the types in the unload SQL query or source table) or on the unloaded column values (i.e. the smallest data types and precision that support the values in the output columns of the unload SQL statement or source table). For more information, check ENABLE_UNLOAD_PHYSICAL_TYPE_OPTIMIZATION docs.
            enable_unredacted_query_syntax_error: (Boolean) Controls whether query text is redacted if a SQL query fails due to a syntax or parsing error. If FALSE, the content of a failed query is redacted in the views, pages, and functions that provide a query history. Only users with a role that is granted or inherits the AUDIT privilege can set the ENABLE_UNREDACTED_QUERY_SYNTAX_ERROR parameter. When using the ALTER USER command to set the parameter to TRUE for a particular user, modify the user that you want to see the query text, not the user who executed the query (if those are different users). For more information, check ENABLE_UNREDACTED_QUERY_SYNTAX_ERROR docs.
            enable_unredacted_secure_object_error: '(Boolean) Controls whether error messages related to secure objects are redacted in metadata. For more information, see Secure objects: Redaction of information in error messages. Only users with a role that is granted or inherits the AUDIT privilege can set the ENABLE_UNREDACTED_SECURE_OBJECT_ERROR parameter. When using the ALTER USER command to set the parameter to TRUE for a particular user, modify the user that you want to see the redacted error messages in metadata, not the user who caused the error. For more information, check ENABLE_UNREDACTED_SECURE_OBJECT_ERROR docs.'
            enforce_network_rules_for_internal_stages: (Boolean) Specifies whether a network policy that uses network rules can restrict access to AWS internal stages. This parameter has no effect on network policies that do not use network rules. This account-level parameter affects both account-level and user-level network policies. For details about using network policies and network rules to restrict access to AWS internal stages, including the use of this parameter, see Protecting internal stages on AWS. For more information, check ENFORCE_NETWORK_RULES_FOR_INTERNAL_STAGES docs.
            error_on_nondeterministic_merge: (Boolean) Specifies whether to return an error when the MERGE command is used to update or delete a target row that joins multiple source rows and the system cannot determine the action to perform on the target row. For more information, check ERROR_ON_NONDETERMINISTIC_MERGE docs.
            error_on_nondeterministic_update: (Boolean) Specifies whether to return an error when the UPDATE command is used to update a target row that joins multiple source rows and the system cannot determine the action to perform on the target row. For more information, check ERROR_ON_NONDETERMINISTIC_UPDATE docs.
            event_table: '(String) Specifies the name of the event table for logging messages from stored procedures and UDFs contained by the object with which the event table is associated. Associating an event table with a database is available in Enterprise Edition or higher. Due to technical limitations (read more here), avoid using the following characters: |, ., ". For more information, check EVENT_TABLE docs.'
            external_oauth_add_privileged_roles_to_blocked_list: (Boolean) Determines whether the ACCOUNTADMIN, ORGADMIN, GLOBALORGADMIN, and SECURITYADMIN roles can be used as the primary role when creating a Snowflake session based on the access token from the External OAuth authorization server. For more information, check EXTERNAL_OAUTH_ADD_PRIVILEGED_ROLES_TO_BLOCKED_LIST docs.
            external_volume: '(String) Specifies the external volume for Apache Iceberg™ tables. For more information, see the Iceberg table documentation. Due to technical limitations (read more here), avoid using the following characters: |, ., ". For more information, check EXTERNAL_VOLUME docs.'
            feature_policy: (String) Specifies feature policy for the current account.
            geography_output_format: '(String) Display format for GEOGRAPHY values. Valid values are (case-insensitive): GeoJSON | WKT | WKB | EWKT | EWKB. For more information, check GEOGRAPHY_OUTPUT_FORMAT docs.'
            geometry_output_format: '(String) Display format for GEOMETRY values. Valid values are (case-insensitive): GeoJSON | WKT | WKB | EWKT | EWKB. For more information, check GEOMETRY_OUTPUT_FORMAT docs.'
            hybrid_table_lock_timeout: (Number) Number of seconds to wait while trying to acquire row-level locks on a hybrid table, before timing out and aborting the statement. For more information, check HYBRID_TABLE_LOCK_TIMEOUT docs.
            id: (String) The ID of this resource.
            initial_replication_size_limit_in_tb: (String) Sets the maximum estimated size limit for the initial replication of a primary database to a secondary database (in TB). Set this parameter on any account that stores a secondary database. This size limit helps prevent accounts from accidentally incurring large database replication charges. To remove the size limit, set the value to 0.0. It is required to pass numbers with scale of at least 1 (e.g. 20.5, 32.25, 33.333, etc.). For more information, check INITIAL_REPLICATION_SIZE_LIMIT_IN_TB docs.
            jdbc_treat_decimal_as_int: (Boolean) Specifies how JDBC processes columns that have a scale of zero (0). For more information, check JDBC_TREAT_DECIMAL_AS_INT docs.
            jdbc_treat_timestamp_ntz_as_utc: (Boolean) Specifies how JDBC processes TIMESTAMP_NTZ values (more details). For more information, check JDBC_TREAT_TIMESTAMP_NTZ_AS_UTC docs.
            jdbc_use_session_timezone: (Boolean) Specifies whether the JDBC Driver uses the time zone of the JVM or the time zone of the session (specified by the TIMEZONE parameter) for the getDate(), getTime(), and getTimestamp() methods of the ResultSet class. For more information, check JDBC_USE_SESSION_TIMEZONE docs.
            js_treat_integer_as_bigint: (Boolean) Specifies how the Snowflake Node.js Driver processes numeric columns that have a scale of zero (0), for example INTEGER or NUMBER(p, 0). For more information, check JS_TREAT_INTEGER_AS_BIGINT docs.
            json_indent: (Number) Specifies the number of blank spaces to indent each new element in JSON output in the session. Also specifies whether to insert newline characters after each element. For more information, check JSON_INDENT docs.
            listing_auto_fulfillment_replication_refresh_schedule: (String) Sets the time interval used to refresh the application package based data products to other regions. For more information, check LISTING_AUTO_FULFILLMENT_REPLICATION_REFRESH_SCHEDULE docs.
            lock_timeout: (Number) Number of seconds to wait while trying to lock a resource, before timing out and aborting the statement. For more information, check LOCK_TIMEOUT docs.
            log_level: '(String) Specifies the severity level of messages that should be ingested and made available in the active event table. Messages at the specified level (and at more severe levels) are ingested. For more information about log levels, see Setting levels for logging, metrics, and tracing. Valid values are (case-insensitive): TRACE | DEBUG | INFO | WARN | ERROR | FATAL | OFF. For more information, check LOG_LEVEL docs.'
            max_concurrency_level: (Number) Specifies the concurrency level for SQL statements (that is, queries and DML) executed by a warehouse (more details). For more information, check MAX_CONCURRENCY_LEVEL docs.
            max_data_extension_time_in_days: (Number) Maximum number of days Snowflake can extend the data retention period for tables to prevent streams on the tables from becoming stale. By default, if the DATA_RETENTION_TIME_IN_DAYS setting for a source table is less than 14 days, and a stream has not been consumed, Snowflake temporarily extends this period to the stream’s offset, up to a maximum of 14 days, regardless of the Snowflake Edition for your account. The MAX_DATA_EXTENSION_TIME_IN_DAYS parameter enables you to limit this automatic extension period to control storage costs for data retention or for compliance reasons. For more information, check MAX_DATA_EXTENSION_TIME_IN_DAYS docs.
            metric_level: '(String) Controls how metrics data is ingested into the event table. For more information about metric levels, see Setting levels for logging, metrics, and tracing. Valid values are (case-insensitive): ALL | NONE. For more information, check METRIC_LEVEL docs.'
            min_data_retention_time_in_days: (Number) Minimum number of days for which Snowflake retains historical data for performing Time Travel actions (SELECT, CLONE, UNDROP) on an object. If a minimum number of days for data retention is set on an account, the data retention period for an object is determined by MAX(DATA_RETENTION_TIME_IN_DAYS, MIN_DATA_RETENTION_TIME_IN_DAYS). For more information, check MIN_DATA_RETENTION_TIME_IN_DAYS docs.
            multi_statement_count: (Number) Number of statements to execute when using the multi-statement capability. For more information, check MULTI_STATEMENT_COUNT docs.
            network_policy: '(String) Specifies the network policy to enforce for your account. Network policies enable restricting access to your account based on users’ IP address. For more details, see Controlling network traffic with network policies. Due to technical limitations (read more here), avoid using the following characters: |, ., ". For more information, check NETWORK_POLICY docs.'
            noorder_sequence_as_default: (Boolean) Specifies whether the ORDER or NOORDER property is set by default when you create a new sequence or add a new table column. The ORDER and NOORDER properties determine whether or not the values are generated for the sequence or auto-incremented column in increasing or decreasing order. For more information, check NOORDER_SEQUENCE_AS_DEFAULT docs.
            oauth_add_privileged_roles_to_blocked_list: (Boolean) Determines whether the ACCOUNTADMIN, ORGADMIN, GLOBALORGADMIN, and SECURITYADMIN roles can be used as the primary role when creating a Snowflake session based on the access token from Snowflake’s authorization server. For more information, check OAUTH_ADD_PRIVILEGED_ROLES_TO_BLOCKED_LIST docs.
            odbc_treat_decimal_as_int: (Boolean) Specifies how ODBC processes columns that have a scale of zero (0). For more information, check ODBC_TREAT_DECIMAL_AS_INT docs.
            packages_policy: (String) Specifies packages policy for the current account.
            password_policy: (String) Specifies password policy for the current account. For more information about this resource, see docs.
            periodic_data_rekeying: (Boolean) It enables/disables re-encryption of table data with new keys on a yearly basis to provide additional levels of data protection (more details). For more information, check PERIODIC_DATA_REKEYING docs.
            pipe_execution_paused: (Boolean) Specifies whether to pause a running pipe, primarily in preparation for transferring ownership of the pipe to a different role (more details). For more information, check PIPE_EXECUTION_PAUSED docs.
            prevent_unload_to_inline_url: (Boolean) Specifies whether to prevent ad hoc data unload operations to external cloud storage locations (that is, COPY INTO location statements that specify the cloud storage URL and access settings directly in the statement). For an example, see Unloading data from a table directly to files in an external location. For more information, check PREVENT_UNLOAD_TO_INLINE_URL docs.
            prevent_unload_to_internal_stages: (Boolean) Specifies whether to prevent data unload operations to internal (Snowflake) stages using COPY INTO location statements. For more information, check PREVENT_UNLOAD_TO_INTERNAL_STAGES docs.
            python_profiler_modules: (String) Specifies the list of Python modules to include in a report when profiling Python handler code. For more information, check PYTHON_PROFILER_MODULES docs.
            python_profiler_target_stage: (String) Specifies the fully-qualified name of the stage in which to save a report when profiling Python handler code. For more information, check PYTHON_PROFILER_TARGET_STAGE docs.
            query_tag: (String) Optional string that can be used to tag queries and other SQL statements executed within a session. The tags are displayed in the output of the QUERY_HISTORY, QUERY_HISTORY_BY_* functions. For more information, check QUERY_TAG docs.
            quoted_identifiers_ignore_case: (Boolean) Specifies whether letters in double-quoted object identifiers are stored and resolved as uppercase letters. By default, Snowflake preserves the case of alphabetic characters when storing and resolving double-quoted identifiers (see Identifier resolution). You can use this parameter in situations in which third-party applications always use double quotes around identifiers. For more information, check QUOTED_IDENTIFIERS_IGNORE_CASE docs.
            read: (String)
            replace_invalid_characters: (Boolean) Specifies whether to replace invalid UTF-8 characters with the Unicode replacement character (�) in query results for Apache Iceberg™ tables that use an external catalog. For more information, check REPLACE_INVALID_CHARACTERS docs.
            require_storage_integration_for_stage_creation: (Boolean) Specifies whether to require a storage integration object as cloud credentials when creating a named external stage (using CREATE STAGE) to access a private cloud storage location. For more information, check REQUIRE_STORAGE_INTEGRATION_FOR_STAGE_CREATION docs.
            require_storage_integration_for_stage_operation: (Boolean) Specifies whether to require using a named external stage that references a storage integration object as cloud credentials when loading data from or unloading data to a private cloud storage location. For more information, check REQUIRE_STORAGE_INTEGRATION_FOR_STAGE_OPERATION docs.
            resource_monitor: (String) Parameter that specifies the name of the resource monitor used to control all virtual warehouses created in the account. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            rows_per_resultset: (Number) Specifies the maximum number of rows returned in a result set. A value of 0 specifies no maximum. For more information, check ROWS_PER_RESULTSET docs.
            s3_stage_vpce_dns_name: (String) Specifies the DNS name of an Amazon S3 interface endpoint. Requests sent to the internal stage of an account via AWS PrivateLink for Amazon S3 use this endpoint to connect. For more information, see Accessing Internal stages with dedicated interface endpoints. For more information, check S3_STAGE_VPCE_DNS_NAME docs.
            saml_identity_provider: (String) Enables federated authentication. This deprecated parameter enables federated authentication (more details). For more information, check SAML_IDENTITY_PROVIDER docs.
            search_path: (String) Specifies the path to search to resolve unqualified object names in queries. For more information, see Name resolution in queries. Comma-separated list of identifiers. An identifier can be a fully or partially qualified schema name. For more information, check SEARCH_PATH docs.
            serverless_task_max_statement_size: '(String) Specifies the maximum allowed warehouse size for Serverless tasks. Valid values are (case-insensitive): XSMALL | X-SMALL | SMALL | MEDIUM | LARGE | XLARGE | X-LARGE | XXLARGE | X2LARGE | 2X-LARGE | XXXLARGE | X3LARGE | 3X-LARGE | X4LARGE | 4X-LARGE | X5LARGE | 5X-LARGE | X6LARGE | 6X-LARGE. For more information, check SERVERLESS_TASK_MAX_STATEMENT_SIZE docs.'
            serverless_task_min_statement_size: '(String) Specifies the minimum allowed warehouse size for Serverless tasks. Valid values are (case-insensitive): XSMALL | X-SMALL | SMALL | MEDIUM | LARGE | XLARGE | X-LARGE | XXLARGE | X2LARGE | 2X-LARGE | XXXLARGE | X3LARGE | 3X-LARGE | X4LARGE | 4X-LARGE | X5LARGE | 5X-LARGE | X6LARGE | 6X-LARGE. For more information, check SERVERLESS_TASK_MIN_STATEMENT_SIZE docs.'
            session_policy: (String) Specifies session policy for the current account.
            simulated_data_sharing_consumer: (String) Specifies the name of a consumer account to simulate for testing/validating shared data, particularly shared secure views. When this parameter is set in a session, shared views return rows as if executed in the specified consumer account rather than the provider account. For more information, check SIMULATED_DATA_SHARING_CONSUMER docs.
            sso_login_page: (Boolean) This deprecated parameter disables preview mode for testing SSO (after enabling federated authentication) before rolling it out to users. For more information, check SSO_LOGIN_PAGE docs.
            statement_queued_timeout_in_seconds: (Number) Amount of time, in seconds, a SQL statement (query, DDL, DML, etc.) remains queued for a warehouse before it is canceled by the system. This parameter can be used in conjunction with the MAX_CONCURRENCY_LEVEL parameter to ensure a warehouse is never backlogged. For more information, check STATEMENT_QUEUED_TIMEOUT_IN_SECONDS docs.
            statement_timeout_in_seconds: (Number) Amount of time, in seconds, after which a running SQL statement (query, DDL, DML, etc.) is canceled by the system. For more information, check STATEMENT_TIMEOUT_IN_SECONDS docs.
            storage_serialization_policy: '(String) Specifies the storage serialization policy for Snowflake-managed Apache Iceberg™ tables. Valid values are (case-insensitive): COMPATIBLE | OPTIMIZED. For more information, check STORAGE_SERIALIZATION_POLICY docs.'
            strict_json_output: (Boolean) This parameter specifies whether JSON output in a session is compatible with the general standard (as described by http://json.org). By design, Snowflake allows JSON input that contains non-standard values; however, these non-standard values might result in Snowflake outputting JSON that is incompatible with other platforms and languages. This parameter, when enabled, ensures that Snowflake outputs valid/compatible JSON. For more information, check STRICT_JSON_OUTPUT docs.
            suspend_task_after_num_failures: (Number) Specifies the number of consecutive failed task runs after which the current task is suspended automatically. The default is 0 (no automatic suspension). For more information, check SUSPEND_TASK_AFTER_NUM_FAILURES docs.
            task_auto_retry_attempts: (Number) Specifies the number of automatic task graph retry attempts. If any task graphs complete in a FAILED state, Snowflake can automatically retry the task graphs from the last task in the graph that failed. For more information, check TASK_AUTO_RETRY_ATTEMPTS docs.
            time_input_format: (String) Specifies the input format for the TIME data type. For more information, see Date and time input and output formats. Any valid, supported time format or AUTO (AUTO specifies that Snowflake attempts to automatically detect the format of times stored in the system during the session). For more information, check TIME_INPUT_FORMAT docs.
            time_output_format: (String) Specifies the display format for the TIME data type. For more information, see Date and time input and output formats. For more information, check TIME_OUTPUT_FORMAT docs.
            timeouts: (Block, Optional) (see below for nested schema)
            timestamp_day_is_always_24h: (Boolean) Specifies whether the DATEADD function (and its aliases) always consider a day to be exactly 24 hours for expressions that span multiple days. For more information, check TIMESTAMP_DAY_IS_ALWAYS_24H docs.
            timestamp_input_format: (String) Specifies the input format for the TIMESTAMP data type alias. For more information, see Date and time input and output formats. Any valid, supported timestamp format or AUTO (AUTO specifies that Snowflake attempts to automatically detect the format of timestamps stored in the system during the session). For more information, check TIMESTAMP_INPUT_FORMAT docs.
            timestamp_ltz_output_format: (String) Specifies the display format for the TIMESTAMP_LTZ data type. If no format is specified, defaults to TIMESTAMP_OUTPUT_FORMAT. For more information, see Date and time input and output formats. For more information, check TIMESTAMP_LTZ_OUTPUT_FORMAT docs.
            timestamp_ntz_output_format: (String) Specifies the display format for the TIMESTAMP_NTZ data type. For more information, check TIMESTAMP_NTZ_OUTPUT_FORMAT docs.
            timestamp_output_format: (String) Specifies the display format for the TIMESTAMP data type alias. For more information, see Date and time input and output formats. For more information, check TIMESTAMP_OUTPUT_FORMAT docs.
            timestamp_type_mapping: '(String) Specifies the TIMESTAMP_* variation that the TIMESTAMP data type alias maps to. Valid values are (case-insensitive): TIMESTAMP_LTZ | TIMESTAMP_NTZ | TIMESTAMP_TZ. For more information, check TIMESTAMP_TYPE_MAPPING docs.'
            timestamp_tz_output_format: (String) Specifies the display format for the TIMESTAMP_TZ data type. If no format is specified, defaults to TIMESTAMP_OUTPUT_FORMAT. For more information, see Date and time input and output formats. For more information, check TIMESTAMP_TZ_OUTPUT_FORMAT docs.
            timezone: (String) Specifies the time zone for the session. You can specify a time zone name or a link name from release 2021a of the IANA Time Zone Database (e.g. America/Los_Angeles, Europe/London, UTC, Etc/GMT, etc.). For more information, check TIMEZONE docs.
            trace_level: '(String) Controls how trace events are ingested into the event table. For more information about trace levels, see Setting trace level. Valid values are (case-insensitive): ALWAYS | ON_EVENT | PROPAGATE | OFF. For more information, check TRACE_LEVEL docs.'
            transaction_abort_on_error: (Boolean) Specifies the action to perform when a statement issued within a non-autocommit transaction returns with an error. For more information, check TRANSACTION_ABORT_ON_ERROR docs.
            transaction_default_isolation_level: '(String) Specifies the isolation level for transactions in the user session. Valid values are (case-insensitive): READ COMMITTED. For more information, check TRANSACTION_DEFAULT_ISOLATION_LEVEL docs.'
            two_digit_century_start: (Number) Specifies the “century start” year for 2-digit years (i.e. the earliest year such dates can represent). This parameter prevents ambiguous dates when importing or converting data with the YY date format component (i.e. years represented as 2 digits). For more information, check TWO_DIGIT_CENTURY_START docs.
            unsupported_ddl_action: (String) Determines if an unsupported (i.e. non-default) value specified for a constraint property returns an error. For more information, check UNSUPPORTED_DDL_ACTION docs.
            update: (String)
            use_cached_result: (Boolean) Specifies whether to reuse persisted query results, if available, when a matching query is submitted. For more information, check USE_CACHED_RESULT docs.
            user_task_managed_initial_warehouse_size: '(String) Specifies the size of the compute resources to provision for the first run of the task, before a task history is available for Snowflake to determine an ideal size. Once a task has successfully completed a few runs, Snowflake ignores this parameter setting. Valid values are (case-insensitive): XSMALL | X-SMALL | SMALL | MEDIUM | LARGE | XLARGE | X-LARGE | XXLARGE | X2LARGE | 2X-LARGE | XXXLARGE | X3LARGE | 3X-LARGE | X4LARGE | 4X-LARGE | X5LARGE | 5X-LARGE | X6LARGE | 6X-LARGE. For more information, check USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE docs.'
            user_task_minimum_trigger_interval_in_seconds: (Number) Minimum amount of time between Triggered Task executions in seconds For more information, check USER_TASK_MINIMUM_TRIGGER_INTERVAL_IN_SECONDS docs.
            user_task_timeout_ms: (Number) Specifies the time limit on a single run of the task before it times out (in milliseconds). For more information, check USER_TASK_TIMEOUT_MS docs.
            week_of_year_policy: '(Number) Specifies how the weeks in a given year are computed. 0: The semantics used are equivalent to the ISO semantics, in which a week belongs to a given year if at least 4 days of that week are in that year. 1: January 1 is included in the first week of the year and December 31 is included in the last week of the year. For more information, check WEEK_OF_YEAR_POLICY docs.'
            week_start: '(Number) Specifies the first day of the week (used by week-related date functions). 0: Legacy Snowflake behavior is used (i.e. ISO-like semantics). 1 (Monday) to 7 (Sunday): All the week-related functions use weeks that start on the specified day of the week. For more information, check WEEK_START docs.'
        importStatements:
            - |-
              # This resource may contain a any identifier, but the following format is recommended.
              terraform import snowflake_current_account.example 'current_account'
    snowflake_current_organization_account:
        subCategory: Preview
        description: Resource used to manage an organization account within the organization you are connected to. See ALTER ORGANIZATION ACCOUNT https://docs.snowflake.com/en/sql-reference/sql/alter-organization-account documentation for more information on resource capabilities.
        name: snowflake_current_organization_account
        title: snowflake_current_organization_account Resource - terraform-provider-snowflake
        examples:
            - name: minimal
              manifest: '{}'
            - name: complete
              manifest: |-
                {
                  "abort_detached_query": true,
                  "allow_client_mfa_caching": true,
                  "allow_id_token": true,
                  "autocommit": false,
                  "base_location_prefix": "STORAGE_BASE_URL/",
                  "binary_input_format": "BASE64",
                  "binary_output_format": "BASE64",
                  "catalog": "SNOWFLAKE",
                  "client_enable_log_info_statement_parameters": true,
                  "client_encryption_key_size": 256,
                  "client_memory_limit": 1540,
                  "client_metadata_request_use_connection_ctx": true,
                  "client_metadata_use_session_database": true,
                  "client_prefetch_threads": 5,
                  "client_result_chunk_size": 159,
                  "client_result_column_case_insensitive": true,
                  "client_session_keep_alive": true,
                  "client_session_keep_alive_heartbeat_frequency": 3599,
                  "client_timestamp_type_mapping": "TIMESTAMP_NTZ",
                  "comment": "This is a comment for the current organization account resource",
                  "cortex_enabled_cross_region": "ANY_REGION",
                  "cortex_models_allowlist": "All",
                  "csv_timestamp_format": "YYYY-MM-DD",
                  "data_retention_time_in_days": 2,
                  "date_input_format": "YYYY-MM-DD",
                  "date_output_format": "YYYY-MM-DD",
                  "default_ddl_collation": "en-cs",
                  "default_notebook_compute_pool_cpu": "CPU_X64_S",
                  "default_notebook_compute_pool_gpu": "GPU_NV_S",
                  "default_null_ordering": "FIRST",
                  "default_streamlit_notebook_warehouse": "${snowflake_warehouse.example.fully_qualified_name}",
                  "disable_ui_download_button": true,
                  "disable_user_privilege_grants": true,
                  "enable_automatic_sensitive_data_classification_log": false,
                  "enable_egress_cost_optimizer": false,
                  "enable_identifier_first_login": false,
                  "enable_tri_secret_and_rekey_opt_out_for_image_repository": true,
                  "enable_tri_secret_and_rekey_opt_out_for_spcs_block_storage": true,
                  "enable_unhandled_exceptions_reporting": false,
                  "enable_unload_physical_type_optimization": false,
                  "enable_unredacted_query_syntax_error": true,
                  "enable_unredacted_secure_object_error": true,
                  "enforce_network_rules_for_internal_stages": true,
                  "error_on_nondeterministic_merge": false,
                  "error_on_nondeterministic_update": true,
                  "event_table": "\"\u003cdatabase_name\u003e\".\"\u003cschema_name\u003e\".\"\u003cevent_table_name\u003e\"",
                  "external_oauth_add_privileged_roles_to_blocked_list": false,
                  "external_volume": "XWDVEAAT_A6FEE9D6_5D41_AB3D_EB0C_51DA5E5F0BE2",
                  "geography_output_format": "WKT",
                  "geometry_output_format": "WKT",
                  "hybrid_table_lock_timeout": 3599,
                  "initial_replication_size_limit_in_tb": "9.9",
                  "jdbc_treat_decimal_as_int": false,
                  "jdbc_treat_timestamp_ntz_as_utc": true,
                  "jdbc_use_session_timezone": false,
                  "js_treat_integer_as_bigint": true,
                  "json_indent": 4,
                  "listing_auto_fulfillment_replication_refresh_schedule": "2 minutes",
                  "lock_timeout": 43201,
                  "log_level": "INFO",
                  "max_concurrency_level": 7,
                  "max_data_extension_time_in_days": 13,
                  "metric_level": "ALL",
                  "min_data_retention_time_in_days": 1,
                  "multi_statement_count": 0,
                  "network_policy": "${snowflake_network_policy.example.fully_qualified_name}",
                  "noorder_sequence_as_default": false,
                  "oauth_add_privileged_roles_to_blocked_list": false,
                  "odbc_treat_decimal_as_int": true,
                  "password_policy": "${snowflake_password_policy.example.fully_qualified_name}",
                  "periodic_data_rekeying": false,
                  "pipe_execution_paused": true,
                  "prevent_unload_to_inline_url": true,
                  "prevent_unload_to_internal_stages": true,
                  "python_profiler_target_stage": "${snowflake_stage.example.fully_qualified_name}",
                  "query_tag": "test-query-tag",
                  "quoted_identifiers_ignore_case": true,
                  "replace_invalid_characters": true,
                  "require_storage_integration_for_stage_creation": true,
                  "require_storage_integration_for_stage_operation": true,
                  "resource_monitor": "${snowflake_resource_monitor.example.fully_qualified_name}",
                  "rows_per_resultset": 1000,
                  "search_path": "$current, $public",
                  "serverless_task_max_statement_size": "XLARGE",
                  "serverless_task_min_statement_size": "SMALL",
                  "session_policy": "\"\u003cdatabase_name\u003e\".\"\u003cschema_name\u003e\".\"\u003csession_policy_name\u003e\"",
                  "sso_login_page": true,
                  "statement_queued_timeout_in_seconds": 1,
                  "statement_timeout_in_seconds": 1,
                  "storage_serialization_policy": "OPTIMIZED",
                  "strict_json_output": true,
                  "suspend_task_after_num_failures": 3,
                  "task_auto_retry_attempts": 3,
                  "time_input_format": "YYYY-MM-DD",
                  "time_output_format": "YYYY-MM-DD",
                  "timestamp_day_is_always_24h": true,
                  "timestamp_input_format": "YYYY-MM-DD",
                  "timestamp_ltz_output_format": "YYYY-MM-DD",
                  "timestamp_ntz_output_format": "YYYY-MM-DD",
                  "timestamp_output_format": "YYYY-MM-DD",
                  "timestamp_type_mapping": "TIMESTAMP_LTZ",
                  "timestamp_tz_output_format": "YYYY-MM-DD",
                  "timezone": "Europe/London",
                  "trace_level": "PROPAGATE",
                  "transaction_abort_on_error": true,
                  "transaction_default_isolation_level": "READ COMMITTED",
                  "two_digit_century_start": 1971,
                  "unsupported_ddl_action": "FAIL",
                  "use_cached_result": false,
                  "user_task_managed_initial_warehouse_size": "SMALL",
                  "user_task_minimum_trigger_interval_in_seconds": 10,
                  "user_task_timeout_ms": 10,
                  "week_of_year_policy": 1,
                  "week_start": 1
                }
              references:
                default_streamlit_notebook_warehouse: snowflake_warehouse.example.fully_qualified_name
                network_policy: snowflake_network_policy.example.fully_qualified_name
                password_policy: snowflake_password_policy.example.fully_qualified_name
                python_profiler_target_stage: snowflake_stage.example.fully_qualified_name
                resource_monitor: snowflake_resource_monitor.example.fully_qualified_name
        argumentDocs:
            abort_detached_query: (Boolean) Specifies the action that Snowflake performs for in-progress queries if connectivity is lost due to abrupt termination of a session (e.g. network outage, browser termination, service interruption). For more information, check ABORT_DETACHED_QUERY docs.
            account_locator: (String)
            account_locator_url: (String)
            account_name: (String)
            account_old_url_last_used: (String)
            account_old_url_saved_on: (String)
            account_url: (String)
            active_python_profiler: '(String) Sets the profiler to use for the session when profiling Python handler code. Valid values are (case-insensitive): LINE | MEMORY. For more information, check ACTIVE_PYTHON_PROFILER docs.'
            allow_client_mfa_caching: (Boolean) Specifies whether an MFA token can be saved in the client-side operating system keystore to promote continuous, secure connectivity without users needing to respond to an MFA prompt at the start of each connection attempt to Snowflake. For details and the list of supported Snowflake-provided clients, see Using MFA token caching to minimize the number of prompts during authentication — optional. For more information, check ALLOW_CLIENT_MFA_CACHING docs.
            allow_id_token: (Boolean) Specifies whether a connection token can be saved in the client-side operating system keystore to promote continuous, secure connectivity without users needing to enter login credentials at the start of each connection attempt to Snowflake. For details and the list of supported Snowflake-provided clients, see Using connection caching to minimize the number of prompts for authentication — optional. For more information, check ALLOW_ID_TOKEN docs.
            autocommit: (Boolean) Specifies whether autocommit is enabled for the session. Autocommit determines whether a DML statement, when executed without an active transaction, is automatically committed after the statement successfully completes. For more information, see Transactions. For more information, check AUTOCOMMIT docs.
            base_location_prefix: (String) Specifies a prefix for Snowflake to use in the write path for Snowflake-managed Apache Iceberg™ tables. For more information, see data and metadata directories for Iceberg tables. For more information, check BASE_LOCATION_PREFIX docs.
            binary_input_format: '(String) The format of VARCHAR values passed as input to VARCHAR-to-BINARY conversion functions. For more information, see Binary input and output. Valid values are (case-insensitive): HEX | BASE64 | UTF8. For more information, check BINARY_INPUT_FORMAT docs.'
            binary_output_format: '(String) The format for VARCHAR values returned as output by BINARY-to-VARCHAR conversion functions. For more information, see Binary input and output. Valid values are (case-insensitive): HEX | BASE64. For more information, check BINARY_OUTPUT_FORMAT docs.'
            catalog: '(String) Specifies the catalog for Apache Iceberg™ tables. For more information, see the Iceberg table documentation. Due to technical limitations (read more here), avoid using the following characters: |, ., ". For more information, check CATALOG docs.'
            catalog_sync: '(String) Specifies the name of your catalog integration for Snowflake Open Catalog. Snowflake syncs tables that use the specified catalog integration with your Snowflake Open Catalog account. For more information, see Sync a Snowflake-managed table with Snowflake Open Catalog. Due to technical limitations (read more here), avoid using the following characters: |, ., ". For more information, check CATALOG_SYNC docs.'
            client_enable_log_info_statement_parameters: (Boolean) Enables users to log the data values bound to PreparedStatements (more details). For more information, check CLIENT_ENABLE_LOG_INFO_STATEMENT_PARAMETERS docs.
            client_encryption_key_size: (Number) Specifies the AES encryption key size, in bits, used by Snowflake to encrypt/decrypt files stored on internal stages (for loading/unloading data) when you use the SNOWFLAKE_FULL encryption type. For more information, check CLIENT_ENCRYPTION_KEY_SIZE docs.
            client_memory_limit: (Number) Parameter that specifies the maximum amount of memory the JDBC driver or ODBC driver should use for the result set from queries (in MB). For more information, check CLIENT_MEMORY_LIMIT docs.
            client_metadata_request_use_connection_ctx: (Boolean) For specific ODBC functions and JDBC methods, this parameter can change the default search scope from all databases/schemas to the current database/schema. The narrower search typically returns fewer rows and executes more quickly. For more information, check CLIENT_METADATA_REQUEST_USE_CONNECTION_CTX docs.
            client_metadata_use_session_database: (Boolean) For specific ODBC functions and JDBC methods, this parameter can change the default search scope from all databases to the current database. The narrower search typically returns fewer rows and executes more quickly (more details on the usage). For more information, check CLIENT_METADATA_USE_SESSION_DATABASE docs.
            client_prefetch_threads: (Number) Parameter that specifies the number of threads used by the client to pre-fetch large result sets. The driver will attempt to honor the parameter value, but defines the minimum and maximum values (depending on your system’s resources) to improve performance. For more information, check CLIENT_PREFETCH_THREADS docs.
            client_result_chunk_size: (Number) Parameter that specifies the maximum size of each set (or chunk) of query results to download (in MB). The JDBC driver downloads query results in chunks. For more information, check CLIENT_RESULT_CHUNK_SIZE docs.
            client_result_column_case_insensitive: (Boolean) Parameter that indicates whether to match column name case-insensitively in ResultSet.get* methods in JDBC. For more information, check CLIENT_RESULT_COLUMN_CASE_INSENSITIVE docs.
            client_session_keep_alive: (Boolean) Parameter that indicates whether to force a user to log in again after a period of inactivity in the session. For more information, check CLIENT_SESSION_KEEP_ALIVE docs.
            client_session_keep_alive_heartbeat_frequency: (Number) Number of seconds in-between client attempts to update the token for the session. For more information, check CLIENT_SESSION_KEEP_ALIVE_HEARTBEAT_FREQUENCY docs.
            client_timestamp_type_mapping: '(String) Specifies the TIMESTAMP_* variation to use when binding timestamp variables for JDBC or ODBC applications that use the bind API to load data. Valid values are (case-insensitive): TIMESTAMP_LTZ | TIMESTAMP_NTZ. For more information, check CLIENT_TIMESTAMP_TYPE_MAPPING docs.'
            comment: (String) Specifies a comment for the organization account.
            consumption_billing_entity_name: (String)
            cortex_enabled_cross_region: (String) Specifies the regions where an inference request may be processed in case the request cannot be processed in the region where request is originally placed. Specifying DISABLED disables cross-region inferencing. For examples and details, see Cross-region inference. For more information, check CORTEX_ENABLED_CROSS_REGION docs.
            cortex_models_allowlist: (String) Specifies the models that users in the account can access. Use this parameter to allowlist models for all users in the account. If you need to provide specific users with access beyond what you’ve specified in the allowlist, use role-based access control instead. For more information, see Model allowlist. For more information, check CORTEX_MODELS_ALLOWLIST docs.
            create: (String)
            created_on: (String)
            csv_timestamp_format: (String) Specifies the format for TIMESTAMP values in CSV files downloaded from Snowsight. If this parameter is not set, TIMESTAMP_LTZ_OUTPUT_FORMAT will be used for TIMESTAMP_LTZ values, TIMESTAMP_TZ_OUTPUT_FORMAT will be used for TIMESTAMP_TZ and TIMESTAMP_NTZ_OUTPUT_FORMAT for TIMESTAMP_NTZ values. For more information, see Date and time input and output formats or Download your query results. For more information, check CSV_TIMESTAMP_FORMAT docs.
            data_retention_time_in_days: (Number) Number of days for which Snowflake retains historical data for performing Time Travel actions (SELECT, CLONE, UNDROP) on the object. A value of 0 effectively disables Time Travel for the specified database, schema, or table. For more information, see Understanding & using Time Travel. For more information, check DATA_RETENTION_TIME_IN_DAYS docs.
            date_input_format: (String) Specifies the input format for the DATE data type. For more information, see Date and time input and output formats. For more information, check DATE_INPUT_FORMAT docs.
            date_output_format: (String) Specifies the display format for the DATE data type. For more information, see Date and time input and output formats. For more information, check DATE_OUTPUT_FORMAT docs.
            default_ddl_collation: '(String) Sets the default collation used for the following DDL operations: CREATE TABLE, ALTER TABLE … ADD COLUMN. Setting this parameter forces all subsequently-created columns in the affected objects (table, schema, database, or account) to have the specified collation as the default, unless the collation for the column is explicitly defined in the DDL. For more information, check DEFAULT_DDL_COLLATION docs.'
            default_notebook_compute_pool_cpu: '(String) Sets the preferred CPU compute pool used for Notebooks on CPU Container Runtime. Due to technical limitations (read more here), avoid using the following characters: |, ., ". For more information, check DEFAULT_NOTEBOOK_COMPUTE_POOL_CPU docs.'
            default_notebook_compute_pool_gpu: '(String) Sets the preferred GPU compute pool used for Notebooks on GPU Container Runtime. Due to technical limitations (read more here), avoid using the following characters: |, ., ". For more information, check DEFAULT_NOTEBOOK_COMPUTE_POOL_GPU docs.'
            default_null_ordering: '(String) Specifies the default ordering of NULL values in a result set (more details). Valid values are (case-insensitive): FIRST | LAST. For more information, check DEFAULT_NULL_ORDERING docs.'
            default_streamlit_notebook_warehouse: '(String) Specifies the name of the default warehouse to use when creating a notebook. Due to technical limitations (read more here), avoid using the following characters: |, ., ". For more information, check DEFAULT_STREAMLIT_NOTEBOOK_WAREHOUSE docs.'
            delete: (String)
            disable_ui_download_button: (Boolean) Controls whether users in an account see a button to download data in Snowsight or the Classic Console, such as a table returned from running a query in a worksheet. If the button to download is hidden in Snowsight or the Classic Console, users can still download or export data using third-party software. For more information, check DISABLE_UI_DOWNLOAD_BUTTON docs.
            disable_user_privilege_grants: (Boolean) Controls whether users in an account can grant privileges directly to other users. Disabling user privilege grants (that is, setting DISABLE_USER_PRIVILEGE_GRANTS to TRUE) does not affect existing grants to users. Existing grants to users continue to confer privileges to those users. For more information, see GRANT . For more information, check DISABLE_USER_PRIVILEGE_GRANTS docs.
            edition: (String)
            enable_automatic_sensitive_data_classification_log: (Boolean) Controls whether events from automatic sensitive data classification are logged in the user event table. For more information, check ENABLE_AUTOMATIC_SENSITIVE_DATA_CLASSIFICATION_LOG docs.
            enable_egress_cost_optimizer: (Boolean) Enables or disables the Listing Cross-cloud auto-fulfillment Egress cost optimizer. For more information, check ENABLE_EGRESS_COST_OPTIMIZER docs.
            enable_identifier_first_login: (Boolean) Determines the login flow for users. When enabled, Snowflake prompts users for their username or email address before presenting authentication methods. For details, see Identifier-first login. For more information, check ENABLE_IDENTIFIER_FIRST_LOGIN docs.
            enable_internal_stages_privatelink: (Boolean) Specifies whether the SYSTEM$GET_PRIVATELINK_CONFIG function returns the private-internal-stages key in the query result. The corresponding value in the query result is used during the configuration process for private connectivity to internal stages. For more information, check ENABLE_INTERNAL_STAGES_PRIVATELINK docs.
            enable_tri_secret_and_rekey_opt_out_for_image_repository: (Boolean) Specifies choice for the image repository to opt out of Tri-Secret Secure and Periodic rekeying. For more information, check ENABLE_TRI_SECRET_AND_REKEY_OPT_OUT_FOR_IMAGE_REPOSITORY docs.
            enable_tri_secret_and_rekey_opt_out_for_spcs_block_storage: (Boolean) Specifies the choice for the Snowpark Container Services block storage volume to opt out of Tri-Secret Secure and Periodic rekeying. For more information, check ENABLE_TRI_SECRET_AND_REKEY_OPT_OUT_FOR_SPCS_BLOCK_STORAGE docs.
            enable_unhandled_exceptions_reporting: (Boolean) Specifies whether Snowflake may capture – in an event table – log messages or trace event data for unhandled exceptions in procedure or UDF handler code. For more information, see Capturing messages from unhandled exceptions. For more information, check ENABLE_UNHANDLED_EXCEPTIONS_REPORTING docs.
            enable_unload_physical_type_optimization: (Boolean) Specifies whether to set the schema for unloaded Parquet files based on the logical column data types (i.e. the types in the unload SQL query or source table) or on the unloaded column values (i.e. the smallest data types and precision that support the values in the output columns of the unload SQL statement or source table). For more information, check ENABLE_UNLOAD_PHYSICAL_TYPE_OPTIMIZATION docs.
            enable_unredacted_query_syntax_error: (Boolean) Controls whether query text is redacted if a SQL query fails due to a syntax or parsing error. If FALSE, the content of a failed query is redacted in the views, pages, and functions that provide a query history. Only users with a role that is granted or inherits the AUDIT privilege can set the ENABLE_UNREDACTED_QUERY_SYNTAX_ERROR parameter. When using the ALTER USER command to set the parameter to TRUE for a particular user, modify the user that you want to see the query text, not the user who executed the query (if those are different users). For more information, check ENABLE_UNREDACTED_QUERY_SYNTAX_ERROR docs.
            enable_unredacted_secure_object_error: '(Boolean) Controls whether error messages related to secure objects are redacted in metadata. For more information, see Secure objects: Redaction of information in error messages. Only users with a role that is granted or inherits the AUDIT privilege can set the ENABLE_UNREDACTED_SECURE_OBJECT_ERROR parameter. When using the ALTER USER command to set the parameter to TRUE for a particular user, modify the user that you want to see the redacted error messages in metadata, not the user who caused the error. For more information, check ENABLE_UNREDACTED_SECURE_OBJECT_ERROR docs.'
            enforce_network_rules_for_internal_stages: (Boolean) Specifies whether a network policy that uses network rules can restrict access to AWS internal stages. This parameter has no effect on network policies that do not use network rules. This account-level parameter affects both account-level and user-level network policies. For details about using network policies and network rules to restrict access to AWS internal stages, including the use of this parameter, see Protecting internal stages on AWS. For more information, check ENFORCE_NETWORK_RULES_FOR_INTERNAL_STAGES docs.
            error_on_nondeterministic_merge: (Boolean) Specifies whether to return an error when the MERGE command is used to update or delete a target row that joins multiple source rows and the system cannot determine the action to perform on the target row. For more information, check ERROR_ON_NONDETERMINISTIC_MERGE docs.
            error_on_nondeterministic_update: (Boolean) Specifies whether to return an error when the UPDATE command is used to update a target row that joins multiple source rows and the system cannot determine the action to perform on the target row. For more information, check ERROR_ON_NONDETERMINISTIC_UPDATE docs.
            event_table: '(String) Specifies the name of the event table for logging messages from stored procedures and UDFs contained by the object with which the event table is associated. Associating an event table with a database is available in Enterprise Edition or higher. Due to technical limitations (read more here), avoid using the following characters: |, ., ". For more information, check EVENT_TABLE docs.'
            external_oauth_add_privileged_roles_to_blocked_list: (Boolean) Determines whether the ACCOUNTADMIN, ORGADMIN, GLOBALORGADMIN, and SECURITYADMIN roles can be used as the primary role when creating a Snowflake session based on the access token from the External OAuth authorization server. For more information, check EXTERNAL_OAUTH_ADD_PRIVILEGED_ROLES_TO_BLOCKED_LIST docs.
            external_volume: '(String) Specifies the external volume for Apache Iceberg™ tables. For more information, see the Iceberg table documentation. Due to technical limitations (read more here), avoid using the following characters: |, ., ". For more information, check EXTERNAL_VOLUME docs.'
            geography_output_format: '(String) Display format for GEOGRAPHY values. Valid values are (case-insensitive): GeoJSON | WKT | WKB | EWKT | EWKB. For more information, check GEOGRAPHY_OUTPUT_FORMAT docs.'
            geometry_output_format: '(String) Display format for GEOMETRY values. Valid values are (case-insensitive): GeoJSON | WKT | WKB | EWKT | EWKB. For more information, check GEOMETRY_OUTPUT_FORMAT docs.'
            hybrid_table_lock_timeout: (Number) Number of seconds to wait while trying to acquire row-level locks on a hybrid table, before timing out and aborting the statement. For more information, check HYBRID_TABLE_LOCK_TIMEOUT docs.
            id: (String) The ID of this resource.
            initial_replication_size_limit_in_tb: (String) Sets the maximum estimated size limit for the initial replication of a primary database to a secondary database (in TB). Set this parameter on any account that stores a secondary database. This size limit helps prevent accounts from accidentally incurring large database replication charges. To remove the size limit, set the value to 0.0. It is required to pass numbers with scale of at least 1 (e.g. 20.5, 32.25, 33.333, etc.). For more information, check INITIAL_REPLICATION_SIZE_LIMIT_IN_TB docs.
            is_events_account: (Boolean)
            is_org_admin: (Boolean)
            is_organization_account: (Boolean)
            jdbc_treat_decimal_as_int: (Boolean) Specifies how JDBC processes columns that have a scale of zero (0). For more information, check JDBC_TREAT_DECIMAL_AS_INT docs.
            jdbc_treat_timestamp_ntz_as_utc: (Boolean) Specifies how JDBC processes TIMESTAMP_NTZ values (more details). For more information, check JDBC_TREAT_TIMESTAMP_NTZ_AS_UTC docs.
            jdbc_use_session_timezone: (Boolean) Specifies whether the JDBC Driver uses the time zone of the JVM or the time zone of the session (specified by the TIMEZONE parameter) for the getDate(), getTime(), and getTimestamp() methods of the ResultSet class. For more information, check JDBC_USE_SESSION_TIMEZONE docs.
            js_treat_integer_as_bigint: (Boolean) Specifies how the Snowflake Node.js Driver processes numeric columns that have a scale of zero (0), for example INTEGER or NUMBER(p, 0). For more information, check JS_TREAT_INTEGER_AS_BIGINT docs.
            json_indent: (Number) Specifies the number of blank spaces to indent each new element in JSON output in the session. Also specifies whether to insert newline characters after each element. For more information, check JSON_INDENT docs.
            listing_auto_fulfillment_replication_refresh_schedule: (String) Sets the time interval used to refresh the application package based data products to other regions. For more information, check LISTING_AUTO_FULFILLMENT_REPLICATION_REFRESH_SCHEDULE docs.
            lock_timeout: (Number) Number of seconds to wait while trying to lock a resource, before timing out and aborting the statement. For more information, check LOCK_TIMEOUT docs.
            log_level: '(String) Specifies the severity level of messages that should be ingested and made available in the active event table. Messages at the specified level (and at more severe levels) are ingested. For more information about log levels, see Setting levels for logging, metrics, and tracing. Valid values are (case-insensitive): TRACE | DEBUG | INFO | WARN | ERROR | FATAL | OFF. For more information, check LOG_LEVEL docs.'
            managed_accounts: (Number)
            marketplace_consumer_billing_entity_name: (String)
            marketplace_provider_billing_entity_name: (String)
            max_concurrency_level: (Number) Specifies the concurrency level for SQL statements (that is, queries and DML) executed by a warehouse (more details). For more information, check MAX_CONCURRENCY_LEVEL docs.
            max_data_extension_time_in_days: (Number) Maximum number of days Snowflake can extend the data retention period for tables to prevent streams on the tables from becoming stale. By default, if the DATA_RETENTION_TIME_IN_DAYS setting for a source table is less than 14 days, and a stream has not been consumed, Snowflake temporarily extends this period to the stream’s offset, up to a maximum of 14 days, regardless of the Snowflake Edition for your account. The MAX_DATA_EXTENSION_TIME_IN_DAYS parameter enables you to limit this automatic extension period to control storage costs for data retention or for compliance reasons. For more information, check MAX_DATA_EXTENSION_TIME_IN_DAYS docs.
            metric_level: '(String) Controls how metrics data is ingested into the event table. For more information about metric levels, see Setting levels for logging, metrics, and tracing. Valid values are (case-insensitive): ALL | NONE. For more information, check METRIC_LEVEL docs.'
            min_data_retention_time_in_days: (Number) Minimum number of days for which Snowflake retains historical data for performing Time Travel actions (SELECT, CLONE, UNDROP) on an object. If a minimum number of days for data retention is set on an account, the data retention period for an object is determined by MAX(DATA_RETENTION_TIME_IN_DAYS, MIN_DATA_RETENTION_TIME_IN_DAYS). For more information, check MIN_DATA_RETENTION_TIME_IN_DAYS docs.
            multi_statement_count: (Number) Number of statements to execute when using the multi-statement capability. For more information, check MULTI_STATEMENT_COUNT docs.
            name: (String) The identifier (i.e. name) for the organization account within currently used organization. The field name is validated during import and create operations to ensure that it matches the current organization account name.
            network_policy: '(String) Specifies the network policy to enforce for your account. Network policies enable restricting access to your account based on users’ IP address. For more details, see Controlling network traffic with network policies. Due to technical limitations (read more here), avoid using the following characters: |, ., ". For more information, check NETWORK_POLICY docs.'
            noorder_sequence_as_default: (Boolean) Specifies whether the ORDER or NOORDER property is set by default when you create a new sequence or add a new table column. The ORDER and NOORDER properties determine whether or not the values are generated for the sequence or auto-incremented column in increasing or decreasing order. For more information, check NOORDER_SEQUENCE_AS_DEFAULT docs.
            oauth_add_privileged_roles_to_blocked_list: (Boolean) Determines whether the ACCOUNTADMIN, ORGADMIN, GLOBALORGADMIN, and SECURITYADMIN roles can be used as the primary role when creating a Snowflake session based on the access token from Snowflake’s authorization server. For more information, check OAUTH_ADD_PRIVILEGED_ROLES_TO_BLOCKED_LIST docs.
            odbc_treat_decimal_as_int: (Boolean) Specifies how ODBC processes columns that have a scale of zero (0). For more information, check ODBC_TREAT_DECIMAL_AS_INT docs.
            old_account_url: (String)
            organization_name: (String)
            organization_old_url: (String)
            organization_old_url_last_used: (String)
            organization_old_url_saved_on: (String)
            password_policy: (String) Specifies password policy for the current account. For more information about this resource, see docs.
            periodic_data_rekeying: (Boolean) It enables/disables re-encryption of table data with new keys on a yearly basis to provide additional levels of data protection (more details). For more information, check PERIODIC_DATA_REKEYING docs.
            pipe_execution_paused: (Boolean) Specifies whether to pause a running pipe, primarily in preparation for transferring ownership of the pipe to a different role (more details). For more information, check PIPE_EXECUTION_PAUSED docs.
            prevent_unload_to_inline_url: (Boolean) Specifies whether to prevent ad hoc data unload operations to external cloud storage locations (that is, COPY INTO location statements that specify the cloud storage URL and access settings directly in the statement). For an example, see Unloading data from a table directly to files in an external location. For more information, check PREVENT_UNLOAD_TO_INLINE_URL docs.
            prevent_unload_to_internal_stages: (Boolean) Specifies whether to prevent data unload operations to internal (Snowflake) stages using COPY INTO location statements. For more information, check PREVENT_UNLOAD_TO_INTERNAL_STAGES docs.
            python_profiler_modules: (String) Specifies the list of Python modules to include in a report when profiling Python handler code. For more information, check PYTHON_PROFILER_MODULES docs.
            python_profiler_target_stage: (String) Specifies the fully-qualified name of the stage in which to save a report when profiling Python handler code. For more information, check PYTHON_PROFILER_TARGET_STAGE docs.
            query_tag: (String) Optional string that can be used to tag queries and other SQL statements executed within a session. The tags are displayed in the output of the QUERY_HISTORY, QUERY_HISTORY_BY_* functions. For more information, check QUERY_TAG docs.
            quoted_identifiers_ignore_case: (Boolean) Specifies whether letters in double-quoted object identifiers are stored and resolved as uppercase letters. By default, Snowflake preserves the case of alphabetic characters when storing and resolving double-quoted identifiers (see Identifier resolution). You can use this parameter in situations in which third-party applications always use double quotes around identifiers. For more information, check QUOTED_IDENTIFIERS_IGNORE_CASE docs.
            read: (String)
            replace_invalid_characters: (Boolean) Specifies whether to replace invalid UTF-8 characters with the Unicode replacement character (�) in query results for Apache Iceberg™ tables that use an external catalog. For more information, check REPLACE_INVALID_CHARACTERS docs.
            require_storage_integration_for_stage_creation: (Boolean) Specifies whether to require a storage integration object as cloud credentials when creating a named external stage (using CREATE STAGE) to access a private cloud storage location. For more information, check REQUIRE_STORAGE_INTEGRATION_FOR_STAGE_CREATION docs.
            require_storage_integration_for_stage_operation: (Boolean) Specifies whether to require using a named external stage that references a storage integration object as cloud credentials when loading data from or unloading data to a private cloud storage location. For more information, check REQUIRE_STORAGE_INTEGRATION_FOR_STAGE_OPERATION docs.
            resource_monitor: (String) Parameter that specifies the name of the resource monitor used to control all virtual warehouses created in the account. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            rows_per_resultset: (Number) Specifies the maximum number of rows returned in a result set. A value of 0 specifies no maximum. For more information, check ROWS_PER_RESULTSET docs.
            s3_stage_vpce_dns_name: (String) Specifies the DNS name of an Amazon S3 interface endpoint. Requests sent to the internal stage of an account via AWS PrivateLink for Amazon S3 use this endpoint to connect. For more information, see Accessing Internal stages with dedicated interface endpoints. For more information, check S3_STAGE_VPCE_DNS_NAME docs.
            saml_identity_provider: (String) Enables federated authentication. This deprecated parameter enables federated authentication (more details). For more information, check SAML_IDENTITY_PROVIDER docs.
            search_path: (String) Specifies the path to search to resolve unqualified object names in queries. For more information, see Name resolution in queries. Comma-separated list of identifiers. An identifier can be a fully or partially qualified schema name. For more information, check SEARCH_PATH docs.
            serverless_task_max_statement_size: '(String) Specifies the maximum allowed warehouse size for Serverless tasks. Valid values are (case-insensitive): XSMALL | X-SMALL | SMALL | MEDIUM | LARGE | XLARGE | X-LARGE | XXLARGE | X2LARGE | 2X-LARGE | XXXLARGE | X3LARGE | 3X-LARGE | X4LARGE | 4X-LARGE | X5LARGE | 5X-LARGE | X6LARGE | 6X-LARGE. For more information, check SERVERLESS_TASK_MAX_STATEMENT_SIZE docs.'
            serverless_task_min_statement_size: '(String) Specifies the minimum allowed warehouse size for Serverless tasks. Valid values are (case-insensitive): XSMALL | X-SMALL | SMALL | MEDIUM | LARGE | XLARGE | X-LARGE | XXLARGE | X2LARGE | 2X-LARGE | XXXLARGE | X3LARGE | 3X-LARGE | X4LARGE | 4X-LARGE | X5LARGE | 5X-LARGE | X6LARGE | 6X-LARGE. For more information, check SERVERLESS_TASK_MIN_STATEMENT_SIZE docs.'
            session_policy: (String) Specifies session policy for the current account.
            show_output: (List of Object) Saved output for the result of SHOW ORGANIZATION ACCOUNTS (see below for nested schema)
            simulated_data_sharing_consumer: (String) Specifies the name of a consumer account to simulate for testing/validating shared data, particularly shared secure views. When this parameter is set in a session, shared views return rows as if executed in the specified consumer account rather than the provider account. For more information, check SIMULATED_DATA_SHARING_CONSUMER docs.
            snowflake_region: (String)
            sso_login_page: (Boolean) This deprecated parameter disables preview mode for testing SSO (after enabling federated authentication) before rolling it out to users. For more information, check SSO_LOGIN_PAGE docs.
            statement_queued_timeout_in_seconds: (Number) Amount of time, in seconds, a SQL statement (query, DDL, DML, etc.) remains queued for a warehouse before it is canceled by the system. This parameter can be used in conjunction with the MAX_CONCURRENCY_LEVEL parameter to ensure a warehouse is never backlogged. For more information, check STATEMENT_QUEUED_TIMEOUT_IN_SECONDS docs.
            statement_timeout_in_seconds: (Number) Amount of time, in seconds, after which a running SQL statement (query, DDL, DML, etc.) is canceled by the system. For more information, check STATEMENT_TIMEOUT_IN_SECONDS docs.
            storage_serialization_policy: '(String) Specifies the storage serialization policy for Snowflake-managed Apache Iceberg™ tables. Valid values are (case-insensitive): COMPATIBLE | OPTIMIZED. For more information, check STORAGE_SERIALIZATION_POLICY docs.'
            strict_json_output: (Boolean) This parameter specifies whether JSON output in a session is compatible with the general standard (as described by http://json.org). By design, Snowflake allows JSON input that contains non-standard values; however, these non-standard values might result in Snowflake outputting JSON that is incompatible with other platforms and languages. This parameter, when enabled, ensures that Snowflake outputs valid/compatible JSON. For more information, check STRICT_JSON_OUTPUT docs.
            suspend_task_after_num_failures: (Number) Specifies the number of consecutive failed task runs after which the current task is suspended automatically. The default is 0 (no automatic suspension). For more information, check SUSPEND_TASK_AFTER_NUM_FAILURES docs.
            task_auto_retry_attempts: (Number) Specifies the number of automatic task graph retry attempts. If any task graphs complete in a FAILED state, Snowflake can automatically retry the task graphs from the last task in the graph that failed. For more information, check TASK_AUTO_RETRY_ATTEMPTS docs.
            time_input_format: (String) Specifies the input format for the TIME data type. For more information, see Date and time input and output formats. Any valid, supported time format or AUTO (AUTO specifies that Snowflake attempts to automatically detect the format of times stored in the system during the session). For more information, check TIME_INPUT_FORMAT docs.
            time_output_format: (String) Specifies the display format for the TIME data type. For more information, see Date and time input and output formats. For more information, check TIME_OUTPUT_FORMAT docs.
            timeouts: (Block, Optional) (see below for nested schema)
            timestamp_day_is_always_24h: (Boolean) Specifies whether the DATEADD function (and its aliases) always consider a day to be exactly 24 hours for expressions that span multiple days. For more information, check TIMESTAMP_DAY_IS_ALWAYS_24H docs.
            timestamp_input_format: (String) Specifies the input format for the TIMESTAMP data type alias. For more information, see Date and time input and output formats. Any valid, supported timestamp format or AUTO (AUTO specifies that Snowflake attempts to automatically detect the format of timestamps stored in the system during the session). For more information, check TIMESTAMP_INPUT_FORMAT docs.
            timestamp_ltz_output_format: (String) Specifies the display format for the TIMESTAMP_LTZ data type. If no format is specified, defaults to TIMESTAMP_OUTPUT_FORMAT. For more information, see Date and time input and output formats. For more information, check TIMESTAMP_LTZ_OUTPUT_FORMAT docs.
            timestamp_ntz_output_format: (String) Specifies the display format for the TIMESTAMP_NTZ data type. For more information, check TIMESTAMP_NTZ_OUTPUT_FORMAT docs.
            timestamp_output_format: (String) Specifies the display format for the TIMESTAMP data type alias. For more information, see Date and time input and output formats. For more information, check TIMESTAMP_OUTPUT_FORMAT docs.
            timestamp_type_mapping: '(String) Specifies the TIMESTAMP_* variation that the TIMESTAMP data type alias maps to. Valid values are (case-insensitive): TIMESTAMP_LTZ | TIMESTAMP_NTZ | TIMESTAMP_TZ. For more information, check TIMESTAMP_TYPE_MAPPING docs.'
            timestamp_tz_output_format: (String) Specifies the display format for the TIMESTAMP_TZ data type. If no format is specified, defaults to TIMESTAMP_OUTPUT_FORMAT. For more information, see Date and time input and output formats. For more information, check TIMESTAMP_TZ_OUTPUT_FORMAT docs.
            timezone: (String) Specifies the time zone for the session. You can specify a time zone name or a link name from release 2021a of the IANA Time Zone Database (e.g. America/Los_Angeles, Europe/London, UTC, Etc/GMT, etc.). For more information, check TIMEZONE docs.
            trace_level: '(String) Controls how trace events are ingested into the event table. For more information about trace levels, see Setting trace level. Valid values are (case-insensitive): ALWAYS | ON_EVENT | PROPAGATE | OFF. For more information, check TRACE_LEVEL docs.'
            transaction_abort_on_error: (Boolean) Specifies the action to perform when a statement issued within a non-autocommit transaction returns with an error. For more information, check TRANSACTION_ABORT_ON_ERROR docs.
            transaction_default_isolation_level: '(String) Specifies the isolation level for transactions in the user session. Valid values are (case-insensitive): READ COMMITTED. For more information, check TRANSACTION_DEFAULT_ISOLATION_LEVEL docs.'
            two_digit_century_start: (Number) Specifies the “century start” year for 2-digit years (i.e. the earliest year such dates can represent). This parameter prevents ambiguous dates when importing or converting data with the YY date format component (i.e. years represented as 2 digits). For more information, check TWO_DIGIT_CENTURY_START docs.
            unsupported_ddl_action: (String) Determines if an unsupported (i.e. non-default) value specified for a constraint property returns an error. For more information, check UNSUPPORTED_DDL_ACTION docs.
            update: (String)
            use_cached_result: (Boolean) Specifies whether to reuse persisted query results, if available, when a matching query is submitted. For more information, check USE_CACHED_RESULT docs.
            user_task_managed_initial_warehouse_size: '(String) Specifies the size of the compute resources to provision for the first run of the task, before a task history is available for Snowflake to determine an ideal size. Once a task has successfully completed a few runs, Snowflake ignores this parameter setting. Valid values are (case-insensitive): XSMALL | X-SMALL | SMALL | MEDIUM | LARGE | XLARGE | X-LARGE | XXLARGE | X2LARGE | 2X-LARGE | XXXLARGE | X3LARGE | 3X-LARGE | X4LARGE | 4X-LARGE | X5LARGE | 5X-LARGE | X6LARGE | 6X-LARGE. For more information, check USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE docs.'
            user_task_minimum_trigger_interval_in_seconds: (Number) Minimum amount of time between Triggered Task executions in seconds For more information, check USER_TASK_MINIMUM_TRIGGER_INTERVAL_IN_SECONDS docs.
            user_task_timeout_ms: (Number) Specifies the time limit on a single run of the task before it times out (in milliseconds). For more information, check USER_TASK_TIMEOUT_MS docs.
            week_of_year_policy: '(Number) Specifies how the weeks in a given year are computed. 0: The semantics used are equivalent to the ISO semantics, in which a week belongs to a given year if at least 4 days of that week are in that year. 1: January 1 is included in the first week of the year and December 31 is included in the last week of the year. For more information, check WEEK_OF_YEAR_POLICY docs.'
            week_start: '(Number) Specifies the first day of the week (used by week-related date functions). 0: Legacy Snowflake behavior is used (i.e. ISO-like semantics). 1 (Monday) to 7 (Sunday): All the week-related functions use weeks that start on the specified day of the week. For more information, check WEEK_START docs.'
        importStatements:
            - terraform import snowflake_current_organization_account.example '"<organization_account_name>"'
    snowflake_database:
        subCategory: Stable
        description: Represents a standard database. If replication configuration is specified, the database is promoted to serve as a primary database for replication.
        name: snowflake_database
        title: snowflake_database Resource - terraform-provider-snowflake
        examples:
            - name: primary
              manifest: |-
                {
                  "name": "database_name"
                }
            - name: primary
              manifest: |-
                {
                  "catalog": "${snowflake_catalog.example.fully_qualified_name}",
                  "comment": "my standard database",
                  "data_retention_time_in_days": 10,
                  "default_ddl_collation": "en_US",
                  "enable_console_output": false,
                  "external_volume": "${snowflake_external_volume.example.fully_qualified_name}",
                  "is_transient": false,
                  "log_level": "INFO",
                  "max_data_extension_time_in_days": 20,
                  "name": "database_name",
                  "quoted_identifiers_ignore_case": false,
                  "replace_invalid_characters": false,
                  "replication": [
                    {
                      "enable_to_account": [
                        {
                          "account_identifier": "\u003csecondary_account_organization_name\u003e.\u003csecondary_account_name\u003e",
                          "with_failover": true
                        }
                      ],
                      "ignore_edition_check": true
                    }
                  ],
                  "storage_serialization_policy": "COMPATIBLE",
                  "suspend_task_after_num_failures": 10,
                  "task_auto_retry_attempts": 10,
                  "trace_level": "ALWAYS",
                  "user_task_managed_initial_warehouse_size": "LARGE",
                  "user_task_minimum_trigger_interval_in_seconds": 120,
                  "user_task_timeout_ms": 3600000
                }
              references:
                catalog: snowflake_catalog.example.fully_qualified_name
                external_volume: snowflake_external_volume.example.fully_qualified_name
            - name: primary
              manifest: |-
                {
                  "name": "database_name",
                  "replication": [
                    {
                      "dynamic": {
                        "enable_to_account": [
                          {
                            "content": [
                              {
                                "account_identifier": "${enable_to_account.value.account_identifier}",
                                "with_failover": "${enable_to_account.value.with_failover}"
                              }
                            ],
                            "for_each": "${{ for rc in local.replication_configs : rc.account_identifier =\u003e rc }}"
                          }
                        ]
                      },
                      "ignore_edition_check": true
                    }
                  ]
                }
              references:
                replication.dynamic.content.account_identifier: enable_to_account.value.account_identifier
                replication.dynamic.content.with_failover: enable_to_account.value.with_failover
        argumentDocs:
            account_identifier: (String) Specifies account identifier for which replication should be enabled. The account identifiers should be in the form of "<organization_name>"."<account_name>". For more information about this resource, see docs.
            catalog: (String) The database parameter that specifies the default catalog to use for Iceberg tables. For more information, see CATALOG.
            comment: (String) Specifies a comment for the database.
            create: (String)
            data_retention_time_in_days: (Number) Specifies the number of days for which Time Travel actions (CLONE and UNDROP) can be performed on the database, as well as specifying the default Time Travel retention time for all schemas created in the database. For more details, see Understanding & Using Time Travel.
            default_ddl_collation: (String) Specifies a default collation specification for all schemas and tables added to the database. It can be overridden on schema or table level. For more information, see collation specification.
            delete: (String)
            drop_public_schema_on_creation: (Boolean) Specifies whether to drop public schema on creation or not. Modifying the parameter after database is already created won't have any effect.
            enable_console_output: (Boolean) If true, enables stdout/stderr fast path logging for anonymous stored procedures.
            enable_to_account: '(Block List, Min: 1) Entry to enable replication and optionally failover for a given account identifier. (see below for nested schema)'
            external_volume: (String) The database parameter that specifies the default external volume to use for Iceberg tables. For more information, see EXTERNAL_VOLUME.
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            ignore_edition_check: '(Boolean) Allows replicating data to accounts on lower editions in either of the following scenarios: 1. The primary database is in a Business Critical (or higher) account but one or more of the accounts approved for replication are on lower editions. Business Critical Edition is intended for Snowflake accounts with extremely sensitive data. 2. The primary database is in a Business Critical (or higher) account and a signed business associate agreement is in place to store PHI data in the account per HIPAA and HITRUST regulations, but no such agreement is in place for one or more of the accounts approved for replication, regardless if they are Business Critical (or higher) accounts. Both scenarios are prohibited by default in an effort to help prevent account administrators for Business Critical (or higher) accounts from inadvertently replicating sensitive data to accounts on lower editions.'
            is_transient: (Boolean) Specifies the database as transient. Transient databases do not have a Fail-safe period so they do not incur additional storage costs once they leave Time Travel; however, this means they are also not protected by Fail-safe in the event of a data loss.
            log_level: '(String) Specifies the severity level of messages that should be ingested and made available in the active event table. Valid options are: [TRACE DEBUG INFO WARN ERROR FATAL OFF]. Messages at the specified level (and at more severe levels) are ingested. For more information, see LOG_LEVEL.'
            max_data_extension_time_in_days: (Number) Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for tables in the database to prevent streams on the tables from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS.
            name: '(String) Specifies the identifier for the database; must be unique for your account. As a best practice for Database Replication and Failover, it is recommended to give each secondary database the same name as its primary database. This practice supports referencing fully-qualified objects (i.e. ''..'') by other objects in the same database, such as querying a fully-qualified table name in a view. If a secondary database has a different name from the primary database, then these object references would break in the secondary database. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            quoted_identifiers_ignore_case: (Boolean) If true, the case of quoted identifiers is ignored. For more information, see QUOTED_IDENTIFIERS_IGNORE_CASE.
            read: (String)
            replace_invalid_characters: (Boolean) Specifies whether to replace invalid UTF-8 characters with the Unicode replacement character (�) in query results for an Iceberg table. You can only set this parameter for tables that use an external Iceberg catalog. For more information, see REPLACE_INVALID_CHARACTERS.
            replication: '(Block List, Max: 1) Configures replication for a given database. When specified, this database will be promoted to serve as a primary database for replication. A primary database can be replicated in one or more accounts, allowing users in those accounts to query objects in each secondary (i.e. replica) database. (see below for nested schema)'
            storage_serialization_policy: '(String) The storage serialization policy for Iceberg tables that use Snowflake as the catalog. Valid options are: [COMPATIBLE OPTIMIZED]. COMPATIBLE: Snowflake performs encoding and compression of data files that ensures interoperability with third-party compute engines. OPTIMIZED: Snowflake performs encoding and compression of data files that ensures the best table performance within Snowflake. For more information, see STORAGE_SERIALIZATION_POLICY.'
            suspend_task_after_num_failures: (Number) How many times a task must fail in a row before it is automatically suspended. 0 disables auto-suspending. For more information, see SUSPEND_TASK_AFTER_NUM_FAILURES.
            task_auto_retry_attempts: (Number) Maximum automatic retries allowed for a user task. For more information, see TASK_AUTO_RETRY_ATTEMPTS.
            timeouts: (Block, Optional) (see below for nested schema)
            trace_level: '(String) Controls how trace events are ingested into the event table. Valid options are: ALWAYS | ON_EVENT | PROPAGATE | OFF. For information about levels, see TRACE_LEVEL.'
            update: (String)
            user_task_managed_initial_warehouse_size: (String) The initial size of warehouse to use for managed warehouses in the absence of history. For more information, see USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE.
            user_task_minimum_trigger_interval_in_seconds: (Number) Minimum amount of time between Triggered Task executions in seconds.
            user_task_timeout_ms: (Number) User task execution timeout in milliseconds. For more information, see USER_TASK_TIMEOUT_MS.
            with_failover: (Boolean) Specifies if failover should be enabled for the specified account identifier
        importStatements:
            - terraform import snowflake_database.example '"<database_name>"'
    snowflake_database_role:
        subCategory: Stable
        description: Resource used to manage database roles. For more information, check database roles documentation https://docs.snowflake.com/en/sql-reference/sql/create-database-role.
        name: snowflake_database_role
        title: snowflake_database_role Resource - terraform-provider-snowflake
        examples:
            - name: test_database_role
              manifest: |-
                {
                  "comment": "my database role",
                  "database": "${snowflake_database.test_database.fully_qualified_name}",
                  "name": "database_role_name"
                }
              references:
                database: snowflake_database.test_database.fully_qualified_name
              dependencies:
                snowflake_database.test_database: |-
                    {
                      "name": "database_name"
                    }
        argumentDocs:
            comment: (String) Specifies a comment for the database role.
            create: (String)
            created_on: (String)
            database: '(String) The database in which to create the database role. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            database_name: (String)
            delete: (String)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            granted_database_roles: (Number)
            granted_to_database_roles: (Number)
            granted_to_roles: (Number)
            id: (String) The ID of this resource.
            is_current: (Boolean)
            is_default: (Boolean)
            is_inherited: (Boolean)
            name: '(String) Specifies the identifier for the database role. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            owner: (String)
            owner_role_type: (String)
            read: (String)
            show_output: (List of Object) Outputs the result of SHOW DATABASE ROLES for the given database role. Note that this value will be only recomputed whenever comment field changes. (see below for nested schema)
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
        importStatements:
            - terraform import snowflake_database_role.example '"<database_name>"."<database_role_name>"'
    snowflake_dynamic_table:
        subCategory: Preview
        name: snowflake_dynamic_table
        title: snowflake_dynamic_table Resource - terraform-provider-snowflake
        examples:
            - name: dt
              manifest: |-
                {
                  "comment": "example comment",
                  "database": "mydb",
                  "name": "product",
                  "query": "SELECT product_id, product_name FROM \"mydb\".\"myschema\".\"staging_table\"",
                  "schema": "myschema",
                  "target_lag": [
                    {
                      "maximum_duration": "20 minutes"
                    }
                  ],
                  "warehouse": "mywh"
                }
        argumentDocs:
            automatic_clustering: (Boolean) Whether auto-clustering is enabled on the dynamic table. Not currently supported for dynamic tables.
            bytes: (Number) Number of bytes that will be scanned if the entire dynamic table is scanned in a query.
            cluster_by: (String) The clustering key for the dynamic table.
            comment: (String) Specifies a comment for the dynamic table.
            create: (String)
            created_on: (String) Time when this dynamic table was created.
            data_timestamp: (String) Timestamp of the data in the base object(s) that is included in the dynamic table.
            database: (String) The database in which to create the dynamic table.
            delete: (String)
            downstream: (Boolean) Specifies whether the target lag time is downstream.
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            initialize: '(String) (Default: ON_CREATE) Initialize trigger for the dynamic table. Can only be set on creation. Available options are ON_CREATE and ON_SCHEDULE.'
            is_clone: (Boolean) TRUE if the dynamic table has been cloned, else FALSE.
            is_replica: (Boolean) TRUE if the dynamic table is a replica. else FALSE.
            last_suspended_on: (String) Timestamp of last suspension.
            maximum_duration: (String) Specifies the maximum target lag time for the dynamic table.
            name: (String) Specifies the identifier (i.e. name) for the dynamic table; must be unique for the schema in which the dynamic table is created.
            or_replace: '(Boolean) (Default: false) Specifies whether to replace the dynamic table if it already exists.'
            owner: (String) Role that owns the dynamic table.
            query: (String) Specifies the query to use to populate the dynamic table.
            read: (String)
            refresh_mode: '(String) (Default: AUTO) INCREMENTAL to use incremental refreshes, FULL to recompute the whole table on every refresh, or AUTO to let Snowflake decide.'
            refresh_mode_reason: (String) Explanation for why FULL refresh mode was chosen. NULL if refresh mode is not FULL.
            rows: (Number) Number of rows in the table.
            scheduling_state: (String) Displays ACTIVE for dynamic tables that are actively scheduling refreshes and SUSPENDED for suspended dynamic tables.
            schema: (String) The schema in which to create the dynamic table.
            target_lag: '(Block List, Min: 1, Max: 1) Specifies the target lag time for the dynamic table. (see below for nested schema)'
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
            warehouse: (String) The warehouse in which to create the dynamic table.
        importStatements:
            - terraform import snowflake_dynamic_table.dt "mydb|myschema|product"
    snowflake_email_notification_integration:
        subCategory: Preview
        name: snowflake_email_notification_integration
        title: snowflake_email_notification_integration Resource - terraform-provider-snowflake
        examples:
            - name: email_int
              manifest: |-
                {
                  "allowed_recipients": [
                    "john.doe@gmail.com"
                  ],
                  "comment": "A notification integration.",
                  "enabled": true,
                  "name": "notification"
                }
        argumentDocs:
            allowed_recipients: (Set of String) List of email addresses that should receive notifications.
            comment: (String) A comment for the email integration.
            create: (String)
            delete: (String)
            enabled: (Boolean)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            name: (String)
            read: (String)
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
        importStatements:
            - terraform import snowflake_email_notification_integration.example name
    snowflake_execute:
        subCategory: Stable
        description: Resource allowing execution of ANY SQL statement.
        name: snowflake_execute
        title: snowflake_execute Resource - terraform-provider-snowflake
        examples:
            - name: test
              manifest: |-
                {
                  "execute": "CREATE DATABASE ABC",
                  "revert": "DROP DATABASE ABC"
                }
            - name: test
              manifest: |-
                {
                  "execute": "CREATE DATABASE \"abc\"",
                  "revert": "DROP DATABASE \"abc\""
                }
            - name: test
              manifest: |-
                {
                  "execute": "CREATE DATABASE ABC",
                  "query": "SHOW DATABASES LIKE '%ABC%'",
                  "revert": "DROP DATABASE ABC"
                }
            - name: test
              manifest: |-
                {
                  "execute": "GRANT USAGE ON DATABASE ABC TO ROLE XYZ",
                  "revert": "REVOKE USAGE ON DATABASE ABC FROM ROLE XYZ"
                }
            - name: test
              manifest: |-
                {
                  "execute": "GRANT ${join(\",\", each.value.privileges)} ON DATABASE ${each.value.database_name} TO ROLE ${each.value.role_id}",
                  "for_each": "${{ for index, db_grant in var.database_grants : index =\u003e db_grant }}",
                  "revert": "REVOKE ${join(\",\", each.value.privileges)} ON DATABASE ${each.value.database_name} FROM ROLE ${each.value.role_id}"
                }
            - name: test
              manifest: |-
                {
                  "execute": "CREATE DATABASE ABC",
                  "revert": "SELECT 1"
                }
            - name: test
              manifest: |-
                {
                  "execute": "CREATE DATABASE ABC",
                  "revert": "DROP DATABASE ABC"
                }
            - name: test
              manifest: |-
                {
                  "execute": "CREATE DATABASE ABC",
                  "query": "bad query",
                  "revert": "DROP DATABASE ABC"
                }
            - name: test
              manifest: |-
                {
                  "execute": "CREATE DATABASE ABC",
                  "query": "SHOW DATABASES LIKE '%ABC%'",
                  "revert": "DROP DATABASE ABC"
                }
            - name: test
              manifest: |-
                {
                  "execute": "CREATE DATABASE ABC",
                  "query": "SHOW DATABASES LIKE '%ABC%'",
                  "revert": "DROP DATABASE ABC",
                  "timeouts": [
                    {
                      "create": "10m",
                      "delete": "10m",
                      "read": "10m",
                      "update": "10m"
                    }
                  ]
                }
        argumentDocs:
            create: (String)
            delete: (String)
            execute: (String) SQL statement to execute. Forces recreation of resource on change.
            id: (String) The ID of this resource.
            query: (String) Optional SQL statement to do a read. Invoked on every resource refresh and every time it is changed.
            query_results: (List of Map of String) List of key-value maps (text to text) retrieved after executing read query. Will be empty if the query results in an error.
            read: (String)
            revert: (String) SQL statement to revert the execute statement. Invoked when resource is being destroyed.
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
        importStatements:
            - terraform import snowflake_execute.example '<random_uuid>'
    snowflake_external_function:
        subCategory: Preview
        name: snowflake_external_function
        title: snowflake_external_function Resource - terraform-provider-snowflake
        examples:
            - name: test_ext_func
              manifest: |-
                {
                  "api_integration": "api_integration_name",
                  "arg": [
                    {
                      "name": "arg1",
                      "type": "varchar"
                    },
                    {
                      "name": "arg2",
                      "type": "varchar"
                    }
                  ],
                  "database": "my_test_db",
                  "name": "my_function",
                  "return_behavior": "IMMUTABLE",
                  "return_type": "variant",
                  "schema": "my_test_schema",
                  "url_of_proxy_and_resource": "https://123456.execute-api.us-west-2.amazonaws.com/prod/test_func"
                }
        argumentDocs:
            api_integration: (String) The name of the API integration object that should be used to authenticate the call to the proxy service.
            arg: (Block List) Specifies the arguments/inputs for the external function. These should correspond to the arguments that the remote service expects. (see below for nested schema)
            comment: '(String) (Default: user-defined function) A description of the external function.'
            compression: '(String) (Default: AUTO) If specified, the JSON payload is compressed when sent from Snowflake to the proxy service, and when sent back from the proxy service to Snowflake.'
            context_headers: (List of String) Binds Snowflake context function results to HTTP headers.
            create: (String)
            created_on: (String) Date and time when the external function was created.
            database: (String) The database in which to create the external function.
            delete: (String)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            header: (Block Set) Allows users to specify key-value metadata that is sent with every request as HTTP headers. (see below for nested schema)
            id: (String) The ID of this resource.
            max_batch_rows: (Number) This specifies the maximum number of rows in each batch sent to the proxy service.
            name: (String) Specifies the identifier for the external function. The identifier can contain the schema name and database name, as well as the function name. The function's signature (name and argument data types) must be unique within the schema.
            null_input_behavior: '(String) (Default: CALLED ON NULL INPUT) Specifies the behavior of the external function when called with null inputs.'
            read: (String)
            request_translator: (String) This specifies the name of the request translator function
            response_translator: (String) This specifies the name of the response translator function.
            return_behavior: (String) Specifies the behavior of the function when returning results
            return_null_allowed: '(Boolean) (Default: true) Indicates whether the function can return NULL values (true) or must return only NON-NULL values (false).'
            return_type: (String) Specifies the data type returned by the external function.
            schema: (String) The schema in which to create the external function.
            timeouts: (Block, Optional) (see below for nested schema)
            type: (String) Argument type, e.g. VARCHAR
            update: (String)
            url_of_proxy_and_resource: (String) This is the invocation URL of the proxy service and resource through which Snowflake calls the remote service.
            value: (String) Header value
        importStatements:
            - |-
              # format is <database_name>.<schema_name>.<external_function_name>(<arg types, separated with ','>)
              terraform import snowflake_external_function.example 'dbName.schemaName.externalFunctionName(varchar, varchar, varchar)'
    snowflake_external_oauth_integration:
        subCategory: Stable
        description: Resource used to manage external oauth security integration objects. For more information, check security integrations documentation https://docs.snowflake.com/en/sql-reference/sql/create-security-integration-oauth-external.
        name: snowflake_external_oauth_integration
        title: snowflake_external_oauth_integration Resource - terraform-provider-snowflake
        examples:
            - name: test
              manifest: |-
                {
                  "enabled": true,
                  "external_oauth_issuer": "issuer",
                  "external_oauth_snowflake_user_mapping_attribute": "LOGIN_NAME",
                  "external_oauth_token_user_mapping_claim": [
                    "upn"
                  ],
                  "external_oauth_type": "CUSTOM",
                  "name": "test"
                }
            - name: test
              manifest: |-
                {
                  "comment": "comment",
                  "enabled": true,
                  "external_oauth_allowed_roles_list": [
                    "${snowflake_role.one.fully_qualified_name}"
                  ],
                  "external_oauth_any_role_mode": "ENABLE",
                  "external_oauth_audience_list": [
                    "https://example.com"
                  ],
                  "external_oauth_issuer": "issuer",
                  "external_oauth_jws_keys_url": [
                    "https://example.com"
                  ],
                  "external_oauth_scope_delimiter": ",",
                  "external_oauth_scope_mapping_attribute": "scope",
                  "external_oauth_snowflake_user_mapping_attribute": "LOGIN_NAME",
                  "external_oauth_token_user_mapping_claim": [
                    "upn"
                  ],
                  "external_oauth_type": "CUSTOM",
                  "name": "test"
                }
            - name: test
              manifest: |-
                {
                  "comment": "comment",
                  "enabled": true,
                  "external_oauth_any_role_mode": "ENABLE",
                  "external_oauth_audience_list": [
                    "https://example.com"
                  ],
                  "external_oauth_blocked_roles_list": [
                    "${snowflake_role.one.fully_qualified_name}"
                  ],
                  "external_oauth_issuer": "issuer",
                  "external_oauth_rsa_public_key": "${file(\"key.pem\")}",
                  "external_oauth_rsa_public_key_2": "${file(\"key2.pem\")}",
                  "external_oauth_scope_delimiter": ",",
                  "external_oauth_scope_mapping_attribute": "scope",
                  "external_oauth_snowflake_user_mapping_attribute": "LOGIN_NAME",
                  "external_oauth_token_user_mapping_claim": [
                    "upn"
                  ],
                  "external_oauth_type": "CUSTOM",
                  "name": "test"
                }
        argumentDocs:
            category: (String)
            comment: (String) Specifies a comment for the OAuth integration.
            create: (String)
            created_on: (String)
            default: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE SECURITY INTEGRATIONS for the given security integration. (see below for nested schema)
            description: (String)
            enabled: (Boolean) Specifies whether to initiate operation of the integration or suspend it.
            external_oauth_add_privileged_roles_to_blocked_list: (List of Object) (see below for nested schema)
            external_oauth_allowed_roles_list: (Set of String) Specifies the list of roles that the client can set as the primary role. For more information about this resource, see docs.
            external_oauth_any_role_mode: '(String) Specifies whether the OAuth client or user can use a role that is not defined in the OAuth access token. Valid values are (case-insensitive): DISABLE | ENABLE | ENABLE_FOR_PRIVILEGE.'
            external_oauth_audience_list: (Set of String) Specifies additional values that can be used for the access token's audience validation on top of using the Customer's Snowflake Account URL
            external_oauth_blocked_roles_list: (Set of String) Specifies the list of roles that a client cannot set as the primary role. By default, this list includes the ACCOUNTADMIN, ORGADMIN and SECURITYADMIN roles. To remove these privileged roles from the list, use the ALTER ACCOUNT command to set the EXTERNAL_OAUTH_ADD_PRIVILEGED_ROLES_TO_BLOCKED_LIST account parameter to FALSE. For more information about this resource, see docs.
            external_oauth_issuer: (String) Specifies the URL to define the OAuth 2.0 authorization server.
            external_oauth_jws_keys_url: (Set of String) Specifies the endpoint or a list of endpoints from which to download public keys or certificates to validate an External OAuth access token. The maximum number of URLs that can be specified in the list is 3. If removed from the config, the resource is recreated.
            external_oauth_rsa_public_key: (String) Specifies a Base64-encoded RSA public key, without the -----BEGIN PUBLIC KEY----- and -----END PUBLIC KEY----- headers. If removed from the config, the resource is recreated.
            external_oauth_rsa_public_key_2: (String) Specifies a second RSA public key, without the -----BEGIN PUBLIC KEY----- and -----END PUBLIC KEY----- headers. Used for key rotation. If removed from the config, the resource is recreated.
            external_oauth_scope_delimiter: (String) Specifies the scope delimiter in the authorization token.
            external_oauth_scope_mapping_attribute: (String) Specifies the access token claim to map the access token to an account role. If removed from the config, the resource is recreated.
            external_oauth_snowflake_user_mapping_attribute: '(String) Indicates which Snowflake user record attribute should be used to map the access token to a Snowflake user record. Valid values are (case-insensitive): LOGIN_NAME | EMAIL_ADDRESS.'
            external_oauth_token_user_mapping_claim: (Set of String) Specifies the access token claim or claims that can be used to map the access token to a Snowflake user record. If removed from the config, the resource is recreated.
            external_oauth_type: '(String) Specifies the OAuth 2.0 authorization server to be Okta, Microsoft Azure AD, Ping Identity PingFederate, or a Custom OAuth 2.0 authorization server. Valid values are (case-insensitive): OKTA | AZURE | PING_FEDERATE | CUSTOM.'
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            integration_type: (String)
            key: (String)
            level: (String)
            name: '(String) Specifies the name of the External Oath integration. This name follows the rules for Object Identifiers. The name should be unique among security integrations in your account. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            read: (String)
            related_parameters: (List of Object) Parameters related to this security integration. (see below for nested schema)
            show_output: (List of Object) Outputs the result of SHOW SECURITY INTEGRATIONS for the given security integration. (see below for nested schema)
            timeouts: (Block, Optional) (see below for nested schema)
            type: (String)
            update: (String)
            value: (String)
        importStatements:
            - terraform import snowflake_external_oauth_integration.example '"<integration_name>"'
    snowflake_external_table:
        subCategory: Preview
        name: snowflake_external_table
        title: snowflake_external_table Resource - terraform-provider-snowflake
        examples:
            - name: external_table
              manifest: |-
                {
                  "column": [
                    {
                      "name": "id",
                      "type": "int"
                    },
                    {
                      "name": "data",
                      "type": "text"
                    }
                  ],
                  "comment": "External table",
                  "database": "db",
                  "file_format": "TYPE = CSV FIELD_DELIMITER = '|'",
                  "name": "external_table",
                  "schema": "schema"
                }
            - name: external_table_with_location
              manifest: |-
                {
                  "column": [
                    {
                      "name": "id",
                      "type": "int"
                    }
                  ],
                  "database": "db",
                  "location": "@MYDB.MYSCHEMA.MYSTAGE",
                  "name": "external_table_with_location",
                  "schema": "schema"
                }
        argumentDocs:
            as: (String) String that specifies the expression for the column. When queried, the column returns results derived from this expression.
            auto_refresh: '(Boolean) (Default: true) Specifies whether to automatically refresh the external table metadata once, immediately after the external table is created.'
            aws_sns_topic: (String) Specifies the aws sns topic for the external table.
            column: '(Block List, Min: 1) Definitions of a column to create in the external table. Minimum one required. (see below for nested schema)'
            comment: (String) Specifies a comment for the external table.
            copy_grants: '(Boolean) (Default: false) Specifies to retain the access permissions from the original table when an external table is recreated using the CREATE OR REPLACE TABLE variant'
            create: (String)
            database: (String) The database in which to create the external table.
            delete: (String)
            file_format: (String) Specifies the file format for the external table.
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            location: (String) Specifies a location for the external table, using its FQDN. You can hardcode it ("@MYDB.MYSCHEMA.MYSTAGE"), or populate dynamically ("@${snowflake_stage.mystage.fully_qualified_name}")
            name: (String) Specifies the identifier for the external table; must be unique for the database and schema in which the externalTable is created.
            owner: (String) Name of the role that owns the external table.
            partition_by: (List of String) Specifies any partition columns to evaluate for the external table.
            pattern: (String) Specifies the file names and/or paths on the external stage to match.
            read: (String)
            refresh_on_create: '(Boolean) (Default: true) Specifies weather to refresh when an external table is created.'
            schema: (String) The schema in which to create the external table.
            table_format: (String) Identifies the external table table type. For now, only "delta" for Delta Lake table format is supported.
            tag: (Block List, Deprecated) Definitions of a tag to associate with the resource. (see below for nested schema)
            timeouts: (Block, Optional) (see below for nested schema)
            type: (String) Column type, e.g. VARIANT
            update: (String)
            value: (String) Tag value, e.g. marketing_info.
        importStatements:
            - |-
              # format is database name | schema name | external table name
              terraform import snowflake_external_table.example 'dbName|schemaName|externalTableName'
    snowflake_external_volume Resource - terraform-provider-snowflake:
        subCategory: Preview
        description: Resource used to manage external volume objects. For more information, check external volume documentation https://docs.snowflake.com/en/sql-reference/commands-data-loading#external-volume.
        name: snowflake_external_volume Resource - terraform-provider-snowflake
        title: snowflake_external_volume Resource - terraform-provider-snowflake
        argumentDocs:
            allow_writes: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies whether write operations are allowed for the external volume; must be set to TRUE for Iceberg tables that use Snowflake as the catalog. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            azure_tenant_id: (String) Specifies the ID for your Office 365 tenant that the allowed and blocked storage accounts belong to.
            comment: (String) Specifies a comment for the external volume.
            create: (String)
            default: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE EXTERNAL VOLUME for the given external volume. (see below for nested schema)
            encryption_kms_key_id: (String) Specifies the ID for the KMS-managed key used to encrypt files.
            encryption_type: (String) Specifies the encryption type used.
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            name: '(String) Identifier for the external volume; must be unique for your account. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            parent: (String)
            read: (String)
            show_output: (List of Object) Outputs the result of SHOW EXTERNAL VOLUMES for the given external volume. (see below for nested schema)
            storage_aws_external_id: (String) External ID that Snowflake uses to establish a trust relationship with AWS.
            storage_aws_role_arn: (String) Specifies the case-sensitive Amazon Resource Name (ARN) of the AWS identity and access management (IAM) role that grants privileges on the S3 bucket containing your data files.
            storage_base_url: (String) Specifies the base URL for your cloud storage location.
            storage_location: '(Block List, Min: 1) List of named cloud storage locations in different regions and, optionally, cloud platforms. Minimum 1 required. The order of the list is important as it impacts the active storage location, and updates will be triggered if it changes. Note that not all parameter combinations are valid as they depend on the given storage_provider. Consult the docs for more details on this. (see below for nested schema)'
            storage_location_name: '(String) Name of the storage location. Must be unique for the external volume. Do not use the name terraform_provider_sentinel_storage_location - this is reserved for the provider for performing update operations. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            storage_provider: '(String) Specifies the cloud storage provider that stores your data files. Valid values are (case-insensitive): GCS | AZURE | S3 | S3GOV.'
            timeouts: (Block, Optional) (see below for nested schema)
            type: (String)
            update: (String)
            value: (String)
        importStatements: []
    snowflake_failover_group:
        subCategory: Preview
        name: snowflake_failover_group
        title: snowflake_failover_group Resource - terraform-provider-snowflake
        examples:
            - name: source_failover_group
              manifest: |-
                {
                  "allowed_accounts": [
                    "\u003corg_name\u003e.\u003ctarget_account_name1\u003e",
                    "\u003corg_name\u003e.\u003ctarget_account_name2\u003e"
                  ],
                  "allowed_databases": [
                    "${snowflake_database.db.name}"
                  ],
                  "allowed_integration_types": [
                    "SECURITY INTEGRATIONS"
                  ],
                  "name": "FG1",
                  "object_types": [
                    "WAREHOUSES",
                    "DATABASES",
                    "INTEGRATIONS",
                    "ROLES"
                  ],
                  "replication_schedule": [
                    {
                      "cron": [
                        {
                          "expression": "0 0 10-20 * TUE,THU",
                          "time_zone": "UTC"
                        }
                      ]
                    }
                  ]
                }
              dependencies:
                snowflake_database.db: |-
                    {
                      "name": "db1"
                    }
            - name: target_failover_group
              manifest: |-
                {
                  "from_replica": [
                    {
                      "name": "${snowflake_failover_group.source_failover_group.name}",
                      "organization_name": "...",
                      "source_account_name": "..."
                    }
                  ],
                  "name": "FG1",
                  "provider": "${snowflake.account2}"
                }
              references:
                from_replica.name: snowflake_failover_group.source_failover_group.name
                provider: snowflake.account2
              dependencies:
                snowflake_database.db: |-
                    {
                      "name": "db1"
                    }
        argumentDocs:
            allowed_accounts: (Set of String) Specifies the target account or list of target accounts to which replication and failover of specified objects from the source account is enabled. Secondary failover groups in the target accounts in this list can be promoted to serve as the primary failover group in case of failover. Expected in the form <org_name>.<target_account_name>. This value is case-sensitive.
            allowed_databases: (Set of String) Specifies the database or list of databases for which you are enabling replication and failover from the source account to the target account. The OBJECT_TYPES list must include DATABASES to set this parameter.
            allowed_integration_types: '(Set of String) Type(s) of integrations for which you are enabling replication and failover from the source account to the target account. This property requires that the OBJECT_TYPES list include INTEGRATIONS to set this parameter. The following integration types are supported: "SECURITY INTEGRATIONS", "API INTEGRATIONS", "STORAGE INTEGRATIONS", "EXTERNAL ACCESS INTEGRATIONS", "NOTIFICATION INTEGRATIONS"'
            allowed_shares: (Set of String) Specifies the share or list of shares for which you are enabling replication and failover from the source account to the target account. The OBJECT_TYPES list must include SHARES to set this parameter.
            create: (String)
            cron: '(Block List, Max: 1) Specifies the cron expression for the replication schedule. The cron expression must be in the following format: "minute hour day-of-month month day-of-week". The following values are supported: minute: 0-59 hour: 0-23 day-of-month: 1-31 month: 1-12 day-of-week: 0-6 (0 is Sunday) (see below for nested schema)'
            delete: (String)
            expression: '(String) Specifies the cron expression for the replication schedule. The cron expression must be in the following format: "minute hour day-of-month month day-of-week". The following values are supported: minute: 0-59 hour: 0-23 day-of-month: 1-31 month: 1-12 day-of-week: 0-6 (0 is Sunday)'
            from_replica: '(Block List, Max: 1) Specifies the name of the replica to use as the source for the failover group. (see below for nested schema)'
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            ignore_edition_check: '(Boolean) (Default: false) Allows replicating objects to accounts on lower editions.'
            interval: (Number) Specifies the interval in minutes for the replication schedule. The interval must be greater than 0 and less than 1440 (24 hours).
            name: (String) Specifies the identifier for the failover group. The identifier must start with an alphabetic character and cannot contain spaces or special characters unless the identifier string is enclosed in double quotes (e.g. "My object"). Identifiers enclosed in double quotes are also case-sensitive.
            object_types: '(Set of String) Type(s) of objects for which you are enabling replication and failover from the source account to the target account. The following object types are supported: "ACCOUNT PARAMETERS", "DATABASES", "INTEGRATIONS", "NETWORK POLICIES", "RESOURCE MONITORS", "ROLES", "SHARES", "USERS", "WAREHOUSES"'
            organization_name: (String) Name of your Snowflake organization.
            read: (String)
            replication_schedule: '(Block List, Max: 1) Specifies the schedule for refreshing secondary failover groups. (see below for nested schema)'
            source_account_name: (String) Source account from which you are enabling replication and failover of the specified objects.
            time_zone: (String) Specifies the time zone for secondary group refresh.
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
        importStatements:
            - terraform import snowflake_failover_group.example 'fg1'
    snowflake_file_format:
        subCategory: Preview
        name: snowflake_file_format
        title: snowflake_file_format Resource - terraform-provider-snowflake
        examples:
            - name: example_file_format
              manifest: |-
                {
                  "database": "EXAMPLE_DB",
                  "format_type": "CSV",
                  "name": "EXAMPLE_FILE_FORMAT",
                  "schema": "EXAMPLE_SCHEMA"
                }
        argumentDocs:
            allow_duplicate: (Boolean) Boolean that specifies to allow duplicate object field names (only the last one will be preserved).
            binary_as_text: (Boolean) Boolean that specifies whether to interpret columns with no defined logical data type as UTF-8 text.
            binary_format: (String) Defines the encoding format for binary input or output.
            comment: (String) Specifies a comment for the file format.
            compression: (String) Specifies the current compression algorithm for the data file.
            create: (String)
            database: (String) The database in which to create the file format.
            date_format: (String) Defines the format of date values in the data files (data loading) or table (data unloading).
            delete: (String)
            disable_auto_convert: (Boolean) Boolean that specifies whether the XML parser disables automatic conversion of numeric and Boolean values from text to native representation.
            disable_snowflake_data: (Boolean) Boolean that specifies whether the XML parser disables recognition of Snowflake semi-structured data tags.
            empty_field_as_null: (Boolean) Specifies whether to insert SQL NULL for empty fields in an input file, which are represented by two successive delimiters.
            enable_octal: (Boolean) Boolean that enables parsing of octal numbers.
            encoding: (String) String (constant) that specifies the character set of the source data when loading data into a table.
            error_on_column_count_mismatch: (Boolean) Boolean that specifies whether to generate a parsing error if the number of delimited columns (i.e. fields) in an input file does not match the number of columns in the corresponding table.
            escape: (String) Single character string used as the escape character for field values.
            escape_unenclosed_field: (String) Single character string used as the escape character for unenclosed field values only.
            field_delimiter: (String) Specifies one or more singlebyte or multibyte characters that separate fields in an input file (data loading) or unloaded file (data unloading).
            field_optionally_enclosed_by: (String) Character used to enclose strings.
            file_extension: (String) Specifies the extension for files unloaded to a stage.
            format_type: (String) Specifies the format of the input files (for data loading) or output files (for data unloading).
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            ignore_utf8_errors: (Boolean) Boolean that specifies whether UTF-8 encoding errors produce error conditions.
            name: (String) Specifies the identifier for the file format; must be unique for the database and schema in which the file format is created.
            null_if: (List of String) String used to convert to and from SQL NULL.
            parse_header: (Boolean) Boolean that specifies whether to use the first row headers in the data files to determine column names.
            preserve_space: (Boolean) Boolean that specifies whether the XML parser preserves leading and trailing spaces in element content.
            read: (String)
            record_delimiter: (String) Specifies one or more singlebyte or multibyte characters that separate records in an input file (data loading) or unloaded file (data unloading).
            replace_invalid_characters: (Boolean) Boolean that specifies whether to replace invalid UTF-8 characters with the Unicode replacement character (�).
            schema: (String) The schema in which to create the file format.
            skip_blank_lines: (Boolean) Boolean that specifies to skip any blank lines encountered in the data files.
            skip_byte_order_mark: (Boolean) Boolean that specifies whether to skip the BOM (byte order mark), if present in a data file.
            skip_header: (Number) Number of lines at the start of the file to skip.
            strip_null_values: (Boolean) Boolean that instructs the JSON parser to remove object fields or array elements containing null values.
            strip_outer_array: (Boolean) Boolean that instructs the JSON parser to remove outer brackets.
            strip_outer_element: (Boolean) Boolean that specifies whether the XML parser strips out the outer XML element, exposing 2nd level elements as separate documents.
            time_format: (String) Defines the format of time values in the data files (data loading) or table (data unloading).
            timeouts: (Block, Optional) (see below for nested schema)
            timestamp_format: (String) Defines the format of timestamp values in the data files (data loading) or table (data unloading).
            trim_space: (Boolean) Boolean that specifies whether to remove white space from fields.
            update: (String)
        importStatements:
            - |-
              # format is database name | schema name | file format name
              terraform import snowflake_file_format.example 'dbName|schemaName|fileFormatName'
    snowflake_function_java:
        subCategory: Preview
        description: Resource used to manage java function objects. For more information, check function documentation https://docs.snowflake.com/en/sql-reference/sql/create-function.
        name: snowflake_function_java
        title: snowflake_function_java Resource - terraform-provider-snowflake
        examples:
            - name: w
              manifest: |-
                {
                  "arguments": [
                    {
                      "arg_data_type": "VARCHAR(100)",
                      "arg_name": "x"
                    }
                  ],
                  "database": "Database",
                  "function_definition": "  class TestFunc {\n    public static String echoVarchar(String x) {\n      return x;\n    }\n  }\n",
                  "handler": "TestFunc.echoVarchar",
                  "name": "Name",
                  "return_type": "VARCHAR(100)",
                  "schema": "Schema"
                }
        argumentDocs:
            arg_data_type: (String) The argument type.
            arg_default_value: (String) Optional default value for the argument. For text values use single quotes. Numeric values can be unquoted. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            arg_name: (String) The argument name. The provider wraps it in double quotes by default, so be aware of that while referencing the argument in the function definition.
            arguments: (Block List) List of the arguments for the function. Consult the docs for more details. (see below for nested schema)
            arguments_raw: (String)
            catalog_name: (String)
            comment: '(String) (Default: user-defined function) Specifies a comment for the function.'
            create: (String)
            created_on: (String)
            database: '(String) The database in which to create the function. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            default: (String)
            delete: (String)
            description: (String)
            enable_console_output: (Boolean) Enable stdout/stderr fast path logging for anonymous stored procs. This is a public parameter (similar to LOG_LEVEL). For more information, check ENABLE_CONSOLE_OUTPUT docs.
            external_access_integrations: (Set of String) The names of external access integrations needed in order for this function’s handler code to access external networks. An external access integration specifies network rules and secrets that specify external locations and credentials (if any) allowed for use by handler code when making requests of an external network, such as an external REST API.
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            function_definition: (String) Defines the handler code executed when the UDF is called. Wrapping $$ signs are added by the provider automatically; do not include them. The function_definition value must be Java source code. For more information, see Introduction to Java UDFs. To mitigate permadiff on this field, the provider replaces blank characters with a space. This can lead to false positives in cases where a change in case or run of whitespace is semantically significant.
            function_language: (String) Specifies language for the user. Used to detect external changes.
            handler: '(String) The name of the handler method or class. If the handler is for a scalar UDF, returning a non-tabular value, the HANDLER value should be a method name, as in the following form: MyClass.myMethod. If the handler is for a tabular UDF, the HANDLER value should be the name of a handler class.'
            id: (String) The ID of this resource.
            imports: (Block Set) The location (stage), path, and name of the file(s) to import. A file can be a JAR file or another type of file. If the file is a JAR file, it can contain one or more .class files and zero or more resource files. JNI (Java Native Interface) is not supported. Snowflake prohibits loading libraries that contain native code (as opposed to Java bytecode). Java UDFs can also read non-JAR files. For an example, see Reading a file specified statically in IMPORTS. Consult the docs. (see below for nested schema)
            is_aggregate: (Boolean)
            is_ansi: (Boolean)
            is_builtin: (Boolean)
            is_data_metric: (Boolean)
            is_external_function: (Boolean)
            is_memoizable: (Boolean)
            is_secure: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies that the function is secure. By design, the Snowflake''s SHOW FUNCTIONS command does not provide information about secure functions (consult function docs and Protecting Sensitive Information with Secure UDFs and Stored Procedures) which is essential to manage/import function with Terraform. Use the role owning the function while managing secure functions. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            is_table_function: (Boolean)
            key: (String)
            language: (String)
            level: (String)
            log_level: (String) LOG_LEVEL to use when filtering events For more information, check LOG_LEVEL docs.
            max_num_arguments: (Number)
            metric_level: (String) METRIC_LEVEL value to control whether to emit metrics to Event Table For more information, check METRIC_LEVEL docs.
            min_num_arguments: (Number)
            name: '(String) The name of the function; the identifier does not need to be unique for the schema in which the function is created because UDFs are identified and resolved by the combination of the name and argument types. Check the docs. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            null_input_behavior: '(String) Specifies the behavior of the function when called with null inputs. Valid values are (case-insensitive): CALLED ON NULL INPUT | RETURNS NULL ON NULL INPUT.'
            packages: (Set of String) The name and version number of Snowflake system packages required as dependencies. The value should be of the form package_name:version_number, where package_name is snowflake_domain:package.
            parameters: (List of Object) Outputs the result of SHOW PARAMETERS IN FUNCTION for the given function. (see below for nested schema)
            path_on_stage: (String) Path for import on stage, without the leading /.
            read: (String)
            return_results_behavior: '(String) Specifies the behavior of the function when returning results. Valid values are (case-insensitive): VOLATILE | IMMUTABLE.'
            return_type: (String) Specifies the results returned by the UDF, which determines the UDF type. Use <result_data_type> to create a scalar UDF that returns a single value with the specified data type. Use TABLE (col_name col_data_type, ...) to creates a table UDF that returns tabular results with the specified table column(s) and column type(s). For the details, consult the docs.
            runtime_version: (String) Specifies the Java JDK runtime version to use. The supported versions of Java are 11.x and 17.x. If RUNTIME_VERSION is not set, Java JDK 11 is used.
            schema: '(String) The schema in which to create the function. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            secret_id: (String) Fully qualified name of the allowed secret. You will receive an error if you specify a SECRETS value whose secret isn’t also included in an integration specified by the EXTERNAL_ACCESS_INTEGRATIONS parameter.
            secret_variable_name: (String) The variable that will be used in handler code when retrieving information from the secret.
            secrets: (Block Set) Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from secrets in handler code. Secrets you specify here must be allowed by the external access integration specified as a value of this CREATE FUNCTION command’s EXTERNAL_ACCESS_INTEGRATIONS parameter. (see below for nested schema)
            show_output: (List of Object) Outputs the result of SHOW FUNCTION for the given function. (see below for nested schema)
            stage_location: (String) Stage location without leading @. To use your user's stage set this to ~, otherwise pass fully qualified name of the stage (with every part contained in double quotes or use snowflake_stage.<your stage's resource name>.fully_qualified_name if you manage this stage through terraform).
            target_path: '(Block Set, Max: 1) The name of the handler method or class. If the handler is for a scalar UDF, returning a non-tabular value, the HANDLER value should be a method name, as in the following form: MyClass.myMethod. If the handler is for a tabular UDF, the HANDLER value should be the name of a handler class. (see below for nested schema)'
            timeouts: (Block, Optional) (see below for nested schema)
            trace_level: (String) Trace level value to use when generating/filtering trace events For more information, check TRACE_LEVEL docs.
            update: (String)
            valid_for_clustering: (Boolean)
            value: (String)
        importStatements:
            - terraform import snowflake_function_java.example '"<database_name>"."<schema_name>"."<function_name>"(varchar, varchar, varchar)'
    snowflake_function_javascript:
        subCategory: Preview
        description: Resource used to manage javascript function objects. For more information, check function documentation https://docs.snowflake.com/en/sql-reference/sql/create-function.
        name: snowflake_function_javascript
        title: snowflake_function_javascript Resource - terraform-provider-snowflake
        examples:
            - name: minimal
              manifest: |-
                {
                  "arguments": [
                    {
                      "arg_data_type": "VARIANT",
                      "arg_name": "x"
                    }
                  ],
                  "database": "${snowflake_database.test.name}",
                  "function_definition": "  if (x == 0) {\n    return 1;\n  } else {\n    return 2;\n  }\n",
                  "name": "my_javascript_function",
                  "return_type": "VARIANT",
                  "schema": "${snowflake_schema.test.name}"
                }
              references:
                database: snowflake_database.test.name
                schema: snowflake_schema.test.name
        argumentDocs:
            arg_data_type: (String) The argument type.
            arg_default_value: (String) Optional default value for the argument. For text values use single quotes. Numeric values can be unquoted. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            arg_name: (String) The argument name. The provider wraps it in double quotes by default, so be aware of that while referencing the argument in the function definition.
            arguments: (Block List) List of the arguments for the function. Consult the docs for more details. (see below for nested schema)
            arguments_raw: (String)
            catalog_name: (String)
            comment: '(String) (Default: user-defined function) Specifies a comment for the function.'
            create: (String)
            created_on: (String)
            database: '(String) The database in which to create the function. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            default: (String)
            delete: (String)
            description: (String)
            enable_console_output: (Boolean) Enable stdout/stderr fast path logging for anonymous stored procs. This is a public parameter (similar to LOG_LEVEL). For more information, check ENABLE_CONSOLE_OUTPUT docs.
            external_access_integrations: (String)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            function_definition: (String) Defines the handler code executed when the UDF is called. Wrapping $$ signs are added by the provider automatically; do not include them. The function_definition value must be JavaScript source code. For more information, see Introduction to JavaScript UDFs. To mitigate permadiff on this field, the provider replaces blank characters with a space. This can lead to false positives in cases where a change in case or run of whitespace is semantically significant.
            function_language: (String) Specifies language for the user. Used to detect external changes.
            id: (String) The ID of this resource.
            is_aggregate: (Boolean)
            is_ansi: (Boolean)
            is_builtin: (Boolean)
            is_data_metric: (Boolean)
            is_external_function: (Boolean)
            is_memoizable: (Boolean)
            is_secure: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies that the function is secure. By design, the Snowflake''s SHOW FUNCTIONS command does not provide information about secure functions (consult function docs and Protecting Sensitive Information with Secure UDFs and Stored Procedures) which is essential to manage/import function with Terraform. Use the role owning the function while managing secure functions. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            is_table_function: (Boolean)
            key: (String)
            language: (String)
            level: (String)
            log_level: (String) LOG_LEVEL to use when filtering events For more information, check LOG_LEVEL docs.
            max_num_arguments: (Number)
            metric_level: (String) METRIC_LEVEL value to control whether to emit metrics to Event Table For more information, check METRIC_LEVEL docs.
            min_num_arguments: (Number)
            name: '(String) The name of the function; the identifier does not need to be unique for the schema in which the function is created because UDFs are identified and resolved by the combination of the name and argument types. Check the docs. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            null_input_behavior: '(String) Specifies the behavior of the function when called with null inputs. Valid values are (case-insensitive): CALLED ON NULL INPUT | RETURNS NULL ON NULL INPUT.'
            parameters: (List of Object) Outputs the result of SHOW PARAMETERS IN FUNCTION for the given function. (see below for nested schema)
            read: (String)
            return_results_behavior: '(String) Specifies the behavior of the function when returning results. Valid values are (case-insensitive): VOLATILE | IMMUTABLE.'
            return_type: (String) Specifies the results returned by the UDF, which determines the UDF type. Use <result_data_type> to create a scalar UDF that returns a single value with the specified data type. Use TABLE (col_name col_data_type, ...) to creates a table UDF that returns tabular results with the specified table column(s) and column type(s). For the details, consult the docs.
            schema: '(String) The schema in which to create the function. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            secrets: (String)
            show_output: (List of Object) Outputs the result of SHOW FUNCTION for the given function. (see below for nested schema)
            timeouts: (Block, Optional) (see below for nested schema)
            trace_level: (String) Trace level value to use when generating/filtering trace events For more information, check TRACE_LEVEL docs.
            update: (String)
            valid_for_clustering: (Boolean)
            value: (String)
        importStatements:
            - terraform import snowflake_function_javascript.example '"<database_name>"."<schema_name>"."<function_name>"(varchar, varchar, varchar)'
    snowflake_function_python:
        subCategory: Preview
        description: Resource used to manage python function objects. For more information, check function documentation https://docs.snowflake.com/en/sql-reference/sql/create-function.
        name: snowflake_function_python
        title: snowflake_function_python Resource - terraform-provider-snowflake
        examples:
            - name: minimal
              manifest: |-
                {
                  "arguments": [
                    {
                      "arg_data_type": "NUMBER(36, 2)",
                      "arg_name": "x"
                    }
                  ],
                  "database": "${snowflake_database.test.name}",
                  "function_definition": "def some_function(x):\n  result = ''\n  for a in range(5):\n    result += x\n  return result\n",
                  "handler": "some_function",
                  "name": "my_function_function",
                  "return_type": "NUMBER(36, 2)",
                  "runtime_version": "3.9",
                  "schema": "${snowflake_schema.test.name}"
                }
              references:
                database: snowflake_database.test.name
                schema: snowflake_schema.test.name
        argumentDocs:
            arg_data_type: (String) The argument type.
            arg_default_value: (String) Optional default value for the argument. For text values use single quotes. Numeric values can be unquoted. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            arg_name: (String) The argument name. The provider wraps it in double quotes by default, so be aware of that while referencing the argument in the function definition.
            arguments: (Block List) List of the arguments for the function. Consult the docs for more details. (see below for nested schema)
            arguments_raw: (String)
            catalog_name: (String)
            comment: '(String) (Default: user-defined function) Specifies a comment for the function.'
            create: (String)
            created_on: (String)
            database: '(String) The database in which to create the function. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            default: (String)
            delete: (String)
            description: (String)
            enable_console_output: (Boolean) Enable stdout/stderr fast path logging for anonymous stored procs. This is a public parameter (similar to LOG_LEVEL). For more information, check ENABLE_CONSOLE_OUTPUT docs.
            external_access_integrations: (Set of String) The names of external access integrations needed in order for this function’s handler code to access external networks. An external access integration specifies network rules and secrets that specify external locations and credentials (if any) allowed for use by handler code when making requests of an external network, such as an external REST API.
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            function_definition: (String) Defines the handler code executed when the UDF is called. Wrapping $$ signs are added by the provider automatically; do not include them. The function_definition value must be Python source code. For more information, see Introduction to Python UDFs. To mitigate permadiff on this field, the provider replaces blank characters with a space. This can lead to false positives in cases where a change in case or run of whitespace is semantically significant.
            function_language: (String) Specifies language for the user. Used to detect external changes.
            handler: '(String) The name of the handler function or class. If the handler is for a scalar UDF, returning a non-tabular value, the HANDLER value should be a function name. If the handler code is in-line with the CREATE FUNCTION statement, you can use the function name alone. When the handler code is referenced at a stage, this value should be qualified with the module name, as in the following form: my_module.my_function. If the handler is for a tabular UDF, the HANDLER value should be the name of a handler class.'
            id: (String) The ID of this resource.
            imports: (Block Set) The location (stage), path, and name of the file(s) to import. A file can be a .py file or another type of file. Python UDFs can also read non-Python files, such as text files. For an example, see Reading a file. Consult the docs. (see below for nested schema)
            is_aggregate: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies that the function is an aggregate function. For more information about user-defined aggregate functions, see Python user-defined aggregate functions. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            is_ansi: (Boolean)
            is_builtin: (Boolean)
            is_data_metric: (Boolean)
            is_external_function: (Boolean)
            is_memoizable: (Boolean)
            is_secure: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies that the function is secure. By design, the Snowflake''s SHOW FUNCTIONS command does not provide information about secure functions (consult function docs and Protecting Sensitive Information with Secure UDFs and Stored Procedures) which is essential to manage/import function with Terraform. Use the role owning the function while managing secure functions. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            is_table_function: (Boolean)
            key: (String)
            language: (String)
            level: (String)
            log_level: (String) LOG_LEVEL to use when filtering events For more information, check LOG_LEVEL docs.
            max_num_arguments: (Number)
            metric_level: (String) METRIC_LEVEL value to control whether to emit metrics to Event Table For more information, check METRIC_LEVEL docs.
            min_num_arguments: (Number)
            name: '(String) The name of the function; the identifier does not need to be unique for the schema in which the function is created because UDFs are identified and resolved by the combination of the name and argument types. Check the docs. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            null_input_behavior: '(String) Specifies the behavior of the function when called with null inputs. Valid values are (case-insensitive): CALLED ON NULL INPUT | RETURNS NULL ON NULL INPUT.'
            packages: (Set of String) The name and version number of packages required as dependencies. The value should be of the form package_name==version_number.
            parameters: (List of Object) Outputs the result of SHOW PARAMETERS IN FUNCTION for the given function. (see below for nested schema)
            path_on_stage: (String) Path for import on stage, without the leading /.
            read: (String)
            return_results_behavior: '(String) Specifies the behavior of the function when returning results. Valid values are (case-insensitive): VOLATILE | IMMUTABLE.'
            return_type: (String) Specifies the results returned by the UDF, which determines the UDF type. Use <result_data_type> to create a scalar UDF that returns a single value with the specified data type. Use TABLE (col_name col_data_type, ...) to creates a table UDF that returns tabular results with the specified table column(s) and column type(s). For the details, consult the docs.
            runtime_version: '(String) Specifies the Python version to use. The supported versions of Python are: 3.9, 3.10, and 3.11.'
            schema: '(String) The schema in which to create the function. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            secret_id: (String) Fully qualified name of the allowed secret. You will receive an error if you specify a SECRETS value whose secret isn’t also included in an integration specified by the EXTERNAL_ACCESS_INTEGRATIONS parameter.
            secret_variable_name: (String) The variable that will be used in handler code when retrieving information from the secret.
            secrets: (Block Set) Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from secrets in handler code. Secrets you specify here must be allowed by the external access integration specified as a value of this CREATE FUNCTION command’s EXTERNAL_ACCESS_INTEGRATIONS parameter. (see below for nested schema)
            show_output: (List of Object) Outputs the result of SHOW FUNCTION for the given function. (see below for nested schema)
            stage_location: (String) Stage location without leading @. To use your user's stage set this to ~, otherwise pass fully qualified name of the stage (with every part contained in double quotes or use snowflake_stage.<your stage's resource name>.fully_qualified_name if you manage this stage through terraform).
            timeouts: (Block, Optional) (see below for nested schema)
            trace_level: (String) Trace level value to use when generating/filtering trace events For more information, check TRACE_LEVEL docs.
            update: (String)
            valid_for_clustering: (Boolean)
            value: (String)
        importStatements:
            - terraform import snowflake_function_python.example '"<database_name>"."<schema_name>"."<function_name>"(varchar, varchar, varchar)'
    snowflake_function_scala:
        subCategory: Preview
        description: Resource used to manage scala function objects. For more information, check function documentation https://docs.snowflake.com/en/sql-reference/sql/create-function.
        name: snowflake_function_scala
        title: snowflake_function_scala Resource - terraform-provider-snowflake
        examples:
            - name: minimal
              manifest: |-
                {
                  "arguments": [
                    {
                      "arg_data_type": "VARCHAR(100)",
                      "arg_name": "x"
                    }
                  ],
                  "database": "${snowflake_database.test.name}",
                  "function_definition": "  class TestFunc {\n    def echoVarchar(x : String): String = {\n      return x\n    }\n  }\n",
                  "handler": "TestFunc.echoVarchar",
                  "name": "my_scala_function",
                  "return_type": "VARCHAR(100)",
                  "runtime_version": "2.12",
                  "schema": "${snowflake_schema.test.name}"
                }
              references:
                database: snowflake_database.test.name
                schema: snowflake_schema.test.name
            - name: complete
              manifest: |-
                {
                  "arguments": [
                    {
                      "arg_data_type": "VARCHAR(100)",
                      "arg_name": "x"
                    }
                  ],
                  "comment": "some comment",
                  "database": "${snowflake_database.test.name}",
                  "external_access_integrations": [
                    "external_access_integration_name",
                    "external_access_integration_name_2"
                  ],
                  "function_definition": "  class TestFunc {\n    def echoVarchar(x : String): String = {\n      return x\n    }\n  }\n",
                  "handler": "TestFunc.echoVarchar",
                  "imports": [
                    {
                      "path_on_stage": "jar_name.jar",
                      "stage_location": "~"
                    },
                    {
                      "path_on_stage": "second_jar_name.jar",
                      "stage_location": "~"
                    }
                  ],
                  "is_secure": "false",
                  "name": "my_scala_function",
                  "null_input_behavior": "CALLED ON NULL INPUT",
                  "packages": [
                    "com.snowflake:snowpark:1.14.0",
                    "com.snowflake:telemetry:0.1.0"
                  ],
                  "return_results_behavior": "VOLATILE",
                  "return_type": "VARCHAR(100)",
                  "runtime_version": "2.12",
                  "schema": "${snowflake_schema.test.name}",
                  "secrets": [
                    {
                      "secret_id": "${snowflake_secret_with_authorization_code_grant.one.fully_qualified_name}",
                      "secret_variable_name": "abc"
                    },
                    {
                      "secret_id": "${snowflake_secret_with_authorization_code_grant.two.fully_qualified_name}",
                      "secret_variable_name": "def"
                    }
                  ],
                  "target_path": [
                    {
                      "path_on_stage": "target_jar_name.jar",
                      "stage_location": "${snowflake_stage.test.fully_qualified_name}"
                    }
                  ]
                }
              references:
                database: snowflake_database.test.name
                schema: snowflake_schema.test.name
                secrets.secret_id: snowflake_secret_with_authorization_code_grant.two.fully_qualified_name
                target_path.stage_location: snowflake_stage.test.fully_qualified_name
        argumentDocs:
            arg_data_type: (String) The argument type.
            arg_default_value: (String) Optional default value for the argument. For text values use single quotes. Numeric values can be unquoted. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            arg_name: (String) The argument name. The provider wraps it in double quotes by default, so be aware of that while referencing the argument in the function definition.
            arguments: (Block List) List of the arguments for the function. Consult the docs for more details. (see below for nested schema)
            arguments_raw: (String)
            catalog_name: (String)
            comment: '(String) (Default: user-defined function) Specifies a comment for the function.'
            create: (String)
            created_on: (String)
            database: '(String) The database in which to create the function. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            default: (String)
            delete: (String)
            description: (String)
            enable_console_output: (Boolean) Enable stdout/stderr fast path logging for anonymous stored procs. This is a public parameter (similar to LOG_LEVEL). For more information, check ENABLE_CONSOLE_OUTPUT docs.
            external_access_integrations: (Set of String) The names of external access integrations needed in order for this function’s handler code to access external networks. An external access integration specifies network rules and secrets that specify external locations and credentials (if any) allowed for use by handler code when making requests of an external network, such as an external REST API.
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            function_definition: (String) Defines the handler code executed when the UDF is called. Wrapping $$ signs are added by the provider automatically; do not include them. The function_definition value must be Scala source code. For more information, see Introduction to Scala UDFs. To mitigate permadiff on this field, the provider replaces blank characters with a space. This can lead to false positives in cases where a change in case or run of whitespace is semantically significant.
            function_language: (String) Specifies language for the user. Used to detect external changes.
            handler: '(String) The name of the handler method or class. If the handler is for a scalar UDF, returning a non-tabular value, the HANDLER value should be a method name, as in the following form: MyClass.myMethod.'
            id: (String) The ID of this resource.
            imports: (Block Set) The location (stage), path, and name of the file(s) to import, such as a JAR or other kind of file. The JAR file might contain handler dependency libraries. It can contain one or more .class files and zero or more resource files. JNI (Java Native Interface) is not supported. Snowflake prohibits loading libraries that contain native code (as opposed to Java bytecode). A non-JAR file might a file read by handler code. For an example, see Reading a file specified statically in IMPORTS. Consult the docs. (see below for nested schema)
            is_aggregate: (Boolean)
            is_ansi: (Boolean)
            is_builtin: (Boolean)
            is_data_metric: (Boolean)
            is_external_function: (Boolean)
            is_memoizable: (Boolean)
            is_secure: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies that the function is secure. By design, the Snowflake''s SHOW FUNCTIONS command does not provide information about secure functions (consult function docs and Protecting Sensitive Information with Secure UDFs and Stored Procedures) which is essential to manage/import function with Terraform. Use the role owning the function while managing secure functions. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            is_table_function: (Boolean)
            key: (String)
            language: (String)
            level: (String)
            log_level: (String) LOG_LEVEL to use when filtering events For more information, check LOG_LEVEL docs.
            max_num_arguments: (Number)
            metric_level: (String) METRIC_LEVEL value to control whether to emit metrics to Event Table For more information, check METRIC_LEVEL docs.
            min_num_arguments: (Number)
            name: '(String) The name of the function; the identifier does not need to be unique for the schema in which the function is created because UDFs are identified and resolved by the combination of the name and argument types. Check the docs. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            null_input_behavior: '(String) Specifies the behavior of the function when called with null inputs. Valid values are (case-insensitive): CALLED ON NULL INPUT | RETURNS NULL ON NULL INPUT.'
            packages: (Set of String) The name and version number of Snowflake system packages required as dependencies. The value should be of the form package_name:version_number, where package_name is snowflake_domain:package.
            parameters: (List of Object) Outputs the result of SHOW PARAMETERS IN FUNCTION for the given function. (see below for nested schema)
            path_on_stage: (String) Path for import on stage, without the leading /.
            read: (String)
            return_results_behavior: '(String) Specifies the behavior of the function when returning results. Valid values are (case-insensitive): VOLATILE | IMMUTABLE.'
            return_type: (String) Specifies the results returned by the UDF, which determines the UDF type. Use <result_data_type> to create a scalar UDF that returns a single value with the specified data type. Use TABLE (col_name col_data_type, ...) to creates a table UDF that returns tabular results with the specified table column(s) and column type(s). For the details, consult the docs.
            runtime_version: '(String) Specifies the Scala runtime version to use. The supported versions of Scala are: 2.12.'
            schema: '(String) The schema in which to create the function. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            secret_id: (String) Fully qualified name of the allowed secret. You will receive an error if you specify a SECRETS value whose secret isn’t also included in an integration specified by the EXTERNAL_ACCESS_INTEGRATIONS parameter.
            secret_variable_name: (String) The variable that will be used in handler code when retrieving information from the secret.
            secrets: (Block Set) Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from secrets in handler code. Secrets you specify here must be allowed by the external access integration specified as a value of this CREATE FUNCTION command’s EXTERNAL_ACCESS_INTEGRATIONS parameter. (see below for nested schema)
            show_output: (List of Object) Outputs the result of SHOW FUNCTION for the given function. (see below for nested schema)
            stage_location: (String) Stage location without leading @. To use your user's stage set this to ~, otherwise pass fully qualified name of the stage (with every part contained in double quotes or use snowflake_stage.<your stage's resource name>.fully_qualified_name if you manage this stage through terraform).
            target_path: '(Block Set, Max: 1) The name of the handler method or class. If the handler is for a scalar UDF, returning a non-tabular value, the HANDLER value should be a method name, as in the following form: MyClass.myMethod. (see below for nested schema)'
            timeouts: (Block, Optional) (see below for nested schema)
            trace_level: (String) Trace level value to use when generating/filtering trace events For more information, check TRACE_LEVEL docs.
            update: (String)
            valid_for_clustering: (Boolean)
            value: (String)
        importStatements:
            - terraform import snowflake_function_scala.example '"<database_name>"."<schema_name>"."<function_name>"(varchar, varchar, varchar)'
    snowflake_function_sql:
        subCategory: Preview
        description: Resource used to manage sql function objects. For more information, check function documentation https://docs.snowflake.com/en/sql-reference/sql/create-function.
        name: snowflake_function_sql
        title: snowflake_function_sql Resource - terraform-provider-snowflake
        examples:
            - name: minimal
              manifest: |-
                {
                  "arguments": [
                    {
                      "arg_data_type": "FLOAT",
                      "arg_name": "arg_name"
                    }
                  ],
                  "database": "${snowflake_database.test.name}",
                  "function_definition": "arg_name\n",
                  "name": "my_sql_function",
                  "return_type": "FLOAT",
                  "schema": "${snowflake_schema.test.name}"
                }
              references:
                database: snowflake_database.test.name
                schema: snowflake_schema.test.name
            - name: complete
              manifest: |-
                {
                  "arguments": [
                    {
                      "arg_data_type": "FLOAT",
                      "arg_name": "arg_name"
                    }
                  ],
                  "comment": "some comment",
                  "database": "${snowflake_database.test.name}",
                  "function_definition": "arg_name\n",
                  "is_secure": "false",
                  "name": "my_sql_function",
                  "return_results_behavior": "VOLATILE",
                  "return_type": "FLOAT",
                  "schema": "${snowflake_schema.test.name}"
                }
              references:
                database: snowflake_database.test.name
                schema: snowflake_schema.test.name
        argumentDocs:
            arg_data_type: (String) The argument type.
            arg_default_value: (String) Optional default value for the argument. For text values use single quotes. Numeric values can be unquoted. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            arg_name: (String) The argument name. The provider wraps it in double quotes by default, so be aware of that while referencing the argument in the function definition.
            arguments: (Block List) List of the arguments for the function. Consult the docs for more details. (see below for nested schema)
            arguments_raw: (String)
            catalog_name: (String)
            comment: '(String) (Default: user-defined function) Specifies a comment for the function.'
            create: (String)
            created_on: (String)
            database: '(String) The database in which to create the function. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            default: (String)
            delete: (String)
            description: (String)
            enable_console_output: (Boolean) Enable stdout/stderr fast path logging for anonymous stored procs. This is a public parameter (similar to LOG_LEVEL). For more information, check ENABLE_CONSOLE_OUTPUT docs.
            external_access_integrations: (String)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            function_definition: (String) Defines the handler code executed when the UDF is called. Wrapping $$ signs are added by the provider automatically; do not include them. The function_definition value must be SQL source code. For more information, see Introduction to SQL UDFs. To mitigate permadiff on this field, the provider replaces blank characters with a space. This can lead to false positives in cases where a change in case or run of whitespace is semantically significant.
            function_language: (String) Specifies language for the user. Used to detect external changes.
            id: (String) The ID of this resource.
            is_aggregate: (Boolean)
            is_ansi: (Boolean)
            is_builtin: (Boolean)
            is_data_metric: (Boolean)
            is_external_function: (Boolean)
            is_memoizable: (Boolean)
            is_secure: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies that the function is secure. By design, the Snowflake''s SHOW FUNCTIONS command does not provide information about secure functions (consult function docs and Protecting Sensitive Information with Secure UDFs and Stored Procedures) which is essential to manage/import function with Terraform. Use the role owning the function while managing secure functions. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            is_table_function: (Boolean)
            key: (String)
            language: (String)
            level: (String)
            log_level: (String) LOG_LEVEL to use when filtering events For more information, check LOG_LEVEL docs.
            max_num_arguments: (Number)
            metric_level: (String) METRIC_LEVEL value to control whether to emit metrics to Event Table For more information, check METRIC_LEVEL docs.
            min_num_arguments: (Number)
            name: '(String) The name of the function; the identifier does not need to be unique for the schema in which the function is created because UDFs are identified and resolved by the combination of the name and argument types. Check the docs. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            parameters: (List of Object) Outputs the result of SHOW PARAMETERS IN FUNCTION for the given function. (see below for nested schema)
            read: (String)
            return_results_behavior: '(String) Specifies the behavior of the function when returning results. Valid values are (case-insensitive): VOLATILE | IMMUTABLE.'
            return_type: (String) Specifies the results returned by the UDF, which determines the UDF type. Use <result_data_type> to create a scalar UDF that returns a single value with the specified data type. Use TABLE (col_name col_data_type, ...) to creates a table UDF that returns tabular results with the specified table column(s) and column type(s). For the details, consult the docs.
            schema: '(String) The schema in which to create the function. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            secrets: (String)
            show_output: (List of Object) Outputs the result of SHOW FUNCTION for the given function. (see below for nested schema)
            timeouts: (Block, Optional) (see below for nested schema)
            trace_level: (String) Trace level value to use when generating/filtering trace events For more information, check TRACE_LEVEL docs.
            update: (String)
            valid_for_clustering: (Boolean)
            value: (String)
        importStatements:
            - terraform import snowflake_function_sql.example '"<database_name>"."<schema_name>"."<function_name>"(varchar, varchar, varchar)'
    snowflake_git_repository:
        subCategory: Preview
        description: Resource used to manage git repositories. For more information, check git repositories documentation https://docs.snowflake.com/en/sql-reference/sql/create-git-repository.
        name: snowflake_git_repository
        title: snowflake_git_repository Resource - terraform-provider-snowflake
        examples:
            - name: basic
              manifest: |-
                {
                  "api_integration": "API_INTEGRATION",
                  "database": "DATABASE",
                  "name": "GIT_REPOSITORY",
                  "origin": "https://github.com/user/repo",
                  "schema": "SCHEMA"
                }
            - name: complete
              manifest: |-
                {
                  "api_integration": "API_INTEGRATION",
                  "comment": "comment",
                  "database": "DATABASE",
                  "git_credentials": "${snowflake_secret_with_basic_authentication.secret_name.fully_qualified_name}",
                  "name": "GIT_REPOSITORY",
                  "origin": "https://github.com/user/repo",
                  "schema": "SCHEMA"
                }
              references:
                git_credentials: snowflake_secret_with_basic_authentication.secret_name.fully_qualified_name
        argumentDocs:
            api_integration: (String) Identifier of API INTEGRATION containing information about the remote Git repository such as allowed credentials and prefixes for target URLs.
            comment: (String) Specifies a comment for the git repository.
            create: (String)
            created_on: (String)
            database: '(String) The database in which to create the git repository. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            database_name: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE GIT REPOSITORY for the given git repository. (see below for nested schema)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            git_credentials: (String) Specifies the Snowflake secret fully qualified name (e.g "\"<db_name>\".\"<schema_name>\".\"<secret_name>\"") containing the credentials to use for authenticating with the remote Git repository. Omit this parameter to use the default secret specified by the API integration or if this integration does not require authentication.
            id: (String) The ID of this resource.
            last_fetched_at: (String)
            name: '(String) Specifies the identifier for the git repository; must be unique for the schema in which the git repository is created. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            origin: (String) Specifies the origin URL of the remote Git repository that this Git repository clone represents. The URL must use HTTPS.
            owner: (String)
            owner_role_type: (String)
            read: (String)
            schema: '(String) The schema in which to create the git repository. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            show_output: (List of Object) Outputs the result of SHOW GIT REPOSITORIES for the given git repository. (see below for nested schema)
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
        importStatements:
            - terraform import snowflake_git_repository.example '"<db_name>"."<schema_name>"."<git_repository_name>"'
    snowflake_grant_account_role:
        subCategory: Stable
        name: snowflake_grant_account_role
        title: snowflake_grant_account_role Resource - terraform-provider-snowflake
        examples:
            - name: g
              manifest: |-
                {
                  "parent_role_name": "${snowflake_account_role.parent_role.name}",
                  "role_name": "${snowflake_account_role.role.name}"
                }
              references:
                parent_role_name: snowflake_account_role.parent_role.name
                role_name: snowflake_account_role.role.name
              dependencies:
                snowflake_account_role.parent_role: |-
                    {
                      "name": "PARENT_ROLE"
                    }
                snowflake_account_role.role: |-
                    {
                      "name": "ROLE"
                    }
                snowflake_user.user: |-
                    {
                      "name": "USER"
                    }
            - name: g
              manifest: |-
                {
                  "role_name": "${snowflake_account_role.role.name}",
                  "user_name": "${snowflake_user.user.name}"
                }
              references:
                role_name: snowflake_account_role.role.name
                user_name: snowflake_user.user.name
              dependencies:
                snowflake_account_role.parent_role: |-
                    {
                      "name": "PARENT_ROLE"
                    }
                snowflake_account_role.role: |-
                    {
                      "name": "ROLE"
                    }
                snowflake_user.user: |-
                    {
                      "name": "USER"
                    }
        argumentDocs:
            create: (String)
            delete: (String)
            id: (String) The ID of this resource.
            parent_role_name: (String) The fully qualified name of the parent role which will create a parent-child relationship between the roles. For more information about this resource, see docs.
            read: (String)
            role_name: (String) The fully qualified name of the role which will be granted to the user or parent role. For more information about this resource, see docs.
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
            user_name: (String) The fully qualified name of the user on which specified role will be granted. For more information about this resource, see docs.
        importStatements:
            - |-
              # format is role_name (string) | grantee_object_type (ROLE|USER) | grantee_name (string)
              terraform import snowflake_grant_account_role.example '"test_role"|ROLE|"test_parent_role"'
    snowflake_grant_application_role:
        subCategory: Stable
        name: snowflake_grant_application_role
        title: snowflake_grant_application_role Resource - terraform-provider-snowflake
        examples:
            - name: g
              manifest: |-
                {
                  "application_role_name": "${local.application_role_identifier}",
                  "parent_account_role_name": "${snowflake_account_role.role.name}"
                }
              references:
                application_role_name: local.application_role_identifier
                parent_account_role_name: snowflake_account_role.role.name
              dependencies:
                snowflake_account_role.role: |-
                    {
                      "name": "my_role"
                    }
            - name: g
              manifest: |-
                {
                  "application_name": "my_second_application",
                  "application_role_name": "${local.application_role_identifier}"
                }
              references:
                application_role_name: local.application_role_identifier
              dependencies:
                snowflake_account_role.role: |-
                    {
                      "name": "my_role"
                    }
        argumentDocs:
            application_name: (String) The fully qualified name of the application on which application role will be granted.
            application_role_name: (String) Specifies the identifier for the application role to grant.
            create: (String)
            delete: (String)
            id: (String) The ID of this resource.
            parent_account_role_name: (String) The fully qualified name of the account role on which application role will be granted. For more information about this resource, see docs.
            read: (String)
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
        importStatements:
            - |-
              # format is application_role_name (string) | object_type (ACCOUNT_ROLE|APPLICATION) | grantee_name (string)
              terraform import snowflake_grant_application_role.example '"my_application"."app_role_1"|ACCOUNT_ROLE|"my_role"'
    snowflake_grant_database_role:
        subCategory: Stable
        name: snowflake_grant_database_role
        title: snowflake_grant_database_role Resource - terraform-provider-snowflake
        examples:
            - name: g
              manifest: |-
                {
                  "database_role_name": "\"${var.database}\".\"${snowflake_database_role.database_role.name}\"",
                  "parent_role_name": "${snowflake_account_role.parent_role.name}"
                }
              references:
                parent_role_name: snowflake_account_role.parent_role.name
              dependencies:
                snowflake_account_role.parent_role: |-
                    {
                      "name": "${var.parent_role_name}"
                    }
                snowflake_database_role.database_role: |-
                    {
                      "database": "${var.database}",
                      "name": "${var.database_role_name}"
                    }
                snowflake_database_role.parent_database_role: |-
                    {
                      "database": "${var.database}",
                      "name": "${var.parent_database_role_name}"
                    }
            - name: g
              manifest: |-
                {
                  "database_role_name": "\"${var.database}\".\"${snowflake_database_role.database_role.name}\"",
                  "parent_database_role_name": "\"${var.database}\".\"${snowflake_database_role.parent_database_role.name}\""
                }
              dependencies:
                snowflake_account_role.parent_role: |-
                    {
                      "name": "${var.parent_role_name}"
                    }
                snowflake_database_role.database_role: |-
                    {
                      "database": "${var.database}",
                      "name": "${var.database_role_name}"
                    }
                snowflake_database_role.parent_database_role: |-
                    {
                      "database": "${var.database}",
                      "name": "${var.parent_database_role_name}"
                    }
            - name: g
              manifest: |-
                {
                  "database_role_name": "\"${var.database}\".\"${snowflake_database_role.database_role.name}\"",
                  "share_name": "${snowflake_share.share.name}"
                }
              references:
                share_name: snowflake_share.share.name
              dependencies:
                snowflake_account_role.parent_role: |-
                    {
                      "name": "${var.parent_role_name}"
                    }
                snowflake_database_role.database_role: |-
                    {
                      "database": "${var.database}",
                      "name": "${var.database_role_name}"
                    }
                snowflake_database_role.parent_database_role: |-
                    {
                      "database": "${var.database}",
                      "name": "${var.parent_database_role_name}"
                    }
        argumentDocs:
            create: (String)
            database_role_name: (String) The fully qualified name of the database role which will be granted to share or parent role. For more information about this resource, see docs.
            delete: (String)
            id: (String) The ID of this resource.
            parent_database_role_name: (String) The fully qualified name of the parent database role which will create a parent-child relationship between the roles. For more information about this resource, see docs.
            parent_role_name: (String) The fully qualified name of the parent account role which will create a parent-child relationship between the roles. For more information about this resource, see docs.
            read: (String)
            share_name: (String) The fully qualified name of the share on which privileges will be granted. For more information about this resource, see docs.
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
        importStatements:
            - |-
              # format is database_role_name (string) | object_type (ROLE|DATABASE ROLE|SHARE) | grantee_name (string)
              terraform import snowflake_grant_database_role.example '"ABC"."test_db_role"|ROLE|"test_parent_role"'
    snowflake_grant_ownership:
        subCategory: Stable
        name: snowflake_grant_ownership
        title: snowflake_grant_ownership Resource - terraform-provider-snowflake
        examples:
            - name: test
              manifest: |-
                {
                  "account_role_name": "${snowflake_role.test.name}",
                  "on": [
                    {
                      "object_name": "\"${snowflake_database.test.name}\".\"${snowflake_schema.test.name}\"",
                      "object_type": "SCHEMA"
                    }
                  ],
                  "outbound_privileges": "COPY"
                }
              references:
                account_role_name: snowflake_role.test.name
              dependencies:
                snowflake_account_role.test: |-
                    {
                      "name": "role"
                    }
                snowflake_database.test: |-
                    {
                      "name": "database"
                    }
                snowflake_database_role.test: |-
                    {
                      "database": "${snowflake_database.test.name}",
                      "name": "test_database_role"
                    }
                snowflake_grant_account_role.test: |-
                    {
                      "role_name": "${snowflake_account_role.test.name}",
                      "user_name": "username"
                    }
                snowflake_schema.test: |-
                    {
                      "database": "${snowflake_database.test.name}",
                      "depends_on": [
                        "${snowflake_grant_ownership.test}",
                        "${snowflake_grant_account_role.test}"
                      ],
                      "name": "schema",
                      "provider": "${snowflake.secondary}"
                    }
            - name: test
              manifest: |-
                {
                  "database_role_name": "${snowflake_database_role.test.fully_qualified_name}",
                  "on": [
                    {
                      "object_name": "\"${snowflake_database.test.name}\".\"${snowflake_schema.test.name}\"",
                      "object_type": "SCHEMA"
                    }
                  ],
                  "outbound_privileges": "REVOKE"
                }
              references:
                database_role_name: snowflake_database_role.test.fully_qualified_name
              dependencies:
                snowflake_account_role.test: |-
                    {
                      "name": "role"
                    }
                snowflake_database.test: |-
                    {
                      "name": "database"
                    }
                snowflake_database_role.test: |-
                    {
                      "database": "${snowflake_database.test.name}",
                      "name": "test_database_role"
                    }
                snowflake_grant_account_role.test: |-
                    {
                      "role_name": "${snowflake_account_role.test.name}",
                      "user_name": "username"
                    }
                snowflake_schema.test: |-
                    {
                      "database": "${snowflake_database.test.name}",
                      "depends_on": [
                        "${snowflake_grant_ownership.test}",
                        "${snowflake_grant_account_role.test}"
                      ],
                      "name": "schema",
                      "provider": "${snowflake.secondary}"
                    }
            - name: test
              manifest: |-
                {
                  "account_role_name": "${snowflake_role.test.name}",
                  "on": [
                    {
                      "all": [
                        {
                          "in_database": "${snowflake_database.test.name}",
                          "object_type_plural": "TABLES"
                        }
                      ]
                    }
                  ]
                }
              references:
                account_role_name: snowflake_role.test.name
                on.all.in_database: snowflake_database.test.name
              dependencies:
                snowflake_account_role.test: |-
                    {
                      "name": "role"
                    }
                snowflake_database.test: |-
                    {
                      "name": "database"
                    }
                snowflake_database_role.test: |-
                    {
                      "database": "${snowflake_database.test.name}",
                      "name": "test_database_role"
                    }
                snowflake_grant_account_role.test: |-
                    {
                      "role_name": "${snowflake_account_role.test.name}",
                      "user_name": "username"
                    }
                snowflake_schema.test: |-
                    {
                      "database": "${snowflake_database.test.name}",
                      "depends_on": [
                        "${snowflake_grant_ownership.test}",
                        "${snowflake_grant_account_role.test}"
                      ],
                      "name": "schema",
                      "provider": "${snowflake.secondary}"
                    }
            - name: test
              manifest: |-
                {
                  "account_role_name": "${snowflake_role.test.name}",
                  "on": [
                    {
                      "all": [
                        {
                          "in_schema": "\"${snowflake_database.test.name}\".\"${snowflake_schema.test.name}\"",
                          "object_type_plural": "TABLES"
                        }
                      ]
                    }
                  ]
                }
              references:
                account_role_name: snowflake_role.test.name
              dependencies:
                snowflake_account_role.test: |-
                    {
                      "name": "role"
                    }
                snowflake_database.test: |-
                    {
                      "name": "database"
                    }
                snowflake_database_role.test: |-
                    {
                      "database": "${snowflake_database.test.name}",
                      "name": "test_database_role"
                    }
                snowflake_grant_account_role.test: |-
                    {
                      "role_name": "${snowflake_account_role.test.name}",
                      "user_name": "username"
                    }
                snowflake_schema.test: |-
                    {
                      "database": "${snowflake_database.test.name}",
                      "depends_on": [
                        "${snowflake_grant_ownership.test}",
                        "${snowflake_grant_account_role.test}"
                      ],
                      "name": "schema",
                      "provider": "${snowflake.secondary}"
                    }
            - name: test
              manifest: |-
                {
                  "account_role_name": "${snowflake_role.test.name}",
                  "on": [
                    {
                      "future": [
                        {
                          "in_database": "${snowflake_database.test.name}",
                          "object_type_plural": "TABLES"
                        }
                      ]
                    }
                  ]
                }
              references:
                account_role_name: snowflake_role.test.name
                on.future.in_database: snowflake_database.test.name
              dependencies:
                snowflake_account_role.test: |-
                    {
                      "name": "role"
                    }
                snowflake_database.test: |-
                    {
                      "name": "database"
                    }
                snowflake_database_role.test: |-
                    {
                      "database": "${snowflake_database.test.name}",
                      "name": "test_database_role"
                    }
                snowflake_grant_account_role.test: |-
                    {
                      "role_name": "${snowflake_account_role.test.name}",
                      "user_name": "username"
                    }
                snowflake_schema.test: |-
                    {
                      "database": "${snowflake_database.test.name}",
                      "depends_on": [
                        "${snowflake_grant_ownership.test}",
                        "${snowflake_grant_account_role.test}"
                      ],
                      "name": "schema",
                      "provider": "${snowflake.secondary}"
                    }
            - name: test
              manifest: |-
                {
                  "account_role_name": "${snowflake_role.test.name}",
                  "on": [
                    {
                      "future": [
                        {
                          "in_schema": "\"${snowflake_database.test.name}\".\"${snowflake_schema.test.name}\"",
                          "object_type_plural": "TABLES"
                        }
                      ]
                    }
                  ]
                }
              references:
                account_role_name: snowflake_role.test.name
              dependencies:
                snowflake_account_role.test: |-
                    {
                      "name": "role"
                    }
                snowflake_database.test: |-
                    {
                      "name": "database"
                    }
                snowflake_database_role.test: |-
                    {
                      "database": "${snowflake_database.test.name}",
                      "name": "test_database_role"
                    }
                snowflake_grant_account_role.test: |-
                    {
                      "role_name": "${snowflake_account_role.test.name}",
                      "user_name": "username"
                    }
                snowflake_schema.test: |-
                    {
                      "database": "${snowflake_database.test.name}",
                      "depends_on": [
                        "${snowflake_grant_ownership.test}",
                        "${snowflake_grant_account_role.test}"
                      ],
                      "name": "schema",
                      "provider": "${snowflake.secondary}"
                    }
            - name: test
              manifest: |-
                {
                  "account_role_name": "${snowflake_account_role.test.name}",
                  "on": [
                    {
                      "object_name": "${snowflake_database.test.name}",
                      "object_type": "DATABASE"
                    }
                  ]
                }
              references:
                account_role_name: snowflake_account_role.test.name
                on.object_name: snowflake_database.test.name
              dependencies:
                snowflake_account_role.test: |-
                    {
                      "name": "role"
                    }
                snowflake_database.test: |-
                    {
                      "name": "database"
                    }
                snowflake_database_role.test: |-
                    {
                      "database": "${snowflake_database.test.name}",
                      "name": "test_database_role"
                    }
                snowflake_grant_account_role.test: |-
                    {
                      "role_name": "${snowflake_account_role.test.name}",
                      "user_name": "username"
                    }
                snowflake_schema.test: |-
                    {
                      "database": "${snowflake_database.test.name}",
                      "depends_on": [
                        "${snowflake_grant_ownership.test}",
                        "${snowflake_grant_account_role.test}"
                      ],
                      "name": "schema",
                      "provider": "${snowflake.secondary}"
                    }
        argumentDocs:
            account_role_name: (String) The fully qualified name of the account role to which privileges will be granted. For more information about this resource, see docs.
            all: '(Block List, Max: 1) Configures the privilege to be granted on all objects in either a database or schema. (see below for nested schema)'
            create: (String)
            database_role_name: (String) The fully qualified name of the database role to which privileges will be granted. For more information about this resource, see docs.
            delete: (String)
            future: '(Block List, Max: 1) Configures the privilege to be granted on all objects in either a database or schema. (see below for nested schema)'
            id: (String) The ID of this resource.
            in_database: (String) The fully qualified name of the database. For more information about this resource, see docs.
            in_schema: (String) The fully qualified name of the schema. For more information about this resource, see docs.
            object_name: (String) Specifies the identifier for the object on which you are transferring ownership.
            object_type: '(String) Specifies the type of object on which you are transferring ownership. Available values are: AGGREGATION POLICY | ALERT | AUTHENTICATION POLICY | COMPUTE POOL | DATA METRIC FUNCTION | DATABASE | DATABASE ROLE | DYNAMIC TABLE | EVENT TABLE | EXTERNAL TABLE | EXTERNAL VOLUME | FAILOVER GROUP | FILE FORMAT | FUNCTION | GIT REPOSITORY | HYBRID TABLE | ICEBERG TABLE | IMAGE REPOSITORY | INTEGRATION | MATERIALIZED VIEW | NETWORK POLICY | NETWORK RULE | PACKAGES POLICY | PIPE | PROCEDURE | MASKING POLICY | PASSWORD POLICY | PROJECTION POLICY | REPLICATION GROUP | RESOURCE MONITOR | ROLE | ROW ACCESS POLICY | SCHEMA | SESSION POLICY | SECRET | SEQUENCE | STAGE | STREAM | TABLE | TAG | TASK | USER | VIEW | WAREHOUSE'
            object_type_plural: '(String) Specifies the type of object in plural form on which you are transferring ownership. Available values are: AGGREGATION POLICIES | ALERTS | AUTHENTICATION POLICIES | COMPUTE POOLS | DATA METRIC FUNCTIONS | DATABASES | DATABASE ROLES | DYNAMIC TABLES | EVENT TABLES | EXTERNAL TABLES | EXTERNAL VOLUMES | FAILOVER GROUPS | FILE FORMATS | FUNCTIONS | GIT REPOSITORIES | HYBRID TABLES | ICEBERG TABLES | IMAGE REPOSITORIES | INTEGRATIONS | MATERIALIZED VIEWS | NETWORK POLICIES | NETWORK RULES | PACKAGES POLICIES | PIPES | PROCEDURES | MASKING POLICIES | PASSWORD POLICIES | PROJECTION POLICIES | REPLICATION GROUPS | RESOURCE MONITORS | ROLES | ROW ACCESS POLICIES | SCHEMAS | SESSION POLICIES | SECRETS | SEQUENCES | STAGES | STREAMS | TABLES | TAGS | TASKS | USERS | VIEWS | WAREHOUSES. For more information head over to Snowflake documentation.'
            "on": '(Block List, Min: 1, Max: 1) Configures which object(s) should transfer their ownership to the specified role. (see below for nested schema)'
            outbound_privileges: field is set to COPY.
            outbound_privileges = "COPY": |-
                if you want to move grants automatically to the owner (also enables the provider to resume the task automatically)
                If originally the first owner won't be granted with OPERATE, USAGE (on the warehouse), EXECUTE TASK (on the account), and outbound privileges won't be set to COPY, then you have to resume suspended tasks manually.
            read: (String)
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
        importStatements: []
    snowflake_grant_privileges_to_account_role:
        subCategory: Stable
        name: snowflake_grant_privileges_to_account_role
        title: snowflake_grant_privileges_to_account_role Resource - terraform-provider-snowflake
        examples:
            - name: example
              manifest: |-
                {
                  "account_role_name": "${snowflake_account_role.db_role.name}",
                  "on_account": true,
                  "privileges": [
                    "CREATE DATABASE",
                    "CREATE USER"
                  ]
                }
              references:
                account_role_name: snowflake_account_role.db_role.name
              dependencies:
                snowflake_account_role.db_role: |-
                    {
                      "name": "role_name"
                    }
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
            - name: example
              manifest: |-
                {
                  "account_role_name": "${snowflake_account_role.db_role.name}",
                  "all_privileges": true,
                  "on_account": true,
                  "with_grant_option": true
                }
              references:
                account_role_name: snowflake_account_role.db_role.name
              dependencies:
                snowflake_account_role.db_role: |-
                    {
                      "name": "role_name"
                    }
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
            - name: example
              manifest: |-
                {
                  "account_role_name": "${snowflake_account_role.db_role.name}",
                  "all_privileges": true,
                  "always_apply": true,
                  "on_account": true,
                  "with_grant_option": true
                }
              references:
                account_role_name: snowflake_account_role.db_role.name
              dependencies:
                snowflake_account_role.db_role: |-
                    {
                      "name": "role_name"
                    }
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
            - name: example
              manifest: |-
                {
                  "account_role_name": "${snowflake_account_role.db_role.name}",
                  "on_account_object": [
                    {
                      "object_name": "${snowflake_database.db.name}",
                      "object_type": "DATABASE"
                    }
                  ],
                  "privileges": [
                    "CREATE SCHEMA",
                    "CREATE DATABASE ROLE"
                  ]
                }
              references:
                account_role_name: snowflake_account_role.db_role.name
                on_account_object.object_name: snowflake_database.db.name
              dependencies:
                snowflake_account_role.db_role: |-
                    {
                      "name": "role_name"
                    }
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
            - name: example
              manifest: |-
                {
                  "account_role_name": "${snowflake_account_role.db_role.name}",
                  "all_privileges": true,
                  "on_account_object": [
                    {
                      "object_name": "${snowflake_database.db.name}",
                      "object_type": "DATABASE"
                    }
                  ],
                  "with_grant_option": true
                }
              references:
                account_role_name: snowflake_account_role.db_role.name
                on_account_object.object_name: snowflake_database.db.name
              dependencies:
                snowflake_account_role.db_role: |-
                    {
                      "name": "role_name"
                    }
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
            - name: example
              manifest: |-
                {
                  "account_role_name": "${snowflake_account_role.db_role.name}",
                  "on_account_object": [
                    {
                      "object_name": "SNOWFLAKE",
                      "object_type": "DATABASE"
                    }
                  ],
                  "privileges": [
                    "IMPORTED PRIVILEGES"
                  ]
                }
              references:
                account_role_name: snowflake_account_role.db_role.name
              dependencies:
                snowflake_account_role.db_role: |-
                    {
                      "name": "role_name"
                    }
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
            - name: example
              manifest: |-
                {
                  "account_role_name": "${snowflake_account_role.db_role.name}",
                  "all_privileges": true,
                  "always_apply": true,
                  "on_account_object": [
                    {
                      "object_name": "${snowflake_database.db.name}",
                      "object_type": "DATABASE"
                    }
                  ],
                  "with_grant_option": true
                }
              references:
                account_role_name: snowflake_account_role.db_role.name
                on_account_object.object_name: snowflake_database.db.name
              dependencies:
                snowflake_account_role.db_role: |-
                    {
                      "name": "role_name"
                    }
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
            - name: example
              manifest: |-
                {
                  "account_role_name": "${snowflake_account_role.db_role.name}",
                  "on_schema": [
                    {
                      "schema_name": "${snowflake_schema.my_schema.fully_qualified_name}"
                    }
                  ],
                  "privileges": [
                    "MODIFY",
                    "CREATE TABLE"
                  ]
                }
              references:
                account_role_name: snowflake_account_role.db_role.name
                on_schema.schema_name: snowflake_schema.my_schema.fully_qualified_name
              dependencies:
                snowflake_account_role.db_role: |-
                    {
                      "name": "role_name"
                    }
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
            - name: example
              manifest: |-
                {
                  "account_role_name": "${snowflake_account_role.db_role.name}",
                  "all_privileges": true,
                  "on_schema": [
                    {
                      "schema_name": "${snowflake_schema.my_schema.fully_qualified_name}"
                    }
                  ],
                  "with_grant_option": true
                }
              references:
                account_role_name: snowflake_account_role.db_role.name
                on_schema.schema_name: snowflake_schema.my_schema.fully_qualified_name
              dependencies:
                snowflake_account_role.db_role: |-
                    {
                      "name": "role_name"
                    }
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
            - name: example
              manifest: |-
                {
                  "account_role_name": "${snowflake_account_role.db_role.name}",
                  "on_schema": [
                    {
                      "all_schemas_in_database": "${snowflake_database.db.name}"
                    }
                  ],
                  "privileges": [
                    "MODIFY",
                    "CREATE TABLE"
                  ]
                }
              references:
                account_role_name: snowflake_account_role.db_role.name
                on_schema.all_schemas_in_database: snowflake_database.db.name
              dependencies:
                snowflake_account_role.db_role: |-
                    {
                      "name": "role_name"
                    }
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
            - name: example
              manifest: |-
                {
                  "account_role_name": "${snowflake_account_role.db_role.name}",
                  "on_schema": [
                    {
                      "future_schemas_in_database": "${snowflake_database.db.name}"
                    }
                  ],
                  "privileges": [
                    "MODIFY",
                    "CREATE TABLE"
                  ]
                }
              references:
                account_role_name: snowflake_account_role.db_role.name
                on_schema.future_schemas_in_database: snowflake_database.db.name
              dependencies:
                snowflake_account_role.db_role: |-
                    {
                      "name": "role_name"
                    }
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
            - name: example
              manifest: |-
                {
                  "account_role_name": "${snowflake_account_role.db_role.name}",
                  "on_schema_object": [
                    {
                      "object_name": "${snowflake_view.my_view.fully_qualified_name}",
                      "object_type": "VIEW"
                    }
                  ],
                  "privileges": [
                    "SELECT",
                    "REFERENCES"
                  ]
                }
              references:
                account_role_name: snowflake_account_role.db_role.name
                on_schema_object.object_name: snowflake_view.my_view.fully_qualified_name
              dependencies:
                snowflake_account_role.db_role: |-
                    {
                      "name": "role_name"
                    }
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
            - name: example
              manifest: |-
                {
                  "account_role_name": "${snowflake_account_role.db_role.name}",
                  "all_privileges": true,
                  "on_schema_object": [
                    {
                      "object_name": "${snowflake_view.my_view.fully_qualified_name}",
                      "object_type": "VIEW"
                    }
                  ],
                  "with_grant_option": true
                }
              references:
                account_role_name: snowflake_account_role.db_role.name
                on_schema_object.object_name: snowflake_view.my_view.fully_qualified_name
              dependencies:
                snowflake_account_role.db_role: |-
                    {
                      "name": "role_name"
                    }
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
            - name: example
              manifest: |-
                {
                  "account_role_name": "${snowflake_account_role.db_role.name}",
                  "on_schema_object": [
                    {
                      "all": [
                        {
                          "in_database": "${snowflake_database.db.name}",
                          "object_type_plural": "TABLES"
                        }
                      ]
                    }
                  ],
                  "privileges": [
                    "SELECT",
                    "INSERT"
                  ]
                }
              references:
                account_role_name: snowflake_account_role.db_role.name
                on_schema_object.all.in_database: snowflake_database.db.name
              dependencies:
                snowflake_account_role.db_role: |-
                    {
                      "name": "role_name"
                    }
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
            - name: example
              manifest: |-
                {
                  "account_role_name": "${snowflake_account_role.db_role.name}",
                  "on_schema_object": [
                    {
                      "all": [
                        {
                          "in_schema": "${snowflake_schema.my_schema.fully_qualified_name}",
                          "object_type_plural": "TABLES"
                        }
                      ]
                    }
                  ],
                  "privileges": [
                    "SELECT",
                    "INSERT"
                  ]
                }
              references:
                account_role_name: snowflake_account_role.db_role.name
                on_schema_object.all.in_schema: snowflake_schema.my_schema.fully_qualified_name
              dependencies:
                snowflake_account_role.db_role: |-
                    {
                      "name": "role_name"
                    }
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
            - name: example
              manifest: |-
                {
                  "account_role_name": "${snowflake_account_role.db_role.name}",
                  "on_schema_object": [
                    {
                      "future": [
                        {
                          "in_database": "${snowflake_database.db.name}",
                          "object_type_plural": "TABLES"
                        }
                      ]
                    }
                  ],
                  "privileges": [
                    "SELECT",
                    "INSERT"
                  ]
                }
              references:
                account_role_name: snowflake_account_role.db_role.name
                on_schema_object.future.in_database: snowflake_database.db.name
              dependencies:
                snowflake_account_role.db_role: |-
                    {
                      "name": "role_name"
                    }
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
            - name: example
              manifest: |-
                {
                  "account_role_name": "${snowflake_account_role.db_role.name}",
                  "on_schema_object": [
                    {
                      "future": [
                        {
                          "in_schema": "${snowflake_schema.my_schema.fully_qualified_name}",
                          "object_type_plural": "TABLES"
                        }
                      ]
                    }
                  ],
                  "privileges": [
                    "SELECT",
                    "INSERT"
                  ]
                }
              references:
                account_role_name: snowflake_account_role.db_role.name
                on_schema_object.future.in_schema: snowflake_schema.my_schema.fully_qualified_name
              dependencies:
                snowflake_account_role.db_role: |-
                    {
                      "name": "role_name"
                    }
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
        argumentDocs:
            CREATE SNOWFLAKE.ML.ANOMALY_DETECTION: or CREATE SNOWFLAKE.ML.FORECAST privileges on schema results in a permadiff because of the probably incorrect Snowflake's behavior of SHOW GRANTS ON <object_type> <object_name>. More in the comment.
            account_role_name: (String) The fully qualified name of the account role to which privileges will be granted. For more information about this resource, see docs.
            all: '(Block List, Max: 1) Configures the privilege to be granted on all objects in either a database or schema. (see below for nested schema)'
            all_privileges: '(Boolean) (Default: false) Grant all privileges on the account role. When all privileges cannot be granted, the provider returns a warning, which is aligned with the Snowsight behavior.'
            all_schemas_in_database: (String) The fully qualified name of the database.
            always_apply: '(Boolean) (Default: false) If true, the resource will always produce a “plan” and on “apply” it will re-grant defined privileges. It is supposed to be used only in “grant privileges on all X’s in database / schema Y” or “grant all privileges to X” scenarios to make sure that every new object in a given database / schema is granted by the account role and every new privilege is granted to the database role. Important note: this flag is not compliant with the Terraform assumptions of the config being eventually convergent (producing an empty plan).'
            always_apply_trigger: '(String) (Default: ``) This is a helper field and should not be set. Its main purpose is to help to achieve the functionality described by the always_apply field.'
            create: (String)
            delete: (String)
            future: '(Block List, Max: 1) Configures the privilege to be granted on future objects in either a database or schema. (see below for nested schema)'
            future_schemas_in_database: (String) The fully qualified name of the database.
            id: (String) The ID of this resource.
            in_database: (String)
            in_schema: (String)
            object_name: (String) The fully qualified name of the object on which privileges will be granted.
            object_type: '(String) The object type of the account object on which privileges will be granted. Valid values are: USER | RESOURCE MONITOR | WAREHOUSE | COMPUTE POOL | DATABASE | INTEGRATION | FAILOVER GROUP | REPLICATION GROUP | EXTERNAL VOLUME'
            object_type_plural: '(String) The plural object type of the schema object on which privileges will be granted. Valid values are: AGGREGATION POLICIES | ALERTS | AUTHENTICATION POLICIES | CORTEX SEARCH SERVICES | DATA METRIC FUNCTIONS | DYNAMIC TABLES | EVENT TABLES | EXTERNAL TABLES | FILE FORMATS | FUNCTIONS | GIT REPOSITORIES | HYBRID TABLES | IMAGE REPOSITORIES | ICEBERG TABLES | MASKING POLICIES | MATERIALIZED VIEWS | MODELS | NETWORK RULES | NOTEBOOKS | PACKAGES POLICIES | PASSWORD POLICIES | PIPES | PROCEDURES | PROJECTION POLICIES | ROW ACCESS POLICIES | SECRETS | SERVICES | SESSION POLICIES | SEQUENCES | SNAPSHOTS | STAGES | STREAMS | TABLES | TAGS | TASKS | VIEWS | STREAMLITS | DATASETS.'
            on_account: '(Boolean) (Default: false) If true, the privileges will be granted on the account.'
            on_account_object: '(Block List, Max: 1) Specifies the account object on which privileges will be granted (see below for nested schema)'
            on_schema: '(Block List, Max: 1) Specifies the schema on which privileges will be granted. (see below for nested schema)'
            on_schema_object: '(Block List, Max: 1) Specifies the schema object on which privileges will be granted. (see below for nested schema)'
            privileges: (Set of String) The privileges to grant on the account role. This field is case-sensitive; use only upper-case privileges.
            read: (String)
            schema_name: (String) The fully qualified name of the schema.
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
            with_grant_option: '(Boolean) (Default: false) Specifies whether the grantee can grant the privileges to other users.'
        importStatements: []
    snowflake_grant_privileges_to_database_role:
        subCategory: Stable
        name: snowflake_grant_privileges_to_database_role
        title: snowflake_grant_privileges_to_database_role Resource - terraform-provider-snowflake
        examples:
            - name: example
              manifest: |-
                {
                  "database_role_name": "${snowflake_database_role.db_role.fully_qualified_name}",
                  "on_database": "${snowflake_database_role.db_role.database}",
                  "privileges": [
                    "CREATE",
                    "MONITOR"
                  ]
                }
              references:
                database_role_name: snowflake_database_role.db_role.fully_qualified_name
                on_database: snowflake_database_role.db_role.database
              dependencies:
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_database_role.db_role: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "db_role_name"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
            - name: example
              manifest: |-
                {
                  "all_privileges": true,
                  "database_role_name": "${snowflake_database_role.db_role.fully_qualified_name}",
                  "on_database": "${snowflake_database_role.db_role.database}",
                  "with_grant_option": true
                }
              references:
                database_role_name: snowflake_database_role.db_role.fully_qualified_name
                on_database: snowflake_database_role.db_role.database
              dependencies:
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_database_role.db_role: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "db_role_name"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
            - name: example
              manifest: |-
                {
                  "all_privileges": true,
                  "always_apply": true,
                  "database_role_name": "${snowflake_database_role.db_role.fully_qualified_name}",
                  "on_database": "${snowflake_database_role.db_role.database}",
                  "with_grant_option": true
                }
              references:
                database_role_name: snowflake_database_role.db_role.fully_qualified_name
                on_database: snowflake_database_role.db_role.database
              dependencies:
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_database_role.db_role: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "db_role_name"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
            - name: example
              manifest: |-
                {
                  "database_role_name": "${snowflake_database_role.db_role.fully_qualified_name}",
                  "on_schema": [
                    {
                      "schema_name": "${snowflake_schema.my_schema.fully_qualified_name}"
                    }
                  ],
                  "privileges": [
                    "MODIFY",
                    "CREATE TABLE"
                  ]
                }
              references:
                database_role_name: snowflake_database_role.db_role.fully_qualified_name
                on_schema.schema_name: snowflake_schema.my_schema.fully_qualified_name
              dependencies:
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_database_role.db_role: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "db_role_name"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
            - name: example
              manifest: |-
                {
                  "all_privileges": true,
                  "database_role_name": "${snowflake_database_role.db_role.fully_qualified_name}",
                  "on_schema": [
                    {
                      "schema_name": "${snowflake_schema.my_schema.fully_qualified_name}"
                    }
                  ],
                  "with_grant_option": true
                }
              references:
                database_role_name: snowflake_database_role.db_role.fully_qualified_name
                on_schema.schema_name: snowflake_schema.my_schema.fully_qualified_name
              dependencies:
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_database_role.db_role: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "db_role_name"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
            - name: example
              manifest: |-
                {
                  "database_role_name": "${snowflake_database_role.db_role.fully_qualified_name}",
                  "on_schema": [
                    {
                      "all_schemas_in_database": "${snowflake_database_role.db_role.database}"
                    }
                  ],
                  "privileges": [
                    "MODIFY",
                    "CREATE TABLE"
                  ]
                }
              references:
                database_role_name: snowflake_database_role.db_role.fully_qualified_name
                on_schema.all_schemas_in_database: snowflake_database_role.db_role.database
              dependencies:
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_database_role.db_role: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "db_role_name"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
            - name: example
              manifest: |-
                {
                  "database_role_name": "${snowflake_database_role.db_role.fully_qualified_name}",
                  "on_schema": [
                    {
                      "future_schemas_in_database": "${snowflake_database_role.db_role.database}"
                    }
                  ],
                  "privileges": [
                    "MODIFY",
                    "CREATE TABLE"
                  ]
                }
              references:
                database_role_name: snowflake_database_role.db_role.fully_qualified_name
                on_schema.future_schemas_in_database: snowflake_database_role.db_role.database
              dependencies:
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_database_role.db_role: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "db_role_name"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
            - name: example
              manifest: |-
                {
                  "database_role_name": "${snowflake_database_role.db_role.fully_qualified_name}",
                  "on_schema_object": [
                    {
                      "object_name": "${snowflake_view.my_view.fully_qualified_name}",
                      "object_type": "VIEW"
                    }
                  ],
                  "privileges": [
                    "SELECT",
                    "REFERENCES"
                  ]
                }
              references:
                database_role_name: snowflake_database_role.db_role.fully_qualified_name
                on_schema_object.object_name: snowflake_view.my_view.fully_qualified_name
              dependencies:
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_database_role.db_role: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "db_role_name"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
            - name: example
              manifest: |-
                {
                  "all_privileges": true,
                  "database_role_name": "${snowflake_database_role.db_role.fully_qualified_name}",
                  "on_schema_object": [
                    {
                      "object_name": "${snowflake_view.my_view.fully_qualified_name}",
                      "object_type": "VIEW"
                    }
                  ],
                  "with_grant_option": true
                }
              references:
                database_role_name: snowflake_database_role.db_role.fully_qualified_name
                on_schema_object.object_name: snowflake_view.my_view.fully_qualified_name
              dependencies:
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_database_role.db_role: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "db_role_name"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
            - name: example
              manifest: |-
                {
                  "database_role_name": "${snowflake_database_role.db_role.fully_qualified_name}",
                  "on_schema_object": [
                    {
                      "all": [
                        {
                          "in_database": "${snowflake_database_role.db_role.database}",
                          "object_type_plural": "TABLES"
                        }
                      ]
                    }
                  ],
                  "privileges": [
                    "SELECT",
                    "INSERT"
                  ]
                }
              references:
                database_role_name: snowflake_database_role.db_role.fully_qualified_name
                on_schema_object.all.in_database: snowflake_database_role.db_role.database
              dependencies:
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_database_role.db_role: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "db_role_name"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
            - name: example
              manifest: |-
                {
                  "database_role_name": "${snowflake_database_role.db_role.fully_qualified_name}",
                  "on_schema_object": [
                    {
                      "all": [
                        {
                          "in_schema": "${snowflake_schema.my_schema.fully_qualified_name}",
                          "object_type_plural": "TABLES"
                        }
                      ]
                    }
                  ],
                  "privileges": [
                    "SELECT",
                    "INSERT"
                  ]
                }
              references:
                database_role_name: snowflake_database_role.db_role.fully_qualified_name
                on_schema_object.all.in_schema: snowflake_schema.my_schema.fully_qualified_name
              dependencies:
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_database_role.db_role: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "db_role_name"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
            - name: example
              manifest: |-
                {
                  "database_role_name": "${snowflake_database_role.db_role.fully_qualified_name}",
                  "on_schema_object": [
                    {
                      "future": [
                        {
                          "in_database": "${snowflake_database_role.db_role.database}",
                          "object_type_plural": "TABLES"
                        }
                      ]
                    }
                  ],
                  "privileges": [
                    "SELECT",
                    "INSERT"
                  ]
                }
              references:
                database_role_name: snowflake_database_role.db_role.fully_qualified_name
                on_schema_object.future.in_database: snowflake_database_role.db_role.database
              dependencies:
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_database_role.db_role: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "db_role_name"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
            - name: example
              manifest: |-
                {
                  "database_role_name": "${snowflake_database_role.db_role.fully_qualified_name}",
                  "on_schema_object": [
                    {
                      "future": [
                        {
                          "in_schema": "${snowflake_schema.my_schema.fully_qualified_name}",
                          "object_type_plural": "TABLES"
                        }
                      ]
                    }
                  ],
                  "privileges": [
                    "SELECT",
                    "INSERT"
                  ]
                }
              references:
                database_role_name: snowflake_database_role.db_role.fully_qualified_name
                on_schema_object.future.in_schema: snowflake_schema.my_schema.fully_qualified_name
              dependencies:
                snowflake_database.db: |-
                    {
                      "name": "database"
                    }
                snowflake_database_role.db_role: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "db_role_name"
                    }
                snowflake_schema.my_schema: |-
                    {
                      "database": "${snowflake_database.db.name}",
                      "name": "my_schema"
                    }
        argumentDocs:
            all: '(Block List, Max: 1) Configures the privilege to be granted on all objects in either a database or schema. (see below for nested schema)'
            all_privileges: '(Boolean) (Default: false) Grant all privileges on the database role.'
            all_schemas_in_database: (String) The fully qualified name of the database.
            always_apply: '(Boolean) (Default: false) If true, the resource will always produce a “plan” and on “apply” it will re-grant defined privileges. It is supposed to be used only in “grant privileges on all X’s in database / schema Y” or “grant all privileges to X” scenarios to make sure that every new object in a given database / schema is granted by the account role and every new privilege is granted to the database role. Important note: this flag is not compliant with the Terraform assumptions of the config being eventually convergent (producing an empty plan).'
            always_apply_trigger: '(String) (Default: ``) This is a helper field and should not be set. Its main purpose is to help to achieve the functionality described by the always_apply field.'
            create: (String)
            database_role_name: (String) The fully qualified name of the database role to which privileges will be granted. For more information about this resource, see docs.
            delete: (String)
            future: '(Block List, Max: 1) Configures the privilege to be granted on future objects in either a database or schema. (see below for nested schema)'
            future_schemas_in_database: (String) The fully qualified name of the database.
            id: (String) The ID of this resource.
            in_database: (String) The fully qualified name of the database.
            in_schema: (String) The fully qualified name of the schema.
            object_name: (String) The fully qualified name of the object on which privileges will be granted.
            object_type: '(String) The object type of the schema object on which privileges will be granted. Valid values are: AGGREGATION POLICY | ALERT | AUTHENTICATION POLICY | CORTEX SEARCH SERVICE | DATA METRIC FUNCTION | DYNAMIC TABLE | EVENT TABLE | EXTERNAL TABLE | FILE FORMAT | FUNCTION | GIT REPOSITORY | HYBRID TABLE | IMAGE REPOSITORY | ICEBERG TABLE | MASKING POLICY | MATERIALIZED VIEW | MODEL | NETWORK RULE | NOTEBOOK | PACKAGES POLICY | PASSWORD POLICY | PIPE | PROCEDURE | PROJECTION POLICY | ROW ACCESS POLICY | SECRET | SERVICE | SESSION POLICY | SEQUENCE | SNAPSHOT | STAGE | STREAM | TABLE | TAG | TASK | VIEW | STREAMLIT | DATASET'
            object_type_plural: '(String) The plural object type of the schema object on which privileges will be granted. Valid values are: AGGREGATION POLICIES | ALERTS | AUTHENTICATION POLICIES | CORTEX SEARCH SERVICES | DATA METRIC FUNCTIONS | DYNAMIC TABLES | EVENT TABLES | EXTERNAL TABLES | FILE FORMATS | FUNCTIONS | GIT REPOSITORIES | HYBRID TABLES | IMAGE REPOSITORIES | ICEBERG TABLES | MASKING POLICIES | MATERIALIZED VIEWS | MODELS | NETWORK RULES | NOTEBOOKS | PACKAGES POLICIES | PASSWORD POLICIES | PIPES | PROCEDURES | PROJECTION POLICIES | ROW ACCESS POLICIES | SECRETS | SERVICES | SESSION POLICIES | SEQUENCES | SNAPSHOTS | STAGES | STREAMS | TABLES | TAGS | TASKS | VIEWS | STREAMLITS | DATASETS.'
            on_database: (String) The fully qualified name of the database on which privileges will be granted. For more information about this resource, see docs.
            on_schema: '(Block List, Max: 1) Specifies the schema on which privileges will be granted. (see below for nested schema)'
            on_schema_object: '(Block List, Max: 1) Specifies the schema object on which privileges will be granted. (see below for nested schema)'
            privileges: (Set of String) The privileges to grant on the database role.
            read: (String)
            schema_name: (String) The fully qualified name of the schema.
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
            with_grant_option: '(Boolean) (Default: false) If specified, allows the recipient role to grant the privileges to other roles.'
        importStatements: []
    snowflake_grant_privileges_to_share:
        subCategory: Stable
        name: snowflake_grant_privileges_to_share
        title: snowflake_grant_privileges_to_share Resource - terraform-provider-snowflake
        examples:
            - name: example
              manifest: |-
                {
                  "on_database": "${snowflake_database.example.name}",
                  "privileges": [
                    "USAGE"
                  ],
                  "to_share": "${snowflake_share.example.name}"
                }
              references:
                on_database: snowflake_database.example.name
                to_share: snowflake_share.example.name
              dependencies:
                snowflake_database.example: |-
                    {
                      "depends_on": [
                        "${snowflake_share.example}"
                      ],
                      "name": "test"
                    }
                snowflake_share.example: |-
                    {
                      "name": "test"
                    }
            - name: example
              manifest: |-
                {
                  "on_schema": "${snowflake_database.example.name}.${snowflake_schema.example.name}",
                  "privileges": [
                    "USAGE"
                  ],
                  "to_share": "${snowflake_share.example.name}"
                }
              references:
                to_share: snowflake_share.example.name
              dependencies:
                snowflake_database.example: |-
                    {
                      "depends_on": [
                        "${snowflake_share.example}"
                      ],
                      "name": "test"
                    }
                snowflake_share.example: |-
                    {
                      "name": "test"
                    }
            - name: example
              manifest: |-
                {
                  "on_table": "${snowflake_database.example.name}.${snowflake_schema.example.name}.${snowflake_table.example.name}",
                  "privileges": [
                    "SELECT"
                  ],
                  "to_share": "${snowflake_share.example.name}"
                }
              references:
                to_share: snowflake_share.example.name
              dependencies:
                snowflake_database.example: |-
                    {
                      "depends_on": [
                        "${snowflake_share.example}"
                      ],
                      "name": "test"
                    }
                snowflake_share.example: |-
                    {
                      "name": "test"
                    }
            - name: example
              manifest: |-
                {
                  "on_all_tables_in_schema": "${snowflake_database.example.name}.${snowflake_schema.example.name}",
                  "privileges": [
                    "SELECT"
                  ],
                  "to_share": "${snowflake_share.example.name}"
                }
              references:
                to_share: snowflake_share.example.name
              dependencies:
                snowflake_database.example: |-
                    {
                      "depends_on": [
                        "${snowflake_share.example}"
                      ],
                      "name": "test"
                    }
                snowflake_share.example: |-
                    {
                      "name": "test"
                    }
            - name: example
              manifest: |-
                {
                  "on_tag": "${snowflake_database.example.name}.${snowflake_schema.example.name}.${snowflake_tag.example.name}",
                  "privileges": [
                    "READ"
                  ],
                  "to_share": "${snowflake_share.example.name}"
                }
              references:
                to_share: snowflake_share.example.name
              dependencies:
                snowflake_database.example: |-
                    {
                      "depends_on": [
                        "${snowflake_share.example}"
                      ],
                      "name": "test"
                    }
                snowflake_share.example: |-
                    {
                      "name": "test"
                    }
            - name: example
              manifest: |-
                {
                  "on_view": "${snowflake_database.example.name}.${snowflake_schema.example.name}.${snowflake_view.example.name}",
                  "privileges": [
                    "SELECT"
                  ],
                  "to_share": "${snowflake_share.example.name}"
                }
              references:
                to_share: snowflake_share.example.name
              dependencies:
                snowflake_database.example: |-
                    {
                      "depends_on": [
                        "${snowflake_share.example}"
                      ],
                      "name": "test"
                    }
                snowflake_share.example: |-
                    {
                      "name": "test"
                    }
        argumentDocs:
            CREATE SNOWFLAKE.ML.ANOMALY_DETECTION: or CREATE SNOWFLAKE.ML.FORECAST privileges on schema results in a permadiff because of the probably incorrect Snowflake's behavior of SHOW GRANTS ON <object_type> <object_name>. More in the comment.
            create: (String)
            delete: (String)
            id: (String) The ID of this resource.
            on_all_tables_in_schema: (String) The fully qualified identifier for the schema for which the specified privilege will be granted for all tables.
            on_database: (String) The fully qualified name of the database on which privileges will be granted. For more information about this resource, see docs.
            on_function: (String) The fully qualified name of the function on which privileges will be granted.
            on_schema: (String) The fully qualified name of the schema on which privileges will be granted. For more information about this resource, see docs.
            on_table: (String) The fully qualified name of the table on which privileges will be granted. For more information about this resource, see docs.
            on_tag: (String) The fully qualified name of the tag on which privileges will be granted. For more information about this resource, see docs.
            on_view: (String) The fully qualified name of the view on which privileges will be granted. For more information about this resource, see docs.
            privileges: '(Set of String) The privileges to grant on the share. See available list of privileges: https://docs.snowflake.com/en/sql-reference/sql/grant-privilege-share#syntax'
            read: (String)
            timeouts: (Block, Optional) (see below for nested schema)
            to_share: (String) The fully qualified name of the share on which privileges will be granted. For more information about this resource, see docs.
            update: (String)
        importStatements: []
    snowflake_image_repository:
        subCategory: Preview
        description: Resource used to manage image repositories. For more information, check image repositories documentation https://docs.snowflake.com/en/sql-reference/sql/create-image-repository. Snowpark Container Services provides an OCIv2-compliant image registry service and a storage unit call repository to store images. See Working with an image registry and repository https://docs.snowflake.com/en/developer-guide/snowpark-container-services/working-with-registry-repository developer guide for more details.
        name: snowflake_image_repository
        title: snowflake_image_repository Resource - terraform-provider-snowflake
        examples:
            - name: basic
              manifest: |-
                {
                  "database": "DATABASE",
                  "name": "BASIC",
                  "schema": "SCHEMA"
                }
            - name: complete
              manifest: |-
                {
                  "comment": "An example image repository",
                  "database": "DATABASE",
                  "name": "BASIC",
                  "schema": "SCHEMA"
                }
        argumentDocs:
            comment: (String) Specifies a comment for the object.
            create: (String)
            created_on: (String)
            database: '(String) The database in which to create the image repository. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            database_name: (String)
            delete: (String)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            name: '(String) Specifies the identifier for the image repository; must be unique for the schema in which the image repository is created. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            owner: (String)
            owner_role_type: (String)
            privatelink_repository_url: (String)
            read: (String)
            repository_url: (String)
            schema: '(String) The schema in which to create the image repository. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            show_output: (List of Object) Outputs the result of SHOW IMAGE REPOSITORIES for the given image repository. (see below for nested schema)
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
        importStatements:
            - terraform import snowflake_image_repository.example '"<database_name>"."<schema_name>"."<image_repository_name>"'
    snowflake_job_service:
        subCategory: Preview
        description: Resource used to manage job services. For more information, check services documentation https://docs.snowflake.com/en/sql-reference/sql/execute-job-service. Executes a Snowpark Container Services service as a job. A service, created using CREATE SERVICE, is long-running and you must explicitly stop it when it is no longer needed. On the other hand, a job, created using EXECUTE JOB SERVICE (with ASYNC=TRUE in this resource), returns immediately while the job is running. See Working with services https://docs.snowflake.com/en/developer-guide/snowpark-container-services/working-with-services developer guide for more details.
        name: snowflake_job_service
        title: snowflake_job_service Resource - terraform-provider-snowflake
        examples:
            - name: basic
              manifest: |-
                {
                  "database": "${snowflake_database.test.name}",
                  "from_specification": [
                    {
                      "file": "spec.yaml",
                      "stage": "${snowflake_stage.test.fully_qualified_name}"
                    }
                  ],
                  "in_compute_pool": "${snowflake_compute_pool.test.name}",
                  "name": "SERVICE",
                  "schema": "${snowflake_schema.test.name}"
                }
              references:
                database: snowflake_database.test.name
                from_specification.stage: snowflake_stage.test.fully_qualified_name
                in_compute_pool: snowflake_compute_pool.test.name
                schema: snowflake_schema.test.name
            - name: basic
              manifest: |-
                {
                  "database": "${snowflake_database.test.name}",
                  "from_specification": [
                    {
                      "text": "spec:\n  containers:\n  - name: example-container\n    image: /database/schema/image_repository/exampleimage:latest\n"
                    }
                  ],
                  "in_compute_pool": "${snowflake_compute_pool.test.name}",
                  "name": "SERVICE",
                  "schema": "${snowflake_schema.test.name}"
                }
              references:
                database: snowflake_database.test.name
                in_compute_pool: snowflake_compute_pool.test.name
                schema: snowflake_schema.test.name
            - name: basic
              manifest: |-
                {
                  "database": "${snowflake_database.test.name}",
                  "from_specification_template": [
                    {
                      "file": "spec.yaml",
                      "path": "path/to/spec",
                      "stage": "${snowflake_stage.test.fully_qualified_name}",
                      "using": [
                        {
                          "key": "tag",
                          "value": "latest"
                        }
                      ]
                    }
                  ],
                  "in_compute_pool": "${snowflake_compute_pool.test.name}",
                  "name": "SERVICE",
                  "schema": "${snowflake_schema.test.name}"
                }
              references:
                database: snowflake_database.test.name
                from_specification_template.stage: snowflake_stage.test.fully_qualified_name
                in_compute_pool: snowflake_compute_pool.test.name
                schema: snowflake_schema.test.name
            - name: basic
              manifest: |-
                {
                  "database": "${snowflake_database.test.name}",
                  "from_specification": [
                    {
                      "text": "spec:\n containers:\n - name: {{ tag }}\n   image: /database/schema/image_repository/exampleimage:latest\n",
                      "using": [
                        {
                          "key": "tag",
                          "value": "latest"
                        }
                      ]
                    }
                  ],
                  "in_compute_pool": "${snowflake_compute_pool.test.name}",
                  "name": "SERVICE",
                  "schema": "${snowflake_schema.test.name}"
                }
              references:
                database: snowflake_database.test.name
                in_compute_pool: snowflake_compute_pool.test.name
                schema: snowflake_schema.test.name
            - name: complete
              manifest: |-
                {
                  "async": true,
                  "comment": "A service.",
                  "database": "${snowflake_database.test.name}",
                  "external_access_integrations": [
                    "INTEGRATION"
                  ],
                  "from_specification": [
                    {
                      "file": "spec.yaml",
                      "path": "path/to/spec",
                      "stage": "${snowflake_stage.test.fully_qualified_name}"
                    }
                  ],
                  "in_compute_pool": "${snowflake_compute_pool.test.name}",
                  "name": "SERVICE",
                  "query_warehouse": "${snowflake_warehouse.test.name}",
                  "schema": "${snowflake_schema.test.name}"
                }
              references:
                database: snowflake_database.test.name
                from_specification.stage: snowflake_stage.test.fully_qualified_name
                in_compute_pool: snowflake_compute_pool.test.name
                query_warehouse: snowflake_warehouse.test.name
                schema: snowflake_schema.test.name
        argumentDocs:
            auto_resume: (Boolean)
            auto_suspend_secs: (Number)
            comment: (String) Specifies a comment for the service.
            compute_pool: '(String) Specifies the name of the compute pool in your account on which to run the service. Identifiers with special or lower-case characters are not supported. This limitation in the provider follows the limitation in Snowflake (see docs). Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            create: (String)
            created_on: (String)
            current_instances: (Number)
            database: '(String) The database in which to create the service. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            database_name: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE SERVICE for the given service. (see below for nested schema)
            dns_name: (String)
            external_access_integrations: (Set of String) Specifies the names of the external access integrations that allow your service to access external sites.
            file: '(String) The file name of the service specification. Example: spec.yaml.'
            from_specification: '(Block List, Max: 1) Specifies the service specification to use for the service. Note that external changes on this field and nested fields are not detected. Use correctly formatted YAML files. Watch out for the space/tabs indentation. See service specification for more information. (see below for nested schema)'
            from_specification_template: '(Block List, Max: 1) Specifies the service specification template to use for the service. Note that external changes on this field and nested fields are not detected. Use correctly formatted YAML files. Watch out for the space/tabs indentation. See service specification for more information. (see below for nested schema)'
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            is_async_job: (Boolean)
            is_job: (Boolean)
            is_upgrading: (Boolean)
            key: (String) The name of the template variable. The provider wraps it in double quotes by default, so be aware of that while referencing the argument in the spec definition.
            managing_object_domain: (String)
            managing_object_name: (String)
            max_instances: (Number)
            min_instances: (Number)
            min_ready_instances: (Number)
            name: '(String) Specifies the identifier for the service; must be unique for the schema in which the service is created. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            owner: (String)
            owner_role_type: (String)
            path: '(String) The path to the service specification file on the given stage. When the path is specified, the / character is automatically added as a path prefix. Example: path/to/spec.'
            query_warehouse: '(String) Warehouse to use if a service container connects to Snowflake to execute a query but does not explicitly specify a warehouse to use. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            read: (String)
            resumed_on: (String)
            schema: '(String) The schema in which to create the service. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            service_type: (String) Specifies a type for the service. This field is used for checking external changes and recreating the resources if needed.
            show_output: (List of Object) Outputs the result of SHOW SERVICES for the given service. (see below for nested schema)
            spec: (String)
            spec_digest: (String)
            stage: '(String) The fully qualified name of the stage containing the service specification file. At symbol (@) is added automatically. Example: "\"<db_name>\".\"<schema_name>\".\"<stage_name>\"". For more information about this resource, see docs.'
            status: (String)
            suspended_on: (String)
            target_instances: (Number)
            text: (String) The embedded text of the service specification.
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
            updated_on: (String)
            using: '(Block List, Min: 1) List of the specified template variables and the values of those variables. (see below for nested schema)'
            value: (String) The value to assign to the variable in the template. The provider wraps it in $$ by default, so be aware of that while referencing the argument in the spec definition. The value must either be alphanumeric or valid JSON.
        importStatements:
            - terraform import snowflake_job_service.example '"<database_name>"."<schema_name>"."<job_service_name>"'
    snowflake_legacy_service_user:
        subCategory: Stable
        description: Resource used to manage legacy service user objects. For more information, check user documentation https://docs.snowflake.com/en/sql-reference/commands-user-role#user-management.
        name: snowflake_legacy_service_user
        title: snowflake_legacy_service_user Resource - terraform-provider-snowflake
        examples:
            - name: minimal
              manifest: |-
                {
                  "name": "Snowflake Legacy Service User - minimal"
                }
            - name: user
              manifest: |-
                {
                  "comment": "A legacy service user of snowflake.",
                  "days_to_expiry": 8,
                  "default_namespace": "some.namespace",
                  "default_role": "role1",
                  "default_secondary_roles_option": "ALL",
                  "default_warehouse": "warehouse",
                  "disabled": "false",
                  "display_name": "Snowflake Legacy Service User display name",
                  "email": "${var.email}",
                  "login_name": "${var.login_name}",
                  "mins_to_unlock": 9,
                  "must_change_password": "true",
                  "name": "Snowflake Legacy Service User",
                  "password": "${var.password}",
                  "rsa_public_key": "...",
                  "rsa_public_key_2": "..."
                }
              references:
                email: var.email
                login_name: var.login_name
                password: var.password
            - name: u
              manifest: |-
                {
                  "abort_detached_query": true,
                  "autocommit": false,
                  "binary_input_format": "UTF8",
                  "binary_output_format": "BASE64",
                  "client_memory_limit": 1024,
                  "client_metadata_request_use_connection_ctx": true,
                  "client_prefetch_threads": 2,
                  "client_result_chunk_size": 48,
                  "client_result_column_case_insensitive": true,
                  "client_session_keep_alive": true,
                  "client_session_keep_alive_heartbeat_frequency": 2400,
                  "client_timestamp_type_mapping": "TIMESTAMP_NTZ",
                  "date_input_format": "YYYY-MM-DD",
                  "date_output_format": "YY-MM-DD",
                  "enable_unload_physical_type_optimization": false,
                  "enable_unredacted_query_syntax_error": true,
                  "error_on_nondeterministic_merge": false,
                  "error_on_nondeterministic_update": true,
                  "geography_output_format": "WKB",
                  "geometry_output_format": "WKB",
                  "jdbc_treat_decimal_as_int": false,
                  "jdbc_treat_timestamp_ntz_as_utc": true,
                  "jdbc_use_session_timezone": false,
                  "json_indent": 4,
                  "lock_timeout": 21222,
                  "log_level": "ERROR",
                  "multi_statement_count": 0,
                  "name": "Snowflake Legacy Service User with all parameters",
                  "network_policy": "BVYDGRAT_0D5E3DD1_F644_03DE_318A_1179886518A7",
                  "noorder_sequence_as_default": false,
                  "odbc_treat_decimal_as_int": true,
                  "prevent_unload_to_internal_stages": true,
                  "query_tag": "some_tag",
                  "quoted_identifiers_ignore_case": true,
                  "rows_per_resultset": 2,
                  "s3_stage_vpce_dns_name": "vpce-id.s3.region.vpce.amazonaws.com",
                  "search_path": "$public, $current",
                  "simulated_data_sharing_consumer": "some_consumer",
                  "statement_queued_timeout_in_seconds": 10,
                  "statement_timeout_in_seconds": 10,
                  "strict_json_output": true,
                  "time_input_format": "HH24:MI",
                  "time_output_format": "HH24:MI",
                  "timestamp_day_is_always_24h": true,
                  "timestamp_input_format": "YYYY-MM-DD",
                  "timestamp_ltz_output_format": "YYYY-MM-DD HH24:MI:SS",
                  "timestamp_ntz_output_format": "YYYY-MM-DD HH24:MI:SS",
                  "timestamp_output_format": "YYYY-MM-DD HH24:MI:SS",
                  "timestamp_type_mapping": "TIMESTAMP_LTZ",
                  "timestamp_tz_output_format": "YYYY-MM-DD HH24:MI:SS",
                  "timezone": "Europe/Warsaw",
                  "trace_level": "PROPAGATE",
                  "transaction_abort_on_error": true,
                  "transaction_default_isolation_level": "READ COMMITTED",
                  "two_digit_century_start": 1980,
                  "unsupported_ddl_action": "FAIL",
                  "use_cached_result": false,
                  "week_of_year_policy": 1,
                  "week_start": 1
                }
        argumentDocs:
            abort_detached_query: (Boolean) Specifies the action that Snowflake performs for in-progress queries if connectivity is lost due to abrupt termination of a session (e.g. network outage, browser termination, service interruption). For more information, check ABORT_DETACHED_QUERY docs.
            autocommit: (Boolean) Specifies whether autocommit is enabled for the session. Autocommit determines whether a DML statement, when executed without an active transaction, is automatically committed after the statement successfully completes. For more information, see Transactions. For more information, check AUTOCOMMIT docs.
            binary_input_format: (String) The format of VARCHAR values passed as input to VARCHAR-to-BINARY conversion functions. For more information, see Binary input and output. For more information, check BINARY_INPUT_FORMAT docs.
            binary_output_format: (String) The format for VARCHAR values returned as output by BINARY-to-VARCHAR conversion functions. For more information, see Binary input and output. For more information, check BINARY_OUTPUT_FORMAT docs.
            client_memory_limit: (Number) Parameter that specifies the maximum amount of memory the JDBC driver or ODBC driver should use for the result set from queries (in MB). For more information, check CLIENT_MEMORY_LIMIT docs.
            client_metadata_request_use_connection_ctx: (Boolean) For specific ODBC functions and JDBC methods, this parameter can change the default search scope from all databases/schemas to the current database/schema. The narrower search typically returns fewer rows and executes more quickly. For more information, check CLIENT_METADATA_REQUEST_USE_CONNECTION_CTX docs.
            client_prefetch_threads: (Number) Parameter that specifies the number of threads used by the client to pre-fetch large result sets. The driver will attempt to honor the parameter value, but defines the minimum and maximum values (depending on your system’s resources) to improve performance. For more information, check CLIENT_PREFETCH_THREADS docs.
            client_result_chunk_size: (Number) Parameter that specifies the maximum size of each set (or chunk) of query results to download (in MB). The JDBC driver downloads query results in chunks. For more information, check CLIENT_RESULT_CHUNK_SIZE docs.
            client_result_column_case_insensitive: (Boolean) Parameter that indicates whether to match column name case-insensitively in ResultSet.get* methods in JDBC. For more information, check CLIENT_RESULT_COLUMN_CASE_INSENSITIVE docs.
            client_session_keep_alive: (Boolean) Parameter that indicates whether to force a user to log in again after a period of inactivity in the session. For more information, check CLIENT_SESSION_KEEP_ALIVE docs.
            client_session_keep_alive_heartbeat_frequency: (Number) Number of seconds in-between client attempts to update the token for the session. For more information, check CLIENT_SESSION_KEEP_ALIVE_HEARTBEAT_FREQUENCY docs.
            client_timestamp_type_mapping: (String) Specifies the TIMESTAMP_* variation to use when binding timestamp variables for JDBC or ODBC applications that use the bind API to load data. For more information, check CLIENT_TIMESTAMP_TYPE_MAPPING docs.
            comment: (String) Specifies a comment for the user.
            created_on: (String)
            date_input_format: (String) Specifies the input format for the DATE data type. For more information, see Date and time input and output formats. For more information, check DATE_INPUT_FORMAT docs.
            date_output_format: (String) Specifies the display format for the DATE data type. For more information, see Date and time input and output formats. For more information, check DATE_OUTPUT_FORMAT docs.
            days_to_expiry: (Number) Specifies the number of days after which the user status is set to Expired and the user is no longer allowed to log in. This is useful for defining temporary users (i.e. users who should only have access to Snowflake for a limited time period). In general, you should not set this property for account administrators (i.e. users with the ACCOUNTADMIN role) because Snowflake locks them out when they become Expired. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            default: (String)
            default_namespace: (String) Specifies the namespace (database only or database and schema) that is active by default for the user’s session upon login. Note that the CREATE USER operation does not verify that the namespace exists.
            default_role: (String) Specifies the role that is active by default for the user’s session upon login. Note that specifying a default role for a user does not grant the role to the user. The role must be granted explicitly to the user using the GRANT ROLE command. In addition, the CREATE USER operation does not verify that the role exists. For more information about this resource, see docs.
            default_secondary_roles: (String)
            default_secondary_roles_option: '(String) (Default: DEFAULT) Specifies the secondary roles that are active for the user’s session upon login. Valid values are (case-insensitive): DEFAULT | NONE | ALL. More information can be found in doc.'
            default_warehouse: (String) Specifies the virtual warehouse that is active by default for the user’s session upon login. Note that the CREATE USER operation does not verify that the warehouse exists. For more information about this resource, see docs.
            description: (String)
            disabled: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies whether the user is disabled, which prevents logging in and aborts all the currently-running queries for the user. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            display_name: (String) Name displayed for the user in the Snowflake web interface.
            email: (String, Sensitive) Email address for the user.
            enable_unload_physical_type_optimization: (Boolean) Specifies whether to set the schema for unloaded Parquet files based on the logical column data types (i.e. the types in the unload SQL query or source table) or on the unloaded column values (i.e. the smallest data types and precision that support the values in the output columns of the unload SQL statement or source table). For more information, check ENABLE_UNLOAD_PHYSICAL_TYPE_OPTIMIZATION docs.
            enable_unredacted_query_syntax_error: (Boolean) Controls whether query text is redacted if a SQL query fails due to a syntax or parsing error. If FALSE, the content of a failed query is redacted in the views, pages, and functions that provide a query history. Only users with a role that is granted or inherits the AUDIT privilege can set the ENABLE_UNREDACTED_QUERY_SYNTAX_ERROR parameter. When using the ALTER USER command to set the parameter to TRUE for a particular user, modify the user that you want to see the query text, not the user who executed the query (if those are different users). For more information, check ENABLE_UNREDACTED_QUERY_SYNTAX_ERROR docs.
            error_on_nondeterministic_merge: (Boolean) Specifies whether to return an error when the MERGE command is used to update or delete a target row that joins multiple source rows and the system cannot determine the action to perform on the target row. For more information, check ERROR_ON_NONDETERMINISTIC_MERGE docs.
            error_on_nondeterministic_update: (Boolean) Specifies whether to return an error when the UPDATE command is used to update a target row that joins multiple source rows and the system cannot determine the action to perform on the target row. For more information, check ERROR_ON_NONDETERMINISTIC_UPDATE docs.
            expires_at_time: (String)
            ext_authn_duo: (Boolean)
            ext_authn_uid: (String)
            first_name: (String)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            geography_output_format: (String) Display format for GEOGRAPHY values. For more information, check GEOGRAPHY_OUTPUT_FORMAT docs.
            geometry_output_format: (String) Display format for GEOMETRY values. For more information, check GEOMETRY_OUTPUT_FORMAT docs.
            has_mfa: (Boolean)
            has_password: (Boolean)
            has_rsa_public_key: (Boolean)
            id: (String) The ID of this resource.
            jdbc_treat_decimal_as_int: (Boolean) Specifies how JDBC processes columns that have a scale of zero (0). For more information, check JDBC_TREAT_DECIMAL_AS_INT docs.
            jdbc_treat_timestamp_ntz_as_utc: (Boolean) Specifies how JDBC processes TIMESTAMP_NTZ values. For more information, check JDBC_TREAT_TIMESTAMP_NTZ_AS_UTC docs.
            jdbc_use_session_timezone: (Boolean) Specifies whether the JDBC Driver uses the time zone of the JVM or the time zone of the session (specified by the TIMEZONE parameter) for the getDate(), getTime(), and getTimestamp() methods of the ResultSet class. For more information, check JDBC_USE_SESSION_TIMEZONE docs.
            json_indent: (Number) Specifies the number of blank spaces to indent each new element in JSON output in the session. Also specifies whether to insert newline characters after each element. For more information, check JSON_INDENT docs.
            key: (String)
            last_name: (String)
            last_success_login: (String)
            level: (String)
            lock_timeout: (Number) Number of seconds to wait while trying to lock a resource, before timing out and aborting the statement. For more information, check LOCK_TIMEOUT docs.
            locked_until_time: (String)
            log_level: (String) Specifies the severity level of messages that should be ingested and made available in the active event table. Messages at the specified level (and at more severe levels) are ingested. For more information about log levels, see Setting log level. For more information, check LOG_LEVEL docs.
            login_name: (String, Sensitive) The name users use to log in. If not supplied, snowflake will use name instead. Login names are always case-insensitive.
            mins_to_bypass_mfa: (String)
            mins_to_unlock: '(Number) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (-1)) Specifies the number of minutes until the temporary lock on the user login is cleared. To protect against unauthorized user login, Snowflake places a temporary lock on a user after five consecutive unsuccessful login attempts. When creating a user, this property can be set to prevent them from logging in until the specified amount of time passes. To remove a lock immediately for a user, specify a value of 0 for this parameter. Note because this value changes continuously after setting it, the provider is currently NOT handling the external changes to it. External changes for this field won''t be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".'
            multi_statement_count: (Number) Number of statements to execute when using the multi-statement capability. For more information, check MULTI_STATEMENT_COUNT docs.
            must_change_password: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies whether the user is forced to change their password on next login (including their first/initial login) into the system. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            name: '(String) Name of the user. Note that if you do not supply login_name this will be used as login_name. Check the docs. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            network_policy: (String) Specifies the network policy to enforce for your account. Network policies enable restricting access to your account based on users’ IP address. For more details, see Controlling network traffic with network policies. Any existing network policy (created using CREATE NETWORK POLICY). For more information, check NETWORK_POLICY docs.
            noorder_sequence_as_default: (Boolean) Specifies whether the ORDER or NOORDER property is set by default when you create a new sequence or add a new table column. The ORDER and NOORDER properties determine whether or not the values are generated for the sequence or auto-incremented column in increasing or decreasing order. For more information, check NOORDER_SEQUENCE_AS_DEFAULT docs.
            odbc_treat_decimal_as_int: (Boolean) Specifies how ODBC processes columns that have a scale of zero (0). For more information, check ODBC_TREAT_DECIMAL_AS_INT docs.
            owner: (String)
            parameters: (List of Object) Outputs the result of SHOW PARAMETERS IN USER for the given user. (see below for nested schema)
            password: '(String, Sensitive) Password for the user. WARNING: this will put the password in the terraform state file. Use carefully. External changes for this field won''t be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".'
            prevent_unload_to_internal_stages: (Boolean) Specifies whether to prevent data unload operations to internal (Snowflake) stages using COPY INTO  statements. For more information, check PREVENT_UNLOAD_TO_INTERNAL_STAGES docs.
            query_tag: (String) Optional string that can be used to tag queries and other SQL statements executed within a session. The tags are displayed in the output of the QUERY_HISTORY, QUERY_HISTORY_BY_* functions. For more information, check QUERY_TAG docs.
            quoted_identifiers_ignore_case: (Boolean) Specifies whether letters in double-quoted object identifiers are stored and resolved as uppercase letters. By default, Snowflake preserves the case of alphabetic characters when storing and resolving double-quoted identifiers (see Identifier resolution). You can use this parameter in situations in which third-party applications always use double quotes around identifiers. For more information, check QUOTED_IDENTIFIERS_IGNORE_CASE docs.
            rows_per_resultset: (Number) Specifies the maximum number of rows returned in a result set. A value of 0 specifies no maximum. For more information, check ROWS_PER_RESULTSET docs.
            rsa_public_key: (String) Specifies the user’s RSA public key; used for key-pair authentication. Must be on 1 line without header and trailer.
            rsa_public_key_2: (String) Specifies the user’s second RSA public key; used to rotate the public and private keys for key-pair authentication based on an expiration schedule set by your organization. Must be on 1 line without header and trailer.
            s3_stage_vpce_dns_name: (String) Specifies the DNS name of an Amazon S3 interface endpoint. Requests sent to the internal stage of an account via AWS PrivateLink for Amazon S3 use this endpoint to connect. For more information, see Accessing Internal stages with dedicated interface endpoints. For more information, check S3_STAGE_VPCE_DNS_NAME docs.
            search_path: (String) Specifies the path to search to resolve unqualified object names in queries. For more information, see Name resolution in queries. Comma-separated list of identifiers. An identifier can be a fully or partially qualified schema name. For more information, check SEARCH_PATH docs.
            show_output: (List of Object) Outputs the result of SHOW USER for the given user. (see below for nested schema)
            simulated_data_sharing_consumer: (String) Specifies the name of a consumer account to simulate for testing/validating shared data, particularly shared secure views. When this parameter is set in a session, shared views return rows as if executed in the specified consumer account rather than the provider account. For more information, see Introduction to Secure Data Sharing and Working with shares. For more information, check SIMULATED_DATA_SHARING_CONSUMER docs.
            snowflake_lock: (Boolean)
            statement_queued_timeout_in_seconds: (Number) Amount of time, in seconds, a SQL statement (query, DDL, DML, etc.) remains queued for a warehouse before it is canceled by the system. This parameter can be used in conjunction with the MAX_CONCURRENCY_LEVEL parameter to ensure a warehouse is never backlogged. For more information, check STATEMENT_QUEUED_TIMEOUT_IN_SECONDS docs.
            statement_timeout_in_seconds: (Number) Amount of time, in seconds, after which a running SQL statement (query, DDL, DML, etc.) is canceled by the system. For more information, check STATEMENT_TIMEOUT_IN_SECONDS docs.
            strict_json_output: (Boolean) This parameter specifies whether JSON output in a session is compatible with the general standard (as described by http://json.org). By design, Snowflake allows JSON input that contains non-standard values; however, these non-standard values might result in Snowflake outputting JSON that is incompatible with other platforms and languages. This parameter, when enabled, ensures that Snowflake outputs valid/compatible JSON. For more information, check STRICT_JSON_OUTPUT docs.
            time_input_format: (String) Specifies the input format for the TIME data type. For more information, see Date and time input and output formats. Any valid, supported time format or AUTO (AUTO specifies that Snowflake attempts to automatically detect the format of times stored in the system during the session). For more information, check TIME_INPUT_FORMAT docs.
            time_output_format: (String) Specifies the display format for the TIME data type. For more information, see Date and time input and output formats. For more information, check TIME_OUTPUT_FORMAT docs.
            timestamp_day_is_always_24h: (Boolean) Specifies whether the DATEADD function (and its aliases) always consider a day to be exactly 24 hours for expressions that span multiple days. For more information, check TIMESTAMP_DAY_IS_ALWAYS_24H docs.
            timestamp_input_format: (String) Specifies the input format for the TIMESTAMP data type alias. For more information, see Date and time input and output formats. Any valid, supported timestamp format or AUTO (AUTO specifies that Snowflake attempts to automatically detect the format of timestamps stored in the system during the session). For more information, check TIMESTAMP_INPUT_FORMAT docs.
            timestamp_ltz_output_format: (String) Specifies the display format for the TIMESTAMP_LTZ data type. If no format is specified, defaults to TIMESTAMP_OUTPUT_FORMAT. For more information, see Date and time input and output formats. For more information, check TIMESTAMP_LTZ_OUTPUT_FORMAT docs.
            timestamp_ntz_output_format: (String) Specifies the display format for the TIMESTAMP_NTZ data type. For more information, check TIMESTAMP_NTZ_OUTPUT_FORMAT docs.
            timestamp_output_format: (String) Specifies the display format for the TIMESTAMP data type alias. For more information, see Date and time input and output formats. For more information, check TIMESTAMP_OUTPUT_FORMAT docs.
            timestamp_type_mapping: (String) Specifies the TIMESTAMP_* variation that the TIMESTAMP data type alias maps to. For more information, check TIMESTAMP_TYPE_MAPPING docs.
            timestamp_tz_output_format: (String) Specifies the display format for the TIMESTAMP_TZ data type. If no format is specified, defaults to TIMESTAMP_OUTPUT_FORMAT. For more information, see Date and time input and output formats. For more information, check TIMESTAMP_TZ_OUTPUT_FORMAT docs.
            timezone: (String) Specifies the time zone for the session. You can specify a time zone name or a link name from release 2021a of the IANA Time Zone Database (e.g. America/Los_Angeles, Europe/London, UTC, Etc/GMT, etc.). For more information, check TIMEZONE docs.
            trace_level: (String) Controls how trace events are ingested into the event table. For more information about trace levels, see Setting trace level. For more information, check TRACE_LEVEL docs.
            transaction_abort_on_error: (Boolean) Specifies the action to perform when a statement issued within a non-autocommit transaction returns with an error. For more information, check TRANSACTION_ABORT_ON_ERROR docs.
            transaction_default_isolation_level: (String) Specifies the isolation level for transactions in the user session. For more information, check TRANSACTION_DEFAULT_ISOLATION_LEVEL docs.
            two_digit_century_start: (Number) Specifies the “century start” year for 2-digit years (i.e. the earliest year such dates can represent). This parameter prevents ambiguous dates when importing or converting data with the YY date format component (i.e. years represented as 2 digits). For more information, check TWO_DIGIT_CENTURY_START docs.
            type: (String)
            unsupported_ddl_action: (String) Determines if an unsupported (i.e. non-default) value specified for a constraint property returns an error. For more information, check UNSUPPORTED_DDL_ACTION docs.
            use_cached_result: (Boolean) Specifies whether to reuse persisted query results, if available, when a matching query is submitted. For more information, check USE_CACHED_RESULT docs.
            user_type: (String) Specifies a type for the user.
            value: (String)
            week_of_year_policy: '(Number) Specifies how the weeks in a given year are computed. 0: The semantics used are equivalent to the ISO semantics, in which a week belongs to a given year if at least 4 days of that week are in that year. 1: January 1 is included in the first week of the year and December 31 is included in the last week of the year. For more information, check WEEK_OF_YEAR_POLICY docs.'
            week_start: '(Number) Specifies the first day of the week (used by week-related date functions). 0: Legacy Snowflake behavior is used (i.e. ISO-like semantics). 1 (Monday) to 7 (Sunday): All the week-related functions use weeks that start on the specified day of the week. For more information, check WEEK_START docs.'
        importStatements:
            - terraform import snowflake_legacy_service_user.example '"<user_name>"'
    snowflake_listing:
        subCategory: Preview
        description: Resource used to manage listing objects. For more information, check listing documentation https://other-docs.snowflake.com/en/collaboration/collaboration-listings-about.
        name: snowflake_listing
        title: snowflake_listing Resource - terraform-provider-snowflake
        examples:
            - name: basic_inlined
              manifest: |-
                {
                  "manifest": [
                    {
                      "from_string": "title: title\nsubtitle: subtitle\ndescription: description\nlisting_terms:\n  type: OFFLINE\n"
                    }
                  ],
                  "name": "LISTING"
                }
            - name: basic_staged
              manifest: |-
                {
                  "manifest": [
                    {
                      "from_stage": {
                        "stage": "${snowflake_stage.test_stage.fully_qualified_name}"
                      }
                    }
                  ],
                  "name": "LISTING"
                }
            - name: basic_inlined
              manifest: |-
                {
                  "application_package": "test_application_package",
                  "comment": "This is a comment for the listing",
                  "manifest": [
                    {
                      "from_string": "title: title\nsubtitle: subtitle\ndescription: description\nlisting_terms:\n  type: OFFLINE\n"
                    }
                  ],
                  "name": "LISTING",
                  "publish": true,
                  "share": "${snowflake_share.test_share.fully_qualified_name}"
                }
              references:
                share: snowflake_share.test_share.fully_qualified_name
            - name: basic_staged
              manifest: |-
                {
                  "application_package": "test_application_package",
                  "comment": "This is a comment for the listing",
                  "manifest": [
                    {
                      "from_stage": {
                        "location": "path/to/manifest",
                        "stage": "${snowflake_stage.test_stage.fully_qualified_name}",
                        "version_comment": "Initial version of the manifest",
                        "version_name": "v1.0.0"
                      }
                    }
                  ],
                  "name": "LISTING",
                  "publish": true,
                  "share": "${snowflake_share.test_share.fully_qualified_name}"
                }
              references:
                share: snowflake_share.test_share.fully_qualified_name
        argumentDocs:
            application_package: (String) Specifies the application package attached to the listing.
            comment: (String) Specifies a comment for the listing.
            create: (String)
            created_on: (String)
            delete: (String)
            detailed_target_accounts: (String)
            distribution: (String)
            from_stage: '(Block List, Max: 1) Manifest provided from a given stage. If the manifest file is in the root, only stage needs to be passed. For more information on manifest syntax, see Listing manifest reference. A proper YAML indentation (2 spaces) is required. (see below for nested schema)'
            from_string: (String) Manifest provided as a string. Wrapping $$ signs are added by the provider automatically; do not include them. For more information on manifest syntax, see Listing manifest reference. Also, the multiline string syntax is a must here. A proper YAML indentation (2 spaces) is required.
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            global_name: (String)
            id: (String) The ID of this resource.
            is_application: (Boolean)
            is_by_request: (Boolean)
            is_limited_trial: (Boolean)
            is_monetized: (Boolean)
            is_mountless_queryable: (Boolean)
            is_targeted: (Boolean)
            location: (String) Location of the manifest file in the stage. If not specified, the manifest file will be expected to be at the root of the stage.
            manifest: '(Block List, Min: 1, Max: 1) Specifies the way manifest is provided for the listing. For more information on manifest syntax, see Listing manifest reference. External changes for this field won''t be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint". (see below for nested schema)'
            name: (String) Specifies the listing identifier (name). It must be unique within the organization, regardless of which Snowflake region the account is located in. Must start with an alphabetic character and cannot contain spaces or special characters except for underscores.
            organization_profile_name: (String)
            owner: (String)
            owner_role_type: (String)
            profile: (String)
            publish: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Determines if the listing should be published.'
            published_on: (String)
            read: (String)
            regions: (String)
            rejected_on: (String)
            review_state: (String)
            share: (String) Specifies the identifier for the share to attach to the listing.
            show_output: (List of Object) Outputs the result of SHOW LISTINGS for the given listing. (see below for nested schema)
            stage: (String) Identifier of the stage where the manifest file is located.
            state: (String)
            subtitle: (String)
            target_accounts: (String)
            timeouts: (Block, Optional) (see below for nested schema)
            title: (String)
            uniform_listing_locator: (String)
            update: (String)
            updated_on: (String)
            version_comment: (String) Specifies a comment for the listing version. Whenever a new version is created, this comment will be associated with it. The comment on the version will be visible in the SHOW VERSIONS IN LISTING command output.
            version_name: (String) Represents manifest version name. It's case-sensitive and used in manifest versioning. Version name should be specified or changed whenever any changes in the manifest should be applied to the listing. Later on the versions of the listing can be analyzed by calling the SHOW VERSIONS IN LISTING command. The resource does not track the changes on the specified stage.
        importStatements:
            - terraform import snowflake_listing.example '"<listing_name>"'
    snowflake_managed_account:
        subCategory: Preview
        name: snowflake_managed_account
        title: snowflake_managed_account Resource - terraform-provider-snowflake
        examples:
            - name: account
              manifest: |-
                {
                  "admin_name": "admin",
                  "admin_password": "${var.admin_password}",
                  "cloud": "aws",
                  "comment": "A managed account.",
                  "locator": "managed-account",
                  "name": "managed account",
                  "region": "us-west-2",
                  "type": "READER"
                }
              references:
                admin_password: var.admin_password
        argumentDocs:
            admin_name: (String) Identifier, as well as login name, for the initial user in the managed account. This user serves as the account administrator for the account.
            admin_password: (String, Sensitive) Password for the initial user in the managed account. Check Snowflake-provided password policy.
            cloud: (String) Cloud in which the managed account is located.
            comment: (String) Specifies a comment for the managed account.
            create: (String)
            created_on: (String) Date and time when the managed account was created.
            delete: (String)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            locator: (String) Display name of the managed account.
            name: (String) Identifier for the managed account; must be unique for your account.
            read: (String)
            region: (String) Snowflake Region in which the managed account is located.
            timeouts: (Block, Optional) (see below for nested schema)
            type: '(String) (Default: READER) Specifies the type of managed account.'
            update: (String)
            url: (String) URL for accessing the managed account, particularly through the web interface.
        importStatements:
            - terraform import snowflake_managed_account.example name
    snowflake_masking_policy:
        subCategory: Stable
        description: Resource used to manage masking policies. For more information, check masking policies documentation https://docs.snowflake.com/en/sql-reference/sql/create-masking-policy.
        name: snowflake_masking_policy
        title: snowflake_masking_policy Resource - terraform-provider-snowflake
        examples:
            - name: test
              manifest: |-
                {
                  "argument": [
                    {
                      "name": "ARG1",
                      "type": "VARCHAR"
                    },
                    {
                      "name": "ARG2",
                      "type": "NUMBER"
                    },
                    {
                      "name": "ARG3",
                      "type": "TIMESTAMP_NTZ"
                    }
                  ],
                  "body": "case\n  when current_role() in ('ROLE_A') then\n    ARG1\n  when is_role_in_session( 'ROLE_B' ) then\n    'ABC123'\n  else\n    '******'\nend\n",
                  "database": "EXAMPLE_DB",
                  "name": "EXAMPLE_MASKING_POLICY",
                  "return_data_type": "VARCHAR",
                  "schema": "EXAMPLE_SCHEMA"
                }
            - name: test
              manifest: |-
                {
                  "argument": [
                    {
                      "name": "ARG1",
                      "type": "VARCHAR"
                    },
                    {
                      "name": "ARG2",
                      "type": "NUMBER"
                    },
                    {
                      "name": "ARG3",
                      "type": "TIMESTAMP_NTZ"
                    }
                  ],
                  "body": "case\n  when current_role() in ('ROLE_A') then\n    ARG1\n  when is_role_in_session( 'ROLE_B' ) then\n    'ABC123'\n  else\n    '******'\nend\n",
                  "comment": "example masking policy",
                  "database": "EXAMPLE_DB",
                  "exempt_other_policies": "true",
                  "name": "EXAMPLE_MASKING_POLICY",
                  "return_data_type": "VARCHAR",
                  "schema": "EXAMPLE_SCHEMA"
                }
        argumentDocs:
            argument: '(Block List, Min: 1) List of the arguments for the masking policy. The first column and its data type always indicate the column data type values to mask or tokenize in the subsequent policy conditions. Note that you can not specify a virtual column as the first column argument in a conditional masking policy. (see below for nested schema)'
            body: (String) Specifies the SQL expression that transforms the data. To mitigate permadiff on this field, the provider replaces blank characters with a space. This can lead to false positives in cases where a change in case or run of whitespace is semantically significant.
            comment: (String) Specifies a comment for the masking policy.
            create: (String)
            created_on: (String)
            database: '(String) The database in which to create the masking policy. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            database_name: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE MASKING POLICY for the given masking policy. (see below for nested schema)
            exempt_other_policies: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies whether the row access policy or conditional masking policy can reference a column that is already protected by a masking policy. Due to Snowflake limitations, when value is changed, the resource is recreated. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            kind: (String)
            name: '(String) Specifies the identifier for the masking policy; must be unique for the database and schema in which the masking policy is created. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            owner: (String)
            owner_role_type: (String)
            read: (String)
            return_data_type: (String) The return data type must match the input data type of the first column that is specified as an input column. For more information about data types, check Snowflake docs.
            return_type: (String)
            schema: '(String) The schema in which to create the masking policy. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            show_output: (List of Object) Outputs the result of SHOW MASKING POLICIES for the given masking policy. (see below for nested schema)
            signature: (List of Object) (see below for nested schema)
            timeouts: (Block, Optional) (see below for nested schema)
            type: (String) The argument type. VECTOR data types are not yet supported. For more information about data types, check Snowflake docs.
            update: (String)
        importStatements:
            - terraform import snowflake_row_access_policy.example '"<database_name>"."<schema_name>"."<masking_policy_name>"'
    snowflake_materialized_view:
        subCategory: Preview
        name: snowflake_materialized_view
        title: snowflake_materialized_view Resource - terraform-provider-snowflake
        examples:
            - name: view
              manifest: |-
                {
                  "comment": "comment",
                  "database": "db",
                  "is_secure": false,
                  "name": "view",
                  "or_replace": false,
                  "schema": "schema",
                  "statement": "select * from foo;\n",
                  "warehouse": "warehouse"
                }
        argumentDocs:
            comment: (String) Specifies a comment for the view.
            create: (String)
            database: (String) The database in which to create the view. Don't use the | character.
            delete: (String)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            is_secure: '(Boolean) (Default: false) Specifies that the view is secure.'
            name: (String) Specifies the identifier for the view; must be unique for the schema in which the view is created.
            or_replace: '(Boolean) (Default: false) Overwrites the View if it exists.'
            read: (String)
            schema: (String) The schema in which to create the view. Don't use the | character.
            statement: (String) Specifies the query used to create the view.
            tag: (Block List, Deprecated) Definitions of a tag to associate with the resource. (see below for nested schema)
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
            value: (String) Tag value, e.g. marketing_info.
            warehouse: (String) The warehouse name.
        importStatements:
            - |-
              # format is database name | schema name | view name
              terraform import snowflake_materialized_view.example 'dbName|schemaName|viewName'
    snowflake_network_policy:
        subCategory: Stable
        description: Resource used to control network traffic. For more information, check an official guide https://docs.snowflake.com/en/user-guide/network-policies on controlling network traffic with network policies.
        name: snowflake_network_policy
        title: snowflake_network_policy Resource - terraform-provider-snowflake
        examples:
            - name: basic
              manifest: |-
                {
                  "name": "network_policy_name"
                }
            - name: complete
              manifest: |-
                {
                  "allowed_ip_list": [
                    "192.168.1.0/24"
                  ],
                  "allowed_network_rule_list": [
                    "${snowflake_network_rule.one.fully_qualified_name}"
                  ],
                  "blocked_ip_list": [
                    "192.168.1.99"
                  ],
                  "blocked_network_rule_list": [
                    "${snowflake_network_rule.two.fully_qualified_name}"
                  ],
                  "comment": "my network policy",
                  "name": "network_policy_name"
                }
        argumentDocs:
            allowed_ip_list: (Set of String) Specifies one or more IPv4 addresses (CIDR notation) that are allowed access to your Snowflake account.
            allowed_network_rule_list: (Set of String) Specifies a list of fully qualified network rules that contain the network identifiers that are allowed access to Snowflake. For more information about this resource, see docs.
            blocked_ip_list: (Set of String) Specifies one or more IPv4 addresses (CIDR notation) that are denied access to your Snowflake account. Do not add 0.0.0.0/0 to blocked_ip_list, in order to block all IP addresses except a select list, you only need to add IP addresses to allowed_ip_list.
            blocked_network_rule_list: (Set of String) Specifies a list of fully qualified network rules that contain the network identifiers that are denied access to Snowflake. For more information about this resource, see docs.
            comment: (String) Specifies a comment for the network policy.
            create: (String)
            created_on: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE NETWORK POLICY for the given network policy. (see below for nested schema)
            entries_in_allowed_ip_list: (Number)
            entries_in_allowed_network_rules: (Number)
            entries_in_blocked_ip_list: (Number)
            entries_in_blocked_network_rules: (Number)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            name: '(String) Specifies the identifier for the network policy; must be unique for the account in which the network policy is created. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            read: (String)
            show_output: (List of Object) Outputs the result of SHOW NETWORK POLICIES for the given network policy. (see below for nested schema)
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
        importStatements:
            - terraform import snowflake_network_policy.example '"<network_policy_name>"'
    snowflake_network_policy_attachment:
        subCategory: Preview
        name: snowflake_network_policy_attachment
        title: snowflake_network_policy_attachment Resource - terraform-provider-snowflake
        examples:
            - name: attach
              manifest: |-
                {
                  "network_policy_name": "policy",
                  "set_for_account": false,
                  "users": [
                    "user1",
                    "user2"
                  ]
                }
        argumentDocs:
            create: (String)
            delete: (String)
            id: (String) The ID of this resource.
            network_policy_name: (String) Specifies the identifier for the network policy; must be unique for the account in which the network policy is created.
            read: (String)
            set_for_account: '(Boolean) (Default: false) Specifies whether the network policy should be applied globally to your Snowflake accountNote: The Snowflake user running terraform apply must be on an IP address allowed by the network policy to set that policy globally on the Snowflake account.Additionally, a Snowflake account can only have one network policy set globally at any given time. This resource does not enforce one-policy-per-account, it is the user''s responsibility to enforce this. If multiple network policy resources have set_for_account: true, the final policy set on the account will be non-deterministic.'
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
            users: (Set of String) Specifies which users the network policy should be attached to
        importStatements:
            - terraform import snowflake_network_policy_attachment.example attachment_policyname
    snowflake_network_rule:
        subCategory: Preview
        name: snowflake_network_rule
        title: snowflake_network_rule Resource - terraform-provider-snowflake
        examples:
            - name: rule
              manifest: |-
                {
                  "comment": "A rule.",
                  "database": "EXAMPLE_DB",
                  "mode": "INGRESS",
                  "name": "rule",
                  "schema": "EXAMPLE_SCHEMA",
                  "type": "IPV4",
                  "value_list": [
                    "192.168.0.100/24",
                    "29.254.123.20"
                  ]
                }
        argumentDocs:
            comment: (String) Specifies a comment for the network rule.
            create: (String)
            database: (String) The database in which to create the network rule.
            delete: (String)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            mode: (String) Specifies what is restricted by the network rule. Valid values are INGRESS, INTERNAL_STAGE and EGRESS; see https://docs.snowflake.com/en/sql-reference/sql/create-network-rule#required-parameters for details.
            name: (String) Specifies the identifier for the network rule; must be unique for the database and schema in which the network rule is created.
            read: (String)
            schema: (String) The schema in which to create the network rule.
            timeouts: (Block, Optional) (see below for nested schema)
            type: (String) Specifies the type of network identifiers being allowed or blocked. A network rule can have only one type. Allowed values are IPV4, AWSVPCEID, AZURELINKID and HOST_PORT; allowed values are determined by the mode of the network rule; see https://docs.snowflake.com/en/sql-reference/sql/create-network-rule#required-parameters for details.
            update: (String)
            value_list: (Set of String) Specifies the network identifiers that will be allowed or blocked. Valid values in the list are determined by the type of network rule, see https://docs.snowflake.com/en/sql-reference/sql/create-network-rule#required-parameters for details.
        importStatements:
            - terraform import snowflake_network_rule.example 'databaseName|schemaName|networkRuleName'
    snowflake_notification_integration:
        subCategory: Preview
        name: snowflake_notification_integration
        title: snowflake_notification_integration Resource - terraform-provider-snowflake
        examples:
            - name: integration
              manifest: |-
                {
                  "azure_storage_queue_primary_uri": "...",
                  "azure_tenant_id": "...",
                  "comment": "A notification integration.",
                  "direction": "OUTBOUND",
                  "enabled": true,
                  "name": "notification",
                  "notification_provider": "AZURE_STORAGE_QUEUE",
                  "type": "QUEUE"
                }
        argumentDocs:
            aws_sns_external_id: (String) The external ID that Snowflake will use when assuming the AWS role
            aws_sns_iam_user_arn: (String) The Snowflake user that will attempt to assume the AWS role.
            aws_sns_role_arn: (String) AWS IAM role ARN for notification integration to assume. Required for AWS_SNS provider
            aws_sns_topic_arn: (String) AWS SNS Topic ARN for notification integration to connect to. Required for AWS_SNS provider.
            aws_sqs_arn: (String, Deprecated) AWS SQS queue ARN for notification integration to connect to
            aws_sqs_external_id: (String, Deprecated) The external ID that Snowflake will use when assuming the AWS role
            aws_sqs_iam_user_arn: (String, Deprecated) The Snowflake user that will attempt to assume the AWS role.
            aws_sqs_role_arn: (String, Deprecated) AWS IAM role ARN for notification integration to assume
            azure_storage_queue_primary_uri: (String) The queue ID for the Azure Queue Storage queue created for Event Grid notifications. Required for AZURE_STORAGE_QUEUE provider
            azure_tenant_id: (String) The ID of the Azure Active Directory tenant used for identity management. Required for AZURE_STORAGE_QUEUE provider
            comment: (String) A comment for the integration
            create: (String)
            created_on: (String) Date and time when the notification integration was created.
            delete: (String)
            direction: (String, Deprecated) Direction of the cloud messaging with respect to Snowflake (required only for error notifications)
            enabled: '(Boolean) (Default: true)'
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            gcp_pubsub_service_account: (String) The GCP service account identifier that Snowflake will use when assuming the GCP role
            gcp_pubsub_subscription_name: (String) The subscription id that Snowflake will listen to when using the GCP_PUBSUB provider.
            gcp_pubsub_topic_name: (String) The topic id that Snowflake will use to push notifications.
            id: (String) The ID of this resource.
            name: (String)
            notification_provider: '(String) The third-party cloud message queuing service (supported values: AZURE_STORAGE_QUEUE, AWS_SNS, GCP_PUBSUB; AWS_SQS is deprecated and will be removed in the future provider versions)'
            read: (String)
            timeouts: (Block, Optional) (see below for nested schema)
            type: '(String, Deprecated) (Default: QUEUE) A type of integration'
            update: (String)
        importStatements:
            - terraform import snowflake_notification_integration.example name
    snowflake_oauth_integration_for_custom_clients:
        subCategory: Stable
        description: Resource used to manage oauth security integration for custom clients objects. For more information, check security integrations documentation https://docs.snowflake.com/en/sql-reference/sql/create-security-integration-oauth-snowflake.
        name: snowflake_oauth_integration_for_custom_clients
        title: snowflake_oauth_integration_for_custom_clients Resource - terraform-provider-snowflake
        examples:
            - name: basic
              manifest: |-
                {
                  "name": "integration",
                  "oauth_client_type": "CONFIDENTIAL",
                  "oauth_redirect_uri": "https://example.com"
                }
            - name: complete
              manifest: |-
                {
                  "blocked_roles_list": [
                    "ACCOUNTADMIN",
                    "SECURITYADMIN",
                    "${snowflake_role.three.fully_qualified_name}",
                    "${snowflake_role.four.fully_qualified_name}"
                  ],
                  "comment": "my oauth integration",
                  "enabled": "true",
                  "name": "integration",
                  "network_policy": "${snowflake_network_policy.example.fully_qualified_name}",
                  "oauth_allow_non_tls_redirect_uri": "true",
                  "oauth_client_rsa_public_key": "${file(\"rsa.pub\")}",
                  "oauth_client_rsa_public_key_2": "${file(\"rsa2.pub\")}",
                  "oauth_client_type": "CONFIDENTIAL",
                  "oauth_enforce_pkce": "true",
                  "oauth_issue_refresh_tokens": "true",
                  "oauth_redirect_uri": "https://example.com",
                  "oauth_refresh_token_validity": 87600,
                  "oauth_use_secondary_roles": "NONE",
                  "pre_authorized_roles_list": [
                    "${snowflake_role.one.fully_qualified_name}",
                    "${snowflake_role.two.fully_qualified_name}"
                  ]
                }
              references:
                network_policy: snowflake_network_policy.example.fully_qualified_name
        argumentDocs:
            blocked_roles_list: (Set of String) A set of Snowflake roles that a user cannot explicitly consent to using after authenticating. By default, this list includes the ACCOUNTADMIN, ORGADMIN and SECURITYADMIN roles. To remove these privileged roles from the list, use the ALTER ACCOUNT command to set the OAUTH_ADD_PRIVILEGED_ROLES_TO_BLOCKED_LIST account parameter to FALSE. For more information about this resource, see docs.
            category: (String)
            comment: (String) Specifies a comment for the OAuth integration.
            create: (String)
            created_on: (String)
            default: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE SECURITY INTEGRATION for the given integration. (see below for nested schema)
            description: (String)
            enabled: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies whether this OAuth integration is enabled or disabled. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            integration_type: (String)
            key: (String)
            level: (String)
            name: '(String) Specifies the name of the OAuth integration. This name follows the rules for Object Identifiers. The name should be unique among security integrations in your account. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            network_policy: (String) Specifies an existing network policy. This network policy controls network traffic that is attempting to exchange an authorization code for an access or refresh token or to use a refresh token to obtain a new access token. For more information about this resource, see docs.
            oauth_add_privileged_roles_to_blocked_list: (List of Object) (see below for nested schema)
            oauth_allow_non_tls_redirect_uri: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) If true, allows setting oauth_redirect_uri to a URI not protected by TLS. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            oauth_allowed_authorization_endpoints: (List of Object) (see below for nested schema)
            oauth_allowed_token_endpoints: (List of Object) (see below for nested schema)
            oauth_authorization_endpoint: (List of Object) (see below for nested schema)
            oauth_client_rsa_public_key: (String) Specifies a Base64-encoded RSA public key, without the -----BEGIN PUBLIC KEY----- and -----END PUBLIC KEY----- headers. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            oauth_client_rsa_public_key_2: (String) Specifies a Base64-encoded RSA public key, without the -----BEGIN PUBLIC KEY----- and -----END PUBLIC KEY----- headers. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            oauth_client_rsa_public_key_2_fp: (List of Object) (see below for nested schema)
            oauth_client_rsa_public_key_fp: (List of Object) (see below for nested schema)
            oauth_client_type: '(String) Specifies the type of client being registered. Snowflake supports both confidential and public clients. Valid options are: PUBLIC | CONFIDENTIAL.'
            oauth_enforce_pkce: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Boolean that specifies whether Proof Key for Code Exchange (PKCE) should be required for the integration. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            oauth_issue_refresh_tokens: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies whether to allow the client to exchange a refresh token for an access token when the current access token has expired. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            oauth_redirect_uri: (String, Sensitive) Specifies the client URI. After a user is authenticated, the web browser is redirected to this URI.
            oauth_refresh_token_validity: '(Number) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (-1)) Specifies how long refresh tokens should be valid (in seconds). OAUTH_ISSUE_REFRESH_TOKENS must be set to TRUE.'
            oauth_token_endpoint: (List of Object) (see below for nested schema)
            oauth_use_secondary_roles: '(String) Specifies whether default secondary roles set in the user properties are activated by default in the session being opened. Valid options are: IMPLICIT | NONE.'
            pre_authorized_roles_list: (Set of String) A set of Snowflake roles that a user does not need to explicitly consent to using after authenticating. For more information about this resource, see docs.
            read: (String)
            related_parameters: (List of Object) Parameters related to this security integration. (see below for nested schema)
            show_output: (List of Object) Outputs the result of SHOW SECURITY INTEGRATION for the given integration. (see below for nested schema)
            timeouts: (Block, Optional) (see below for nested schema)
            type: (String)
            update: (String)
            value: (String)
        importStatements:
            - terraform import snowflake_oauth_integration_for_custom_clients.example '"<integration_name>"'
    snowflake_oauth_integration_for_partner_applications:
        subCategory: Stable
        description: Resource used to manage oauth security integration for partner applications objects. For more information, check security integrations documentation https://docs.snowflake.com/en/sql-reference/sql/create-security-integration-oauth-snowflake.
        name: snowflake_oauth_integration_for_partner_applications
        title: snowflake_oauth_integration_for_partner_applications Resource - terraform-provider-snowflake
        examples:
            - name: test
              manifest: |-
                {
                  "name": "example",
                  "oauth_client": "LOOKER",
                  "oauth_redirect_uri": "http://example.com"
                }
            - name: test
              manifest: |-
                {
                  "blocked_roles_list": [
                    "ACCOUNTADMIN",
                    "SECURITYADMIN",
                    "${snowflake_role.one.fully_qualified_name}",
                    "${snowflake_role.two.fully_qualified_name}"
                  ],
                  "comment": "example oauth integration for partner applications",
                  "enabled": "true",
                  "name": "example",
                  "oauth_client": "TABLEAU_DESKTOP",
                  "oauth_issue_refresh_tokens": "true",
                  "oauth_redirect_uri": "http://example.com",
                  "oauth_refresh_token_validity": 3600,
                  "oauth_use_secondary_roles": "IMPLICIT"
                }
        argumentDocs:
            blocked_roles_list: (Set of String) A set of Snowflake roles that a user cannot explicitly consent to using after authenticating. By default, this list includes the ACCOUNTADMIN, ORGADMIN and SECURITYADMIN roles. To remove these privileged roles from the list, use the ALTER ACCOUNT command to set the OAUTH_ADD_PRIVILEGED_ROLES_TO_BLOCKED_LIST account parameter to FALSE. For more information about this resource, see docs.
            category: (String)
            comment: (String) Specifies a comment for the OAuth integration.
            create: (String)
            created_on: (String)
            default: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE SECURITY INTEGRATION for the given integration. (see below for nested schema)
            description: (String)
            enabled: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies whether this OAuth integration is enabled or disabled. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            integration_type: (String)
            key: (String)
            level: (String)
            name: '(String) Specifies the name of the OAuth integration. This name follows the rules for Object Identifiers. The name should be unique among security integrations in your account. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            network_policy: (List of Object) (see below for nested schema)
            oauth_add_privileged_roles_to_blocked_list: (List of Object) (see below for nested schema)
            oauth_allow_non_tls_redirect_uri: (List of Object) (see below for nested schema)
            oauth_allowed_authorization_endpoints: (List of Object) (see below for nested schema)
            oauth_allowed_token_endpoints: (List of Object) (see below for nested schema)
            oauth_authorization_endpoint: (List of Object) (see below for nested schema)
            oauth_client: '(String) Creates an OAuth interface between Snowflake and a partner application. Valid options are: LOOKER | TABLEAU_DESKTOP | TABLEAU_SERVER.'
            oauth_client_rsa_public_key_2_fp: (List of Object) (see below for nested schema)
            oauth_client_rsa_public_key_fp: (List of Object) (see below for nested schema)
            oauth_client_type: (List of Object) (see below for nested schema)
            oauth_enforce_pkce: (List of Object) (see below for nested schema)
            oauth_issue_refresh_tokens: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies whether to allow the client to exchange a refresh token for an access token when the current access token has expired. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            oauth_redirect_uri: (String, Sensitive) Specifies the client URI. After a user is authenticated, the web browser is redirected to this URI. The field should be only set when OAUTH_CLIENT = LOOKER. In any other case the field should be left out empty.
            oauth_refresh_token_validity: '(Number) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (-1)) Specifies how long refresh tokens should be valid (in seconds). OAUTH_ISSUE_REFRESH_TOKENS must be set to TRUE.'
            oauth_token_endpoint: (List of Object) (see below for nested schema)
            oauth_use_secondary_roles: '(String) Specifies whether default secondary roles set in the user properties are activated by default in the session being opened. Valid options are: IMPLICIT | NONE.'
            pre_authorized_roles_list: (List of Object) (see below for nested schema)
            read: (String)
            related_parameters: (List of Object) Parameters related to this security integration. (see below for nested schema)
            show_output: (List of Object) Outputs the result of SHOW SECURITY INTEGRATION for the given integration. (see below for nested schema)
            timeouts: (Block, Optional) (see below for nested schema)
            type: (String)
            update: (String)
            value: (String)
        importStatements:
            - terraform import snowflake_oauth_integration_for_partner_applications.example "name"
    snowflake_object_parameter:
        subCategory: Preview
        name: snowflake_object_parameter
        title: snowflake_object_parameter Resource - terraform-provider-snowflake
        examples:
            - name: o
              manifest: |-
                {
                  "key": "SUSPEND_TASK_AFTER_NUM_FAILURES",
                  "object_identifier": [
                    {
                      "name": "${snowflake_database.d.name}"
                    }
                  ],
                  "object_type": "DATABASE",
                  "value": "33"
                }
              references:
                object_identifier.name: snowflake_database.d.name
              dependencies:
                snowflake_database.d: |-
                    {
                      "name": "TEST_DB"
                    }
                snowflake_schema.s: |-
                    {
                      "database": "${snowflake_database.d.name}",
                      "name": "TEST_SCHEMA"
                    }
                snowflake_table.t: |-
                    {
                      "column": [
                        {
                          "name": "id",
                          "type": "NUMBER"
                        }
                      ],
                      "database": "${snowflake_database.d.name}",
                      "name": "TEST_TABLE",
                      "schema": "${snowflake_schema.s.name}"
                    }
            - name: o2
              manifest: |-
                {
                  "key": "USER_TASK_TIMEOUT_MS",
                  "object_identifier": [
                    {
                      "database": "${snowflake_database.d.name}",
                      "name": "${snowflake_schema.s.name}"
                    }
                  ],
                  "object_type": "SCHEMA",
                  "value": "500"
                }
              references:
                object_identifier.database: snowflake_database.d.name
                object_identifier.name: snowflake_schema.s.name
              dependencies:
                snowflake_database.d: |-
                    {
                      "name": "TEST_DB"
                    }
                snowflake_schema.s: |-
                    {
                      "database": "${snowflake_database.d.name}",
                      "name": "TEST_SCHEMA"
                    }
                snowflake_table.t: |-
                    {
                      "column": [
                        {
                          "name": "id",
                          "type": "NUMBER"
                        }
                      ],
                      "database": "${snowflake_database.d.name}",
                      "name": "TEST_TABLE",
                      "schema": "${snowflake_schema.s.name}"
                    }
            - name: o3
              manifest: |-
                {
                  "key": "DATA_RETENTION_TIME_IN_DAYS",
                  "object_identifier": [
                    {
                      "database": "${snowflake_database.d.name}",
                      "name": "${snowflake_table.t.name}",
                      "schema": "${snowflake_schema.s.name}"
                    }
                  ],
                  "object_type": "TABLE",
                  "value": "89"
                }
              references:
                object_identifier.database: snowflake_database.d.name
                object_identifier.name: snowflake_table.t.name
                object_identifier.schema: snowflake_schema.s.name
              dependencies:
                snowflake_database.d: |-
                    {
                      "name": "TEST_DB"
                    }
                snowflake_schema.s: |-
                    {
                      "database": "${snowflake_database.d.name}",
                      "name": "TEST_SCHEMA"
                    }
                snowflake_table.t: |-
                    {
                      "column": [
                        {
                          "name": "id",
                          "type": "NUMBER"
                        }
                      ],
                      "database": "${snowflake_database.d.name}",
                      "name": "TEST_TABLE",
                      "schema": "${snowflake_schema.s.name}"
                    }
            - name: o4
              manifest: |-
                {
                  "key": "DATA_RETENTION_TIME_IN_DAYS",
                  "on_account": true,
                  "value": "89"
                }
              dependencies:
                snowflake_database.d: |-
                    {
                      "name": "TEST_DB"
                    }
                snowflake_schema.s: |-
                    {
                      "database": "${snowflake_database.d.name}",
                      "name": "TEST_SCHEMA"
                    }
                snowflake_table.t: |-
                    {
                      "column": [
                        {
                          "name": "id",
                          "type": "NUMBER"
                        }
                      ],
                      "database": "${snowflake_database.d.name}",
                      "name": "TEST_TABLE",
                      "schema": "${snowflake_schema.s.name}"
                    }
        argumentDocs:
            create: (String)
            database: (String) Name of the database that the object was created in.
            delete: (String)
            id: (String) The ID of this resource.
            key: (String) Name of object parameter. Valid values are those in object parameters.
            name: (String) Name of the object to set the parameter for.
            object_identifier: (Block List) Specifies the object identifier for the object parameter. If no value is provided, then the resource will default to setting the object parameter at account level. (see below for nested schema)
            object_type: (String) Type of object to which the parameter applies. Valid values are those in object types. If no value is provided, then the resource will default to setting the object parameter at account level.
            on_account: '(Boolean) (Default: false) If true, the object parameter will be set on the account level.'
            read: (String)
            schema: (String) Name of the schema that the object was created in.
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
            value: (String) Value of object parameter, as a string. Constraints are the same as those for the parameters in Snowflake documentation.
        importStatements:
            - terraform import snowflake_object_parameter.s <key>|<object_type>|<object_identifier>
    snowflake_password_policy Resource - terraform-provider-snowflake:
        subCategory: Preview
        description: A password policy specifies the requirements that must be met to create and reset a password to authenticate to Snowflake.
        name: snowflake_password_policy Resource - terraform-provider-snowflake
        title: snowflake_password_policy Resource - terraform-provider-snowflake
        argumentDocs:
            comment: (String) Adds a comment or overwrites an existing comment for the password policy.
            create: (String)
            database: (String) The database this password policy belongs to.
            delete: (String)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            history: '(Number) (Default: 0) Specifies the number of the most recent passwords that Snowflake stores. These stored passwords cannot be repeated when a user updates their password value. The current password value does not count towards the history. When you increase the history value, Snowflake saves the previous values. When you decrease the value, Snowflake saves the stored values up to that value that is set. For example, if the history value is 8 and you change the history value to 3, Snowflake stores the most recent 3 passwords and deletes the 5 older password values from the history. Default: 0 Max: 24'
            id: (String) The ID of this resource.
            if_not_exists: '(Boolean) (Default: false) Prevent overwriting a previous password policy with the same name.'
            lockout_time_mins: '(Number) (Default: 15) Specifies the number of minutes the user account will be locked after exhausting the designated number of password retries (i.e. PASSWORD_MAX_RETRIES). Supported range: 1 to 999, inclusive. Default: 15'
            max_age_days: '(Number) (Default: 90) Specifies the maximum number of days before the password must be changed. Supported range: 0 to 999, inclusive. A value of zero (i.e. 0) indicates that the password does not need to be changed. Snowflake does not recommend choosing this value for a default account-level password policy or for any user-level policy. Instead, choose a value that meets your internal security guidelines. Default: 90, which means the password must be changed every 90 days.'
            max_length: '(Number) (Default: 256) Specifies the maximum number of characters the password must contain. This number must be greater than or equal to the sum of PASSWORD_MIN_LENGTH, PASSWORD_MIN_UPPER_CASE_CHARS, and PASSWORD_MIN_LOWER_CASE_CHARS. Supported range: 8 to 256, inclusive. Default: 256'
            max_retries: '(Number) (Default: 5) Specifies the maximum number of attempts to enter a password before being locked out. Supported range: 1 to 10, inclusive. Default: 5'
            min_age_days: '(Number) (Default: 0) Specifies the number of days the user must wait before a recently changed password can be changed again. Supported range: 0 to 999, inclusive. Default: 0'
            min_length: '(Number) (Default: 8) Specifies the minimum number of characters the password must contain. Supported range: 8 to 256, inclusive. Default: 8'
            min_lower_case_chars: '(Number) (Default: 1) Specifies the minimum number of lowercase characters the password must contain. Supported range: 0 to 256, inclusive. Default: 1'
            min_numeric_chars: '(Number) (Default: 1) Specifies the minimum number of numeric characters the password must contain. Supported range: 0 to 256, inclusive. Default: 1'
            min_special_chars: '(Number) (Default: 1) Specifies the minimum number of special characters the password must contain. Supported range: 0 to 256, inclusive. Default: 1'
            min_upper_case_chars: '(Number) (Default: 1) Specifies the minimum number of uppercase characters the password must contain. Supported range: 0 to 256, inclusive. Default: 1'
            name: (String) Identifier for the password policy; must be unique for your account.
            or_replace: '(Boolean) (Default: false) Whether to override a previous password policy with the same name.'
            read: (String)
            schema: (String) The schema this password policy belongs to.
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
        importStatements: []
    snowflake_pipe:
        subCategory: Preview
        name: snowflake_pipe
        title: snowflake_pipe Resource - terraform-provider-snowflake
        examples:
            - name: pipe
              manifest: |-
                {
                  "auto_ingest": false,
                  "aws_sns_topic_arn": "...",
                  "comment": "A pipe.",
                  "copy_statement": "copy into ${snowflake_table.table.fully_qualified_name} from @${snowflake_stage.stage.fully_qualified_name}",
                  "database": "${snowflake_database.database.name}",
                  "name": "PIPE",
                  "notification_channel": "...",
                  "schema": "${snowflake_schema.schema.name}"
                }
              references:
                database: snowflake_database.database.name
                schema: snowflake_schema.schema.name
              dependencies:
                snowflake_stage.stage: |-
                    {
                      "database": "${snowflake_database.database.name}",
                      "encryption": "TYPE = 'NONE'",
                      "name": "STAGE",
                      "schema": "${snowflake_schema.schema.name}",
                      "storage_integration": "${snowflake_storage_integration.storage_integration.name}",
                      "url": "s3://com.example.bucket/prefix"
                    }
            - name: pipe_with_stage_change_trigger
              manifest: |-
                {
                  "copy_statement": "copy into ${snowflake_table.table.fully_qualified_name} from @${snowflake_stage.stage.fully_qualified_name}",
                  "database": "${snowflake_stage.stage.database}",
                  "lifecycle": [
                    {
                      "replace_triggered_by": [
                        "${snowflake_stage.stage.url}",
                        "${snowflake_stage.stage.storage_integration}",
                        "${snowflake_stage.stage.encryption}"
                      ]
                    }
                  ],
                  "name": "PIPE_WITH_STAGE_CHANGE_TRIGGER",
                  "schema": "${snowflake_stage.stage.schema}"
                }
              references:
                database: snowflake_stage.stage.database
                schema: snowflake_stage.stage.schema
              dependencies:
                snowflake_stage.stage: |-
                    {
                      "database": "${snowflake_database.database.name}",
                      "encryption": "TYPE = 'NONE'",
                      "name": "STAGE",
                      "schema": "${snowflake_schema.schema.name}",
                      "storage_integration": "${snowflake_storage_integration.storage_integration.name}",
                      "url": "s3://com.example.bucket/prefix"
                    }
        argumentDocs:
            auto_ingest: '(Boolean) (Default: false) Specifies a auto_ingest param for the pipe.'
            aws_sns_topic_arn: (String) Specifies the Amazon Resource Name (ARN) for the SNS topic for your S3 bucket.
            comment: (String) Specifies a comment for the pipe.
            copy_statement: (String) Specifies the copy statement for the pipe.
            create: (String)
            database: (String) The database in which to create the pipe.
            delete: (String)
            error_integration: (String) Specifies the name of the notification integration used for error notifications.
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            integration: (String) Specifies an integration for the pipe.
            name: (String) Specifies the identifier for the pipe; must be unique for the database and schema in which the pipe is created.
            notification_channel: (String) Amazon Resource Name of the Amazon SQS queue for the stage named in the DEFINITION column.
            owner: (String) Name of the role that owns the pipe.
            read: (String)
            schema: (String) The schema in which to create the pipe.
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
        importStatements:
            - |-
              # format is database name | schema name | pipe name
              terraform import snowflake_pipe.example 'dbName|schemaName|pipeName'
    snowflake_primary_connection:
        subCategory: Stable
        description: Resource used to manage primary connections. For managing replicated connection check resource snowflake_secondary_connection ./secondary_connection. For more information, check connection documentation https://docs.snowflake.com/en/sql-reference/sql/create-connection.html.
        name: snowflake_primary_connection
        title: snowflake_primary_connection Resource - terraform-provider-snowflake
        examples:
            - name: basic
              manifest: |-
                {
                  "name": "connection_name"
                }
            - name: complete
              manifest: |-
                {
                  "comment": "my complete connection",
                  "enable_failover_to_accounts": [
                    "\"\u003csecondary_account_organization_name\u003e\".\"\u003csecondary_account_name\u003e\""
                  ],
                  "name": "connection_name"
                }
        argumentDocs:
            account_locator: (String)
            account_name: (String)
            comment: (String) Specifies a comment for the connection.
            connection_url: (String)
            create: (String)
            created_on: (String)
            delete: (String)
            enable_failover_to_accounts: (List of String) Enables failover for given connection to provided accounts. Specifies a list of accounts in your organization where a secondary connection for this primary connection can be promoted to serve as the primary connection. Include your organization name for each account in the list. For more information about this resource, see docs.
            failover_allowed_to_accounts: (List of String)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            is_primary: (Boolean) Indicates if the connection is primary. When Terraform detects that the connection is not primary, the resource is recreated.
            name: '(String) String that specifies the identifier (i.e. name) for the connection. Must start with an alphabetic character and may only contain letters, decimal digits (0-9), and underscores (_). For a primary connection, the name must be unique across connection names and account names in the organization.  Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            organization_name: (String)
            primary: (String)
            read: (String)
            region_group: (String)
            show_output: (List of Object) Outputs the result of SHOW CONNECTIONS for the given connection. (see below for nested schema)
            snowflake_region: (String)
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
        importStatements:
            - terraform import snowflake_primary_connection.example '"<primary_connection_name>"'
    snowflake_procedure_java:
        subCategory: Preview
        description: Resource used to manage java procedure objects. For more information, check procedure documentation https://docs.snowflake.com/en/sql-reference/sql/create-procedure.
        name: snowflake_procedure_java
        title: snowflake_procedure_java Resource - terraform-provider-snowflake
        examples:
            - name: basic
              manifest: |-
                {
                  "arguments": [
                    {
                      "arg_data_type": "VARCHAR(100)",
                      "arg_name": "x"
                    }
                  ],
                  "database": "Database",
                  "handler": "TestFunc.echoVarchar",
                  "name": "ProcedureName",
                  "procedure_definition": "  import com.snowflake.snowpark_java.*;\n  class TestFunc {\n    public static String echoVarchar(Session session, String x) {\n      return x;\n    }\n  }\n",
                  "return_type": "VARCHAR(100)",
                  "runtime_version": "11",
                  "schema": "Schema",
                  "snowpark_package": "1.14.0"
                }
            - name: full
              manifest: |-
                {
                  "arguments": [
                    {
                      "arg_data_type": "VARCHAR(100)",
                      "arg_name": "x"
                    }
                  ],
                  "comment": "some comment",
                  "database": "Database",
                  "execute_as": "CALLER",
                  "external_access_integrations": [
                    "INTEGRATION_1",
                    "INTEGRATION_2"
                  ],
                  "handler": "TestFunc.echoVarchar",
                  "imports": [
                    {
                      "path_on_stage": "tf-1734028486-OLJpF.jar",
                      "stage_location": "~"
                    },
                    {
                      "path_on_stage": "tf-1734028491-EMoDC.jar",
                      "stage_location": "~"
                    }
                  ],
                  "is_secure": "false",
                  "name": "ProcedureName",
                  "null_input_behavior": "CALLED ON NULL INPUT",
                  "packages": [
                    "com.snowflake:telemetry:0.1.0"
                  ],
                  "procedure_definition": "    import com.snowflake.snowpark_java.*;\n  class TestFunc {\n    public static String echoVarchar(Session session, String x) {\n      return x;\n    }\n  }\n",
                  "return_type": "VARCHAR(100)",
                  "runtime_version": "11",
                  "schema": "Schema",
                  "secrets": [
                    {
                      "secret_id": "${snowflake_secret_with_generic_string.example1.fully_qualified_name}",
                      "secret_variable_name": "abc"
                    },
                    {
                      "secret_id": "${snowflake_secret_with_generic_string.example2.fully_qualified_name}",
                      "secret_variable_name": "def"
                    }
                  ],
                  "snowpark_package": "1.14.0",
                  "target_path": [
                    {
                      "path_on_stage": "tf-1734028493-OkoTf.jar",
                      "stage_location": "${snowflake_stage.example.fully_qualified_name}"
                    }
                  ]
                }
              references:
                secrets.secret_id: snowflake_secret_with_generic_string.example2.fully_qualified_name
                target_path.stage_location: snowflake_stage.example.fully_qualified_name
        argumentDocs:
            arg_data_type: (String) The argument type.
            arg_default_value: (String) Optional default value for the argument. For text values use single quotes. Numeric values can be unquoted. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            arg_name: (String) The argument name. The provider wraps it in double quotes by default, so be aware of that while referencing the argument in the procedure definition.
            arguments: (Block List) List of the arguments for the procedure. Consult the docs for more details. (see below for nested schema)
            arguments_raw: (String)
            catalog_name: (String)
            comment: '(String) (Default: user-defined procedure) Specifies a comment for the procedure.'
            create: (String)
            created_on: (String)
            database: '(String) The database in which to create the procedure. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            default: (String)
            delete: (String)
            description: (String)
            enable_console_output: (Boolean) Enable stdout/stderr fast path logging for anonyous stored procs. This is a public parameter (similar to LOG_LEVEL). For more information, check ENABLE_CONSOLE_OUTPUT docs.
            execute_as: '(String) Specifies whether the stored procedure executes with the privileges of the owner (an “owner’s rights” stored procedure) or with the privileges of the caller (a “caller’s rights” stored procedure). If you execute the statement CREATE PROCEDURE … EXECUTE AS CALLER, then in the future the procedure will execute as a caller’s rights procedure. If you execute CREATE PROCEDURE … EXECUTE AS OWNER, then the procedure will execute as an owner’s rights procedure. For more information, see Understanding caller’s rights and owner’s rights stored procedures. Valid values are (case-insensitive): CALLER | OWNER.'
            external_access_integrations: (Set of String) The names of external access integrations needed in order for this procedure’s handler code to access external networks. An external access integration specifies network rules and secrets that specify external locations and credentials (if any) allowed for use by handler code when making requests of an external network, such as an external REST API.
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            handler: '(String) Use the fully qualified name of the method or function for the stored procedure. This is typically in the following form com.my_company.my_package.MyClass.myMethod where com.my_company.my_package corresponds to the package containing the object or class: package com.my_company.my_package;.'
            id: (String) The ID of this resource.
            imports: (Block Set) The location (stage), path, and name of the file(s) to import. You must set the IMPORTS clause to include any files that your stored procedure depends on. If you are writing an in-line stored procedure, you can omit this clause, unless your code depends on classes defined outside the stored procedure or resource files. If you are writing a stored procedure with a staged handler, you must also include a path to the JAR file containing the stored procedure’s handler code. The IMPORTS definition cannot reference variables from arguments that are passed into the stored procedure. Each file in the IMPORTS clause must have a unique name, even if the files are in different subdirectories or different stages. (see below for nested schema)
            is_aggregate: (Boolean)
            is_ansi: (Boolean)
            is_builtin: (Boolean)
            is_secure: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies that the procedure is secure. For more information about secure procedures, see Protecting Sensitive Information with Secure UDFs and Stored Procedures. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            is_table_function: (Boolean)
            key: (String)
            level: (String)
            log_level: (String) LOG_LEVEL to use when filtering events For more information, check LOG_LEVEL docs.
            max_num_arguments: (Number)
            metric_level: (String) METRIC_LEVEL value to control whether to emit metrics to Event Table For more information, check METRIC_LEVEL docs.
            min_num_arguments: (Number)
            name: '(String) The name of the procedure; the identifier does not need to be unique for the schema in which the procedure is created because stored procedures are identified and resolved by the combination of the name and argument types. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            null_input_behavior: '(String) Specifies the behavior of the procedure when called with null inputs. Valid values are (case-insensitive): CALLED ON NULL INPUT | RETURNS NULL ON NULL INPUT.'
            packages: (Set of String) List of the names of packages deployed in Snowflake that should be included in the handler code’s execution environment. The Snowpark package is required for stored procedures, but is specified in the snowpark_package attribute. For more information about Snowpark, see Snowpark API.
            parameters: (List of Object) Outputs the result of SHOW PARAMETERS IN PROCEDURE for the given procedure. (see below for nested schema)
            path_on_stage: (String) Path for import on stage, without the leading /.
            procedure_definition: (String) Defines the code executed by the stored procedure. The definition can consist of any valid code. Wrapping $$ signs are added by the provider automatically; do not include them. The procedure_definition value must be Java source code. For more information, see Java (using Snowpark). To mitigate permadiff on this field, the provider replaces blank characters with a space. This can lead to false positives in cases where a change in case or run of whitespace is semantically significant.
            procedure_language: (String) Specifies language for the procedure. Used to detect external changes.
            read: (String)
            return_type: (String) Specifies the type of the result returned by the stored procedure. For <result_data_type>, use the Snowflake data type that corresponds to the type of the language that you are using (see SQL-Java Data Type Mappings). For RETURNS TABLE ( [ col_name col_data_type [ , ... ] ] ), if you know the Snowflake data types of the columns in the returned table, specify the column names and types. Otherwise (e.g. if you are determining the column types during run time), you can omit the column names and types (i.e. TABLE ()).
            runtime_version: '(String) The language runtime version to use. Currently, the supported versions are: 11.'
            schema: '(String) The schema in which to create the procedure. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            secret_id: (String) Fully qualified name of the allowed secret. You will receive an error if you specify a SECRETS value whose secret isn’t also included in an integration specified by the EXTERNAL_ACCESS_INTEGRATIONS parameter.
            secret_variable_name: (String) The variable that will be used in handler code when retrieving information from the secret.
            secrets: (Block Set) Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from secrets in handler code. Secrets you specify here must be allowed by the external access integration specified as a value of this CREATE FUNCTION command’s EXTERNAL_ACCESS_INTEGRATIONS parameter. (see below for nested schema)
            show_output: (List of Object) Outputs the result of SHOW PROCEDURE for the given procedure. (see below for nested schema)
            snowpark_package: (String) The Snowpark package is required for stored procedures, so it must always be present. For more information about Snowpark, see Snowpark API.
            stage_location: (String) Stage location without leading @. To use your user's stage set this to ~, otherwise pass fully qualified name of the stage (with every part contained in double quotes or use snowflake_stage.<your stage's resource name>.fully_qualified_name if you manage this stage through terraform).
            target_path: '(Block Set, Max: 1) Use the fully qualified name of the method or function for the stored procedure. This is typically in the following form com.my_company.my_package.MyClass.myMethod where com.my_company.my_package corresponds to the package containing the object or class: package com.my_company.my_package;. (see below for nested schema)'
            timeouts: (Block, Optional) (see below for nested schema)
            trace_level: (String) Trace level value to use when generating/filtering trace events For more information, check TRACE_LEVEL docs.
            update: (String)
            valid_for_clustering: (Boolean)
            value: (String)
        importStatements:
            - terraform import snowflake_procedure_java.example '"<database_name>"."<schema_name>"."<function_name>"(varchar, varchar, varchar)'
    snowflake_procedure_javascript:
        subCategory: Preview
        description: Resource used to manage javascript procedure objects. For more information, check procedure documentation https://docs.snowflake.com/en/sql-reference/sql/create-procedure.
        name: snowflake_procedure_javascript
        title: snowflake_procedure_javascript Resource - terraform-provider-snowflake
        examples:
            - name: basic
              manifest: |-
                {
                  "arguments": [
                    {
                      "arg_data_type": "VARCHAR(100)",
                      "arg_name": "x"
                    }
                  ],
                  "database": "Database",
                  "name": "Name",
                  "procedure_definition": "  if (x == 0) {\n    return 1;\n  } else {\n    return 2;\n  }\n",
                  "return_type": "VARCHAR(100)",
                  "schema": "Schema"
                }
        argumentDocs:
            arg_data_type: (String) The argument type.
            arg_default_value: (String) Optional default value for the argument. For text values use single quotes. Numeric values can be unquoted. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            arg_name: (String) The argument name. The provider wraps it in double quotes by default, so be aware of that while referencing the argument in the procedure definition.
            arguments: (Block List) List of the arguments for the procedure. Consult the docs for more details. (see below for nested schema)
            arguments_raw: (String)
            catalog_name: (String)
            comment: '(String) (Default: user-defined procedure) Specifies a comment for the procedure.'
            create: (String)
            created_on: (String)
            database: '(String) The database in which to create the procedure. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            default: (String)
            delete: (String)
            description: (String)
            enable_console_output: (Boolean) Enable stdout/stderr fast path logging for anonyous stored procs. This is a public parameter (similar to LOG_LEVEL). For more information, check ENABLE_CONSOLE_OUTPUT docs.
            execute_as: '(String) Specifies whether the stored procedure executes with the privileges of the owner (an “owner’s rights” stored procedure) or with the privileges of the caller (a “caller’s rights” stored procedure). If you execute the statement CREATE PROCEDURE … EXECUTE AS CALLER, then in the future the procedure will execute as a caller’s rights procedure. If you execute CREATE PROCEDURE … EXECUTE AS OWNER, then the procedure will execute as an owner’s rights procedure. For more information, see Understanding caller’s rights and owner’s rights stored procedures. Valid values are (case-insensitive): CALLER | OWNER.'
            external_access_integrations: (String)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            is_aggregate: (Boolean)
            is_ansi: (Boolean)
            is_builtin: (Boolean)
            is_secure: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies that the procedure is secure. For more information about secure procedures, see Protecting Sensitive Information with Secure UDFs and Stored Procedures. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            is_table_function: (Boolean)
            key: (String)
            level: (String)
            log_level: (String) LOG_LEVEL to use when filtering events For more information, check LOG_LEVEL docs.
            max_num_arguments: (Number)
            metric_level: (String) METRIC_LEVEL value to control whether to emit metrics to Event Table For more information, check METRIC_LEVEL docs.
            min_num_arguments: (Number)
            name: '(String) The name of the procedure; the identifier does not need to be unique for the schema in which the procedure is created because stored procedures are identified and resolved by the combination of the name and argument types. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            null_input_behavior: '(String) Specifies the behavior of the procedure when called with null inputs. Valid values are (case-insensitive): CALLED ON NULL INPUT | RETURNS NULL ON NULL INPUT.'
            parameters: (List of Object) Outputs the result of SHOW PARAMETERS IN PROCEDURE for the given procedure. (see below for nested schema)
            procedure_definition: (String) Defines the code executed by the stored procedure. The definition can consist of any valid code. Wrapping $$ signs are added by the provider automatically; do not include them. The procedure_definition value must be JavaScript source code. For more information, see JavaScript. To mitigate permadiff on this field, the provider replaces blank characters with a space. This can lead to false positives in cases where a change in case or run of whitespace is semantically significant.
            procedure_language: (String) Specifies language for the procedure. Used to detect external changes.
            read: (String)
            return_type: (String) Specifies the type of the result returned by the stored procedure. For <result_data_type>, use the Snowflake data type that corresponds to the type of the language that you are using (see SQL and JavaScript data type mapping). For RETURNS TABLE ( [ col_name col_data_type [ , ... ] ] ), if you know the Snowflake data types of the columns in the returned table, specify the column names and types. Otherwise (e.g. if you are determining the column types during run time), you can omit the column names and types (i.e. TABLE ()).
            schema: '(String) The schema in which to create the procedure. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            secrets: (String)
            show_output: (List of Object) Outputs the result of SHOW PROCEDURE for the given procedure. (see below for nested schema)
            timeouts: (Block, Optional) (see below for nested schema)
            trace_level: (String) Trace level value to use when generating/filtering trace events For more information, check TRACE_LEVEL docs.
            update: (String)
            valid_for_clustering: (Boolean)
            value: (String)
        importStatements:
            - terraform import snowflake_procedure_javascript.example '"<database_name>"."<schema_name>"."<function_name>"(varchar, varchar, varchar)'
    snowflake_procedure_python:
        subCategory: Preview
        description: Resource used to manage python procedure objects. For more information, check procedure documentation https://docs.snowflake.com/en/sql-reference/sql/create-procedure.
        name: snowflake_procedure_python
        title: snowflake_procedure_python Resource - terraform-provider-snowflake
        examples:
            - name: w
              manifest: |-
                {
                  "arguments": [
                    {
                      "arg_data_type": "VARCHAR(100)",
                      "arg_name": "x"
                    }
                  ],
                  "database": "Database",
                  "handler": "echoVarchar",
                  "name": "Name",
                  "procedure_definition": "  def echoVarchar(x):\n  result = ''\n  for a in range(5):\n    result += x\n  return result\n",
                  "return_type": "VARCHAR(100)",
                  "runtime_version": "3.9",
                  "schema": "Schema",
                  "snowpark_package": "1.14.0"
                }
        argumentDocs:
            arg_data_type: (String) The argument type.
            arg_default_value: (String) Optional default value for the argument. For text values use single quotes. Numeric values can be unquoted. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            arg_name: (String) The argument name. The provider wraps it in double quotes by default, so be aware of that while referencing the argument in the procedure definition.
            arguments: (Block List) List of the arguments for the procedure. Consult the docs for more details. (see below for nested schema)
            arguments_raw: (String)
            catalog_name: (String)
            comment: '(String) (Default: user-defined procedure) Specifies a comment for the procedure.'
            create: (String)
            created_on: (String)
            database: '(String) The database in which to create the procedure. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            default: (String)
            delete: (String)
            description: (String)
            enable_console_output: (Boolean) Enable stdout/stderr fast path logging for anonyous stored procs. This is a public parameter (similar to LOG_LEVEL). For more information, check ENABLE_CONSOLE_OUTPUT docs.
            execute_as: '(String) Specifies whether the stored procedure executes with the privileges of the owner (an “owner’s rights” stored procedure) or with the privileges of the caller (a “caller’s rights” stored procedure). If you execute the statement CREATE PROCEDURE … EXECUTE AS CALLER, then in the future the procedure will execute as a caller’s rights procedure. If you execute CREATE PROCEDURE … EXECUTE AS OWNER, then the procedure will execute as an owner’s rights procedure. For more information, see Understanding caller’s rights and owner’s rights stored procedures. Valid values are (case-insensitive): CALLER | OWNER.'
            external_access_integrations: (Set of String) The names of external access integrations needed in order for this procedure’s handler code to access external networks. An external access integration specifies network rules and secrets that specify external locations and credentials (if any) allowed for use by handler code when making requests of an external network, such as an external REST API.
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            handler: (String) Use the name of the stored procedure’s function or method. This can differ depending on whether the code is in-line or referenced at a stage. When the code is in-line, you can specify just the function name. When the code is imported from a stage, specify the fully-qualified handler function name as <module_name>.<function_name>.
            id: (String) The ID of this resource.
            imports: (Block Set) The location (stage), path, and name of the file(s) to import. You must set the IMPORTS clause to include any files that your stored procedure depends on. If you are writing an in-line stored procedure, you can omit this clause, unless your code depends on classes defined outside the stored procedure or resource files. If your stored procedure’s code will be on a stage, you must also include a path to the module file your code is in. The IMPORTS definition cannot reference variables from arguments that are passed into the stored procedure. Each file in the IMPORTS clause must have a unique name, even if the files are in different subdirectories or different stages. (see below for nested schema)
            is_aggregate: (Boolean)
            is_ansi: (Boolean)
            is_builtin: (Boolean)
            is_secure: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies that the procedure is secure. For more information about secure procedures, see Protecting Sensitive Information with Secure UDFs and Stored Procedures. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            is_table_function: (Boolean)
            key: (String)
            level: (String)
            log_level: (String) LOG_LEVEL to use when filtering events For more information, check LOG_LEVEL docs.
            max_num_arguments: (Number)
            metric_level: (String) METRIC_LEVEL value to control whether to emit metrics to Event Table For more information, check METRIC_LEVEL docs.
            min_num_arguments: (Number)
            name: '(String) The name of the procedure; the identifier does not need to be unique for the schema in which the procedure is created because stored procedures are identified and resolved by the combination of the name and argument types. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            null_input_behavior: '(String) Specifies the behavior of the procedure when called with null inputs. Valid values are (case-insensitive): CALLED ON NULL INPUT | RETURNS NULL ON NULL INPUT.'
            packages: (Set of String) List of the names of packages deployed in Snowflake that should be included in the handler code’s execution environment. The Snowpark package is required for stored procedures, but is specified in the snowpark_package attribute. For more information about Snowpark, see Snowpark API.
            parameters: (List of Object) Outputs the result of SHOW PARAMETERS IN PROCEDURE for the given procedure. (see below for nested schema)
            path_on_stage: (String) Path for import on stage, without the leading /.
            procedure_definition: (String) Defines the code executed by the stored procedure. The definition can consist of any valid code. Wrapping $$ signs are added by the provider automatically; do not include them. The procedure_definition value must be Python source code. For more information, see Python (using Snowpark). To mitigate permadiff on this field, the provider replaces blank characters with a space. This can lead to false positives in cases where a change in case or run of whitespace is semantically significant.
            procedure_language: (String) Specifies language for the procedure. Used to detect external changes.
            read: (String)
            return_type: (String) Specifies the type of the result returned by the stored procedure. For <result_data_type>, use the Snowflake data type that corresponds to the type of the language that you are using (see SQL-Python Data Type Mappings). For RETURNS TABLE ( [ col_name col_data_type [ , ... ] ] ), if you know the Snowflake data types of the columns in the returned table, specify the column names and types. Otherwise (e.g. if you are determining the column types during run time), you can omit the column names and types (i.e. TABLE ()).
            runtime_version: '(String) The language runtime version to use. Currently, the supported versions are: 3.9, 3.10, and 3.11.'
            schema: '(String) The schema in which to create the procedure. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            secret_id: (String) Fully qualified name of the allowed secret. You will receive an error if you specify a SECRETS value whose secret isn’t also included in an integration specified by the EXTERNAL_ACCESS_INTEGRATIONS parameter.
            secret_variable_name: (String) The variable that will be used in handler code when retrieving information from the secret.
            secrets: (Block Set) Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from secrets in handler code. Secrets you specify here must be allowed by the external access integration specified as a value of this CREATE FUNCTION command’s EXTERNAL_ACCESS_INTEGRATIONS parameter. (see below for nested schema)
            show_output: (List of Object) Outputs the result of SHOW PROCEDURE for the given procedure. (see below for nested schema)
            snowpark_package: (String) The Snowpark package is required for stored procedures, so it must always be present. For more information about Snowpark, see Snowpark API.
            stage_location: (String) Stage location without leading @. To use your user's stage set this to ~, otherwise pass fully qualified name of the stage (with every part contained in double quotes or use snowflake_stage.<your stage's resource name>.fully_qualified_name if you manage this stage through terraform).
            timeouts: (Block, Optional) (see below for nested schema)
            trace_level: (String) Trace level value to use when generating/filtering trace events For more information, check TRACE_LEVEL docs.
            update: (String)
            valid_for_clustering: (Boolean)
            value: (String)
        importStatements:
            - terraform import snowflake_procedure_python.example '"<database_name>"."<schema_name>"."<function_name>"(varchar, varchar, varchar)'
    snowflake_procedure_scala:
        subCategory: Preview
        description: Resource used to manage scala procedure objects. For more information, check procedure documentation https://docs.snowflake.com/en/sql-reference/sql/create-procedure.
        name: snowflake_procedure_scala
        title: snowflake_procedure_scala Resource - terraform-provider-snowflake
        examples:
            - name: w
              manifest: |-
                {
                  "arguments": [
                    {
                      "arg_data_type": "VARCHAR(100)",
                      "arg_name": "x"
                    }
                  ],
                  "database": "Database",
                  "handler": "TestFunc.echoVarchar",
                  "name": "Name",
                  "procedure_definition": "  import com.snowflake.snowpark_java.Session\n  class TestFunc {\n    def echoVarchar(session : Session, x : String): String = {\n      return x\n    }\n  }\n",
                  "return_type": "VARCHAR(100)",
                  "runtime_version": "2.12",
                  "schema": "Schema",
                  "snowpark_package": "1.14.0"
                }
        argumentDocs:
            arg_data_type: (String) The argument type.
            arg_default_value: (String) Optional default value for the argument. For text values use single quotes. Numeric values can be unquoted. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            arg_name: (String) The argument name. The provider wraps it in double quotes by default, so be aware of that while referencing the argument in the procedure definition.
            arguments: (Block List) List of the arguments for the procedure. Consult the docs for more details. (see below for nested schema)
            arguments_raw: (String)
            catalog_name: (String)
            comment: '(String) (Default: user-defined procedure) Specifies a comment for the procedure.'
            create: (String)
            created_on: (String)
            database: '(String) The database in which to create the procedure. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            default: (String)
            delete: (String)
            description: (String)
            enable_console_output: (Boolean) Enable stdout/stderr fast path logging for anonyous stored procs. This is a public parameter (similar to LOG_LEVEL). For more information, check ENABLE_CONSOLE_OUTPUT docs.
            execute_as: '(String) Specifies whether the stored procedure executes with the privileges of the owner (an “owner’s rights” stored procedure) or with the privileges of the caller (a “caller’s rights” stored procedure). If you execute the statement CREATE PROCEDURE … EXECUTE AS CALLER, then in the future the procedure will execute as a caller’s rights procedure. If you execute CREATE PROCEDURE … EXECUTE AS OWNER, then the procedure will execute as an owner’s rights procedure. For more information, see Understanding caller’s rights and owner’s rights stored procedures. Valid values are (case-insensitive): CALLER | OWNER.'
            external_access_integrations: (Set of String) The names of external access integrations needed in order for this procedure’s handler code to access external networks. An external access integration specifies network rules and secrets that specify external locations and credentials (if any) allowed for use by handler code when making requests of an external network, such as an external REST API.
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            handler: '(String) Use the fully qualified name of the method or function for the stored procedure. This is typically in the following form: com.my_company.my_package.MyClass.myMethod where com.my_company.my_package corresponds to the package containing the object or class: package com.my_company.my_package;.'
            id: (String) The ID of this resource.
            imports: (Block Set) The location (stage), path, and name of the file(s) to import. You must set the IMPORTS clause to include any files that your stored procedure depends on. If you are writing an in-line stored procedure, you can omit this clause, unless your code depends on classes defined outside the stored procedure or resource files. If you are writing a stored procedure with a staged handler, you must also include a path to the JAR file containing the stored procedure’s handler code. The IMPORTS definition cannot reference variables from arguments that are passed into the stored procedure. Each file in the IMPORTS clause must have a unique name, even if the files are in different subdirectories or different stages. (see below for nested schema)
            is_aggregate: (Boolean)
            is_ansi: (Boolean)
            is_builtin: (Boolean)
            is_secure: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies that the procedure is secure. For more information about secure procedures, see Protecting Sensitive Information with Secure UDFs and Stored Procedures. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            is_table_function: (Boolean)
            key: (String)
            level: (String)
            log_level: (String) LOG_LEVEL to use when filtering events For more information, check LOG_LEVEL docs.
            max_num_arguments: (Number)
            metric_level: (String) METRIC_LEVEL value to control whether to emit metrics to Event Table For more information, check METRIC_LEVEL docs.
            min_num_arguments: (Number)
            name: '(String) The name of the procedure; the identifier does not need to be unique for the schema in which the procedure is created because stored procedures are identified and resolved by the combination of the name and argument types. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            null_input_behavior: '(String) Specifies the behavior of the procedure when called with null inputs. Valid values are (case-insensitive): CALLED ON NULL INPUT | RETURNS NULL ON NULL INPUT.'
            packages: (Set of String) List of the names of packages deployed in Snowflake that should be included in the handler code’s execution environment. The Snowpark package is required for stored procedures, but is specified in the snowpark_package attribute. For more information about Snowpark, see Snowpark API.
            parameters: (List of Object) Outputs the result of SHOW PARAMETERS IN PROCEDURE for the given procedure. (see below for nested schema)
            path_on_stage: (String) Path for import on stage, without the leading /.
            procedure_definition: (String) Defines the code executed by the stored procedure. The definition can consist of any valid code. Wrapping $$ signs are added by the provider automatically; do not include them. The procedure_definition value must be Scala source code. For more information, see Scala (using Snowpark). To mitigate permadiff on this field, the provider replaces blank characters with a space. This can lead to false positives in cases where a change in case or run of whitespace is semantically significant.
            procedure_language: (String) Specifies language for the procedure. Used to detect external changes.
            read: (String)
            return_type: (String) Specifies the type of the result returned by the stored procedure. For <result_data_type>, use the Snowflake data type that corresponds to the type of the language that you are using (see SQL-Scala Data Type Mappings). For RETURNS TABLE ( [ col_name col_data_type [ , ... ] ] ), if you know the Snowflake data types of the columns in the returned table, specify the column names and types. Otherwise (e.g. if you are determining the column types during run time), you can omit the column names and types (i.e. TABLE ()).
            runtime_version: '(String) The language runtime version to use. Currently, the supported versions are: 2.12.'
            schema: '(String) The schema in which to create the procedure. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            secret_id: (String) Fully qualified name of the allowed secret. You will receive an error if you specify a SECRETS value whose secret isn’t also included in an integration specified by the EXTERNAL_ACCESS_INTEGRATIONS parameter.
            secret_variable_name: (String) The variable that will be used in handler code when retrieving information from the secret.
            secrets: (Block Set) Assigns the names of secrets to variables so that you can use the variables to reference the secrets when retrieving information from secrets in handler code. Secrets you specify here must be allowed by the external access integration specified as a value of this CREATE FUNCTION command’s EXTERNAL_ACCESS_INTEGRATIONS parameter. (see below for nested schema)
            show_output: (List of Object) Outputs the result of SHOW PROCEDURE for the given procedure. (see below for nested schema)
            snowpark_package: (String) The Snowpark package is required for stored procedures, so it must always be present. For more information about Snowpark, see Snowpark API.
            stage_location: (String) Stage location without leading @. To use your user's stage set this to ~, otherwise pass fully qualified name of the stage (with every part contained in double quotes or use snowflake_stage.<your stage's resource name>.fully_qualified_name if you manage this stage through terraform).
            target_path: '(Block Set, Max: 1) Use the fully qualified name of the method or function for the stored procedure. This is typically in the following form: com.my_company.my_package.MyClass.myMethod where com.my_company.my_package corresponds to the package containing the object or class: package com.my_company.my_package;. (see below for nested schema)'
            timeouts: (Block, Optional) (see below for nested schema)
            trace_level: (String) Trace level value to use when generating/filtering trace events For more information, check TRACE_LEVEL docs.
            update: (String)
            valid_for_clustering: (Boolean)
            value: (String)
        importStatements:
            - terraform import snowflake_procedure_scala.example '"<database_name>"."<schema_name>"."<function_name>"(varchar, varchar, varchar)'
    snowflake_procedure_sql:
        subCategory: Preview
        description: Resource used to manage sql procedure objects. For more information, check procedure documentation https://docs.snowflake.com/en/sql-reference/sql/create-procedure.
        name: snowflake_procedure_sql
        title: snowflake_procedure_sql Resource - terraform-provider-snowflake
        examples:
            - name: w
              manifest: |-
                {
                  "arguments": [
                    {
                      "arg_data_type": "VARCHAR(100)",
                      "arg_name": "x"
                    }
                  ],
                  "database": "Database",
                  "name": "Name",
                  "procedure_definition": "BEGIN\n  RETURN message;\nEND;\n",
                  "return_type": "VARCHAR(100)",
                  "schema": "Schema"
                }
        argumentDocs:
            arg_data_type: (String) The argument type.
            arg_default_value: (String) Optional default value for the argument. For text values use single quotes. Numeric values can be unquoted. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            arg_name: (String) The argument name. The provider wraps it in double quotes by default, so be aware of that while referencing the argument in the procedure definition.
            arguments: (Block List) List of the arguments for the procedure. Consult the docs for more details. (see below for nested schema)
            arguments_raw: (String)
            catalog_name: (String)
            comment: '(String) (Default: user-defined procedure) Specifies a comment for the procedure.'
            create: (String)
            created_on: (String)
            database: '(String) The database in which to create the procedure. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            default: (String)
            delete: (String)
            description: (String)
            enable_console_output: (Boolean) Enable stdout/stderr fast path logging for anonyous stored procs. This is a public parameter (similar to LOG_LEVEL). For more information, check ENABLE_CONSOLE_OUTPUT docs.
            execute_as: '(String) Specifies whether the stored procedure executes with the privileges of the owner (an “owner’s rights” stored procedure) or with the privileges of the caller (a “caller’s rights” stored procedure). If you execute the statement CREATE PROCEDURE … EXECUTE AS CALLER, then in the future the procedure will execute as a caller’s rights procedure. If you execute CREATE PROCEDURE … EXECUTE AS OWNER, then the procedure will execute as an owner’s rights procedure. For more information, see Understanding caller’s rights and owner’s rights stored procedures. Valid values are (case-insensitive): CALLER | OWNER.'
            external_access_integrations: (String)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            is_aggregate: (Boolean)
            is_ansi: (Boolean)
            is_builtin: (Boolean)
            is_secure: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies that the procedure is secure. For more information about secure procedures, see Protecting Sensitive Information with Secure UDFs and Stored Procedures. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            is_table_function: (Boolean)
            key: (String)
            level: (String)
            log_level: (String) LOG_LEVEL to use when filtering events For more information, check LOG_LEVEL docs.
            max_num_arguments: (Number)
            metric_level: (String) METRIC_LEVEL value to control whether to emit metrics to Event Table For more information, check METRIC_LEVEL docs.
            min_num_arguments: (Number)
            name: '(String) The name of the procedure; the identifier does not need to be unique for the schema in which the procedure is created because stored procedures are identified and resolved by the combination of the name and argument types. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            null_input_behavior: '(String) Specifies the behavior of the procedure when called with null inputs. Valid values are (case-insensitive): CALLED ON NULL INPUT | RETURNS NULL ON NULL INPUT.'
            parameters: (List of Object) Outputs the result of SHOW PARAMETERS IN PROCEDURE for the given procedure. (see below for nested schema)
            procedure_definition: (String) Defines the code executed by the stored procedure. The definition can consist of any valid code. Wrapping $$ signs are added by the provider automatically; do not include them. The procedure_definition value must be SQL source code. For more information, see Snowflake Scripting. To mitigate permadiff on this field, the provider replaces blank characters with a space. This can lead to false positives in cases where a change in case or run of whitespace is semantically significant.
            procedure_language: (String) Specifies language for the procedure. Used to detect external changes.
            read: (String)
            return_type: (String) Specifies the type of the result returned by the stored procedure. For <result_data_type>, use the Snowflake data type that corresponds to the type of the language that you are using (see SQL data type). For RETURNS TABLE ( [ col_name col_data_type [ , ... ] ] ), if you know the Snowflake data types of the columns in the returned table, specify the column names and types. Otherwise (e.g. if you are determining the column types during run time), you can omit the column names and types (i.e. TABLE ()).
            schema: '(String) The schema in which to create the procedure. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            secrets: (String)
            show_output: (List of Object) Outputs the result of SHOW PROCEDURE for the given procedure. (see below for nested schema)
            timeouts: (Block, Optional) (see below for nested schema)
            trace_level: (String) Trace level value to use when generating/filtering trace events For more information, check TRACE_LEVEL docs.
            update: (String)
            valid_for_clustering: (Boolean)
            value: (String)
        importStatements:
            - terraform import snowflake_procedure_sql.example '"<database_name>"."<schema_name>"."<function_name>"(varchar, varchar, varchar)'
    snowflake_resource_monitor:
        subCategory: Stable
        description: Resource used to manage resource monitor objects. For more information, check resource monitor documentation https://docs.snowflake.com/en/user-guide/resource-monitors.
        name: snowflake_resource_monitor
        title: snowflake_resource_monitor Resource - terraform-provider-snowflake
        examples:
            - name: minimal
              manifest: |-
                {
                  "name": "resource-monitor-name"
                }
            - name: minimal_working
              manifest: |-
                {
                  "credit_quota": 100,
                  "name": "resource-monitor-name",
                  "notify_users": [
                    "${snowflake_user.one.fully_qualified_name}",
                    "${snowflake_user.two.fully_qualified_name}"
                  ],
                  "suspend_trigger": 100
                }
            - name: complete
              manifest: |-
                {
                  "credit_quota": 100,
                  "end_timestamp": "2035-12-07 00:00",
                  "frequency": "DAILY",
                  "name": "resource-monitor-name",
                  "notify_triggers": [
                    40,
                    50
                  ],
                  "notify_users": [
                    "${snowflake_user.one.fully_qualified_name}",
                    "${snowflake_user.two.fully_qualified_name}"
                  ],
                  "start_timestamp": "2030-12-07 00:00",
                  "suspend_immediate_trigger": 90,
                  "suspend_trigger": 50
                }
        argumentDocs:
            comment: (String)
            create: (String)
            created_on: (String)
            credit_quota: (Number) The number of credits allocated to the resource monitor per frequency interval. When total usage for all warehouses assigned to the monitor reaches this number for the current frequency interval, the resource monitor is considered to be at 100% of quota.
            delete: (String)
            end_time: (String)
            end_timestamp: (String) The date and time when the resource monitor suspends the assigned warehouses.
            frequency: '(String) The frequency interval at which the credit usage resets to 0. Valid values are (case-insensitive): MONTHLY | DAILY | WEEKLY | YEARLY | NEVER. If you set a frequency for a resource monitor, you must also set start_timestamp. If you specify NEVER for the frequency, the credit usage for the warehouse does not reset. After removing this field from the config, the previously set value will be preserved on the Snowflake side, not the default value. That''s due to Snowflake limitation and the lack of unset functionality for this parameter.'
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            level: (String)
            name: '(String) Identifier for the resource monitor; must be unique for your account. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            notify_triggers: (Set of Number) Specifies a list of percentages of the credit quota. After reaching any of the values the users passed in the notify_users field will be notified (to receive the notification they should have notifications enabled). Values over 100 are supported.
            notify_users: (Set of String) Specifies the list of users (their identifiers) to receive email notifications on resource monitors. For more information about this resource, see docs.
            owner: (String)
            read: (String)
            remaining_credits: (Number)
            show_output: (List of Object) Outputs the result of SHOW RESOURCE MONITORS for the given resource monitor. (see below for nested schema)
            start_time: (String)
            start_timestamp: (String) The date and time when the resource monitor starts monitoring credit usage for the assigned warehouses. If you set a start_timestamp for a resource monitor, you must also set frequency.  After removing this field from the config, the previously set value will be preserved on the Snowflake side, not the default value. That's due to Snowflake limitation and the lack of unset functionality for this parameter.
            suspend_at: (Number)
            suspend_immediate_at: (Number)
            suspend_immediate_trigger: (Number) Represents a numeric value specified as a percentage of the credit quota. Values over 100 are supported. After reaching this value, all assigned warehouses immediately cancel any currently running queries or statements. In addition, this action sends a notification to all users who have enabled notifications for themselves.
            suspend_trigger: (Number) Represents a numeric value specified as a percentage of the credit quota. Values over 100 are supported. After reaching this value, all assigned warehouses while allowing currently running queries to complete will be suspended. No new queries can be executed by the warehouses until the credit quota for the resource monitor is increased. In addition, this action sends a notification to all users who have enabled notifications for themselves.
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
            used_credits: (Number)
        importStatements:
            - terraform import snowflake_resource_monitor.example '"<resource_monitor_name>"'
    snowflake_row_access_policy:
        subCategory: Stable
        description: Resource used to manage row access policy objects. For more information, check row access policy documentation https://docs.snowflake.com/en/sql-reference/sql/create-row-access-policy.
        name: snowflake_row_access_policy
        title: snowflake_row_access_policy Resource - terraform-provider-snowflake
        examples:
            - name: example_row_access_policy
              manifest: |-
                {
                  "argument": [
                    {
                      "name": "ARG1",
                      "type": "VARCHAR"
                    },
                    {
                      "name": "ARG2",
                      "type": "NUMBER"
                    },
                    {
                      "name": "ARG3",
                      "type": "TIMESTAMP_NTZ"
                    }
                  ],
                  "body": "case when current_role() in ('ANALYST') then true else false end",
                  "comment": "comment",
                  "database": "EXAMPLE_DB",
                  "name": "EXAMPLE_ROW_ACCESS_POLICY",
                  "schema": "EXAMPLE_SCHEMA"
                }
        argumentDocs:
            argument: '(Block List, Min: 1) List of the arguments for the row access policy. A signature specifies a set of attributes that must be considered to determine whether the row is accessible. The attribute values come from the database object (e.g. table or view) to be protected by the row access policy. If any argument name or type is changed, the resource is recreated. (see below for nested schema)'
            body: (String) Specifies the SQL expression. The expression can be any boolean-valued SQL expression. To mitigate permadiff on this field, the provider replaces blank characters with a space. This can lead to false positives in cases where a change in case or run of whitespace is semantically significant.
            comment: (String) Specifies a comment for the row access policy.
            create: (String)
            created_on: (String)
            database: '(String) The database in which to create the row access policy. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            database_name: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE ROW ACCESS POLICY for the given row access policy. (see below for nested schema)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            kind: (String)
            name: '(String) Specifies the identifier for the row access policy; must be unique for the database and schema in which the row access policy is created. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            options: (String)
            owner: (String)
            owner_role_type: (String)
            read: (String)
            return_type: (String)
            schema: '(String) The schema in which to create the row access policy. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            show_output: (List of Object) Outputs the result of SHOW ROW ACCESS POLICIES for the given row access policy. (see below for nested schema)
            signature: (List of Object) (see below for nested schema)
            timeouts: (Block, Optional) (see below for nested schema)
            type: (String) The argument type. VECTOR data types are not yet supported. For more information about data types, check Snowflake docs.
            update: (String)
        importStatements:
            - terraform import snowflake_row_access_policy.example '"<database_name>"."<schema_name>"."<row_access_policy_name>"'
    snowflake_saml2_integration:
        subCategory: Stable
        description: Resource used to manage SAML2 security integration objects. For more information, check security integrations documentation https://docs.snowflake.com/en/sql-reference/sql/create-security-integration-saml2.
        name: snowflake_saml2_integration
        title: snowflake_saml2_integration Resource - terraform-provider-snowflake
        examples:
            - name: saml_integration
              manifest: |-
                {
                  "name": "saml_integration",
                  "saml2_issuer": "test_issuer",
                  "saml2_provider": "CUSTOM",
                  "saml2_sso_url": "https://example.com",
                  "saml2_x509_cert": "${file(\"cert.pem\")}"
                }
            - name: test
              manifest: |-
                {
                  "allowed_email_patterns": [
                    "^(.+dev)@example.com$"
                  ],
                  "allowed_user_domains": [
                    "example.com"
                  ],
                  "comment": "foo",
                  "enabled": true,
                  "name": "saml_integration",
                  "saml2_enable_sp_initiated": true,
                  "saml2_force_authn": true,
                  "saml2_issuer": "foo",
                  "saml2_post_logout_redirect_url": "https://example.com",
                  "saml2_provider": "CUSTOM",
                  "saml2_requested_nameid_format": "urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified",
                  "saml2_sign_request": true,
                  "saml2_snowflake_acs_url": "example.snowflakecomputing.com/fed/login",
                  "saml2_snowflake_issuer_url": "example.snowflakecomputing.com/fed/login",
                  "saml2_snowflake_x509_cert": "${file(\"snowflake_cert.pem\")}",
                  "saml2_sp_initiated_login_page_label": "foo",
                  "saml2_sso_url": "https://example.com",
                  "saml2_x509_cert": "${file(\"cert.pem\")}"
                }
        argumentDocs:
            allowed_email_patterns: (Set of String) A list of regular expressions that email addresses are matched against to authenticate with a SAML2 security integration. If this field changes value from non-empty to empty, the whole resource is recreated because of Snowflake limitations.
            allowed_user_domains: (Set of String) A list of email domains that can authenticate with a SAML2 security integration. If this field changes value from non-empty to empty, the whole resource is recreated because of Snowflake limitations.
            category: (String)
            comment: (String) Specifies a comment for the integration.
            create: (String)
            created_on: (String)
            default: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE SECURITY INTEGRATION for the given integration. (see below for nested schema)
            enabled: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies whether this security integration is enabled or disabled. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            integration_type: (String)
            name: '(String) Specifies the name of the SAML2 integration. This name follows the rules for Object Identifiers. The name should be unique among security integrations in your account. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            read: (String)
            saml2_digest_methods_used: (List of Object) (see below for nested schema)
            saml2_enable_sp_initiated: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) The Boolean indicating if the Log In With button will be shown on the login page. TRUE: displays the Log in With button on the login page. FALSE: does not display the Log in With button on the login page. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            saml2_force_authn: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) The Boolean indicating whether users, during the initial authentication flow, are forced to authenticate again to access Snowflake. When set to TRUE, Snowflake sets the ForceAuthn SAML parameter to TRUE in the outgoing request from Snowflake to the identity provider. TRUE: forces users to authenticate again to access Snowflake, even if a valid session with the identity provider exists. FALSE: does not force users to authenticate again to access Snowflake. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            saml2_issuer: (String) The string containing the IdP EntityID / Issuer.
            saml2_post_logout_redirect_url: (String) The endpoint to which Snowflake redirects users after clicking the Log Out button in the classic Snowflake web interface. Snowflake terminates the Snowflake session upon redirecting to the specified endpoint.
            saml2_provider: '(String) The string describing the IdP. Valid options are: OKTA | ADFS | CUSTOM.'
            saml2_requested_nameid_format: '(String) The SAML NameID format allows Snowflake to set an expectation of the identifying attribute of the user (i.e. SAML Subject) in the SAML assertion from the IdP to ensure a valid authentication to Snowflake. Valid options are: urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified | urn:oasis:names:tc:SAML:1.1:nameid-format:emailAddress | urn:oasis:names:tc:SAML:1.1:nameid-format:X509SubjectName | urn:oasis:names:tc:SAML:1.1:nameid-format:WindowsDomainQualifiedName | urn:oasis:names:tc:SAML:2.0:nameid-format:kerberos | urn:oasis:names:tc:SAML:2.0:nameid-format:persistent | urn:oasis:names:tc:SAML:2.0:nameid-format:transient.'
            saml2_sign_request: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) The Boolean indicating whether SAML requests are signed. TRUE: allows SAML requests to be signed. FALSE: does not allow SAML requests to be signed. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            saml2_signature_methods_used: (List of Object) (see below for nested schema)
            saml2_snowflake_acs_url: (String) The string containing the Snowflake Assertion Consumer Service URL to which the IdP will send its SAML authentication response back to Snowflake. This property will be set in the SAML authentication request generated by Snowflake when initiating a SAML SSO operation with the IdP. If an incorrect value is specified, Snowflake returns an error message indicating the acceptable values to use. Because Okta does not support underscores in URLs, the underscore in the account name must be converted to a hyphen. See docs.
            saml2_snowflake_issuer_url: (String) The string containing the EntityID / Issuer for the Snowflake service provider. If an incorrect value is specified, Snowflake returns an error message indicating the acceptable values to use. Because Okta does not support underscores in URLs, the underscore in the account name must be converted to a hyphen. See docs.
            saml2_snowflake_metadata: (List of Object) (see below for nested schema)
            saml2_sp_initiated_login_page_label: (String) The string containing the label to display after the Log In With button on the login page. If this field changes value from non-empty to empty, the whole resource is recreated because of Snowflake limitations.
            saml2_sso_url: (String) The string containing the IdP SSO URL, where the user should be redirected by Snowflake (the Service Provider) with a SAML AuthnRequest message.
            saml2_x509_cert: (String, Sensitive) The Base64 encoded IdP signing certificate on a single line without the leading -----BEGIN CERTIFICATE----- and ending -----END CERTIFICATE----- markers.
            show_output: (List of Object) Outputs the result of SHOW SECURITY INTEGRATION for the given integration. (see below for nested schema)
            timeouts: (Block, Optional) (see below for nested schema)
            type: (String)
            update: (String)
            value: (String)
        importStatements:
            - terraform import snowflake_saml2_integration.example '"<integration_name>"'
    snowflake_schema:
        subCategory: Stable
        description: Resource used to manage schema objects. For more information, check schema documentation https://docs.snowflake.com/en/sql-reference/sql/create-schema.
        name: snowflake_schema
        title: snowflake_schema Resource - terraform-provider-snowflake
        examples:
            - name: schema
              manifest: |-
                {
                  "database": "database_name",
                  "name": "schema_name"
                }
            - name: schema
              manifest: |-
                {
                  "catalog": "\u003ccatalog_name\u003e",
                  "comment": "my schema",
                  "data_retention_time_in_days": 1,
                  "database": "database_name",
                  "default_ddl_collation": "en_US",
                  "enable_console_output": false,
                  "external_volume": "\u003cexternal_volume_name\u003e",
                  "is_transient": true,
                  "log_level": "INFO",
                  "max_data_extension_time_in_days": 20,
                  "name": "schema_name",
                  "pipe_execution_paused": false,
                  "quoted_identifiers_ignore_case": false,
                  "replace_invalid_characters": false,
                  "storage_serialization_policy": "COMPATIBLE",
                  "suspend_task_after_num_failures": 10,
                  "task_auto_retry_attempts": 10,
                  "trace_level": "ALWAYS",
                  "user_task_managed_initial_warehouse_size": "LARGE",
                  "user_task_minimum_trigger_interval_in_seconds": 120,
                  "user_task_timeout_ms": 3600000,
                  "with_managed_access": true
                }
        argumentDocs:
            catalog: (String) The database parameter that specifies the default catalog to use for Iceberg tables. For more information, see CATALOG.
            comment: (String) Specifies a comment for the schema.
            create: (String)
            created_on: (String)
            data_retention_time_in_days: (Number) Specifies the number of days for which Time Travel actions (CLONE and UNDROP) can be performed on the database, as well as specifying the default Time Travel retention time for all schemas created in the database. For more details, see Understanding & Using Time Travel.
            database: '(String) The database in which to create the schema. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            database_name: (String)
            default: (String)
            default_ddl_collation: (String) Specifies a default collation specification for all schemas and tables added to the database. It can be overridden on schema or table level. For more information, see collation specification.
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE SCHEMA for the given object. In order to handle this output, one must grant sufficient privileges, e.g. grant_ownership on all objects in the schema. (see below for nested schema)
            description: (String)
            dropped_on: (String)
            enable_console_output: (Boolean) If true, enables stdout/stderr fast path logging for anonymous stored procedures.
            external_volume: (String) The database parameter that specifies the default external volume to use for Iceberg tables. For more information, see EXTERNAL_VOLUME.
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            is_current: (Boolean)
            is_default: (Boolean)
            is_transient: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies the schema as transient. Transient schemas do not have a Fail-safe period so they do not incur additional storage costs once they leave Time Travel; however, this means they are also not protected by Fail-safe in the event of a data loss. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            key: (String)
            kind: (String)
            level: (String)
            log_level: '(String) Specifies the severity level of messages that should be ingested and made available in the active event table. Valid options are: [TRACE DEBUG INFO WARN ERROR FATAL OFF]. Messages at the specified level (and at more severe levels) are ingested. For more information, see LOG_LEVEL.'
            max_data_extension_time_in_days: (Number) Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for tables in the database to prevent streams on the tables from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS.
            name: '(String) Specifies the identifier for the schema; must be unique for the database in which the schema is created. When the name is PUBLIC, during creation the provider checks if this schema has already been created and, in such case, ALTER is used to match the desired state. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            options: (String)
            owner: (String)
            owner_role_type: (String)
            parameters: (List of Object) Outputs the result of SHOW PARAMETERS IN SCHEMA for the given object. (see below for nested schema)
            pipe_execution_paused: (Boolean) Specifies whether to pause a running pipe, primarily in preparation for transferring ownership of the pipe to a different role. For more information, check PIPE_EXECUTION_PAUSED docs.
            quoted_identifiers_ignore_case: (Boolean) If true, the case of quoted identifiers is ignored. For more information, see QUOTED_IDENTIFIERS_IGNORE_CASE.
            read: (String)
            replace_invalid_characters: (Boolean) Specifies whether to replace invalid UTF-8 characters with the Unicode replacement character (�) in query results for an Iceberg table. You can only set this parameter for tables that use an external Iceberg catalog. For more information, see REPLACE_INVALID_CHARACTERS.
            retention_time: (String)
            show_output: (List of Object) Outputs the result of SHOW SCHEMA for the given object. (see below for nested schema)
            storage_serialization_policy: '(String) The storage serialization policy for Iceberg tables that use Snowflake as the catalog. Valid options are: [COMPATIBLE OPTIMIZED]. COMPATIBLE: Snowflake performs encoding and compression of data files that ensures interoperability with third-party compute engines. OPTIMIZED: Snowflake performs encoding and compression of data files that ensures the best table performance within Snowflake. For more information, see STORAGE_SERIALIZATION_POLICY.'
            suspend_task_after_num_failures: (Number) How many times a task must fail in a row before it is automatically suspended. 0 disables auto-suspending. For more information, see SUSPEND_TASK_AFTER_NUM_FAILURES.
            task_auto_retry_attempts: (Number) Maximum automatic retries allowed for a user task. For more information, see TASK_AUTO_RETRY_ATTEMPTS.
            timeouts: (Block, Optional) (see below for nested schema)
            trace_level: '(String) Controls how trace events are ingested into the event table. Valid options are: ALWAYS | ON_EVENT | PROPAGATE | OFF. For information about levels, see TRACE_LEVEL.'
            update: (String)
            user_task_managed_initial_warehouse_size: (String) The initial size of warehouse to use for managed warehouses in the absence of history. For more information, see USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE.
            user_task_minimum_trigger_interval_in_seconds: (Number) Minimum amount of time between Triggered Task executions in seconds.
            user_task_timeout_ms: (Number) User task execution timeout in milliseconds. For more information, see USER_TASK_TIMEOUT_MS.
            value: (String)
            with_managed_access: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies a managed schema. Managed access schemas centralize privilege management with the schema owner. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
        importStatements:
            - terraform import snowflake_schema.example '"<database_name>"."<schema_name>"'
    snowflake_scim_integration:
        subCategory: Stable
        description: Resource used to manage scim security integration objects. For more information, check security integrations documentation https://docs.snowflake.com/en/sql-reference/sql/create-security-integration-scim.
        name: snowflake_scim_integration
        title: snowflake_scim_integration Resource - terraform-provider-snowflake
        examples:
            - name: test
              manifest: |-
                {
                  "enabled": true,
                  "name": "test",
                  "run_as_role": "GENERIC_SCIM_PROVISIONER",
                  "scim_client": "GENERIC",
                  "sync_password": true
                }
            - name: test
              manifest: |-
                {
                  "comment": "foo",
                  "enabled": true,
                  "name": "test",
                  "network_policy": "${snowflake_network_policy.example.fully_qualified_name}",
                  "run_as_role": "GENERIC_SCIM_PROVISIONER",
                  "scim_client": "GENERIC",
                  "sync_password": true
                }
              references:
                network_policy: snowflake_network_policy.example.fully_qualified_name
        argumentDocs:
            category: (String)
            comment: (String) Specifies a comment for the integration.
            create: (String)
            created_on: (String)
            default: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE SECURITY INTEGRATIONS for the given security integration. (see below for nested schema)
            enabled: (Boolean) Specify whether the security integration is enabled.
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            integration_type: (String)
            name: '(String) String that specifies the identifier (i.e. name) for the integration; must be unique in your account. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            network_policy: (String) Specifies an existing network policy that controls SCIM network traffic. For more information about this resource, see docs.
            read: (String)
            run_as_role: '(String) Specify the SCIM role in Snowflake that owns any users and roles that are imported from the identity provider into Snowflake using SCIM. Provider assumes that the specified role is already provided. Valid options are: OKTA_PROVISIONER | AAD_PROVISIONER | GENERIC_SCIM_PROVISIONER.'
            scim_client: '(String) Specifies the client type for the scim integration. Valid options are: OKTA | AZURE | GENERIC.'
            show_output: (List of Object) Outputs the result of SHOW SECURITY INTEGRATIONS for the given security integration. (see below for nested schema)
            sync_password: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies whether to enable or disable the synchronization of a user password from an Okta SCIM client as part of the API request to Snowflake. This property is not supported for Azure SCIM. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            timeouts: (Block, Optional) (see below for nested schema)
            type: (String)
            update: (String)
            value: (String)
        importStatements:
            - terraform import snowflake_scim_integration.example '"<integration_name>"'
    snowflake_secondary_connection:
        subCategory: Stable
        description: Resource used to manage secondary (replicated) connections. To manage primary connection check resource snowflake_primary_connection ./primary_connection. For more information, check connection documentation https://docs.snowflake.com/en/sql-reference/sql/create-connection.html.
        name: snowflake_secondary_connection
        title: snowflake_secondary_connection Resource - terraform-provider-snowflake
        examples:
            - name: basic
              manifest: |-
                {
                  "as_replica_of": "\"\u003corganization_name\u003e\".\"\u003caccount_name\u003e\".\"\u003cconnection_name\u003e\"",
                  "name": "connection_name"
                }
            - name: complete
              manifest: |-
                {
                  "as_replica_of": "\"\u003corganization_name\u003e\".\"\u003caccount_name\u003e\".\"\u003cconnection_name\u003e\"",
                  "comment": "my complete secondary connection",
                  "name": "connection_name"
                }
        argumentDocs:
            account_locator: (String)
            account_name: (String)
            as_replica_of: (String) Specifies the identifier for a primary connection from which to create a replica (i.e. a secondary connection). For more information about this resource, see docs.
            comment: (String) Specifies a comment for the secondary connection.
            connection_url: (String)
            create: (String)
            created_on: (String)
            delete: (String)
            failover_allowed_to_accounts: (List of String)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            is_primary: (Boolean) Indicates if the connection primary status has been changed. If change is detected, resource will be recreated.
            name: '(String) String that specifies the identifier (i.e. name) for the connection. Must start with an alphabetic character and may only contain letters, decimal digits (0-9), and underscores (_). For a secondary connection, the name must match the name of its primary connection. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            organization_name: (String)
            primary: (String)
            read: (String)
            region_group: (String)
            show_output: (List of Object) Outputs the result of SHOW CONNECTIONS for the given connection. (see below for nested schema)
            snowflake_region: (String)
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
        importStatements:
            - terraform import snowflake_secondary_connection.example '"<secondary_connection_name>"'
    snowflake_secondary_database:
        subCategory: Stable
        description: A secondary database creates a replica of an existing primary database (i.e. a secondary database). For more information about database replication, see Introduction to database replication across multiple accounts https://docs.snowflake.com/en/user-guide/db-replication-intro.
        name: snowflake_secondary_database
        title: snowflake_secondary_database Resource - terraform-provider-snowflake
        examples:
            - name: test
              manifest: |-
                {
                  "as_replica_of": "\u003cprimary_account_organization_name\u003e.\u003cprimary_account_name\u003e.${snowflake_database.primary.name}",
                  "name": "${snowflake_database.primary.name}",
                  "provider": "${secondary_account}"
                }
              references:
                name: snowflake_database.primary.name
                provider: secondary_account
              dependencies:
                snowflake_database.primary: |-
                    {
                      "name": "database_name",
                      "provider": "${primary_account}",
                      "replication": [
                        {
                          "enable_to_account": [
                            {
                              "account_identifier": "\u003csecondary_account_organization_name\u003e.\u003csecondary_account_name\u003e",
                              "with_failover": true
                            }
                          ],
                          "ignore_edition_check": true
                        }
                      ]
                    }
                snowflake_database.tasks: |-
                    {
                      "name": "database_for_tasks"
                    }
                snowflake_schema.tasks: |-
                    {
                      "database": "${snowflake_database.tasks.name}",
                      "name": "schema_for_tasks"
                    }
                snowflake_task.refresh_secondary_database: |-
                    {
                      "database": "${snowflake_database.tasks.name}",
                      "name": "refresh_secondary_database",
                      "schedule": "10 minute",
                      "schema": "${snowflake_schema.tasks.name}",
                      "sql_statement": "ALTER DATABASE ${snowflake_secondary_database.test.name} REFRESH"
                    }
            - name: test
              manifest: |-
                {
                  "as_replica_of": "\u003cprimary_account_organization_name\u003e.\u003cprimary_account_name\u003e.${snowflake_database.primary.name}",
                  "catalog": "\u003ccatalog_name\u003e",
                  "comment": "A secondary database",
                  "data_retention_time_in_days": 10,
                  "default_ddl_collation": "en_US",
                  "enable_console_output": false,
                  "external_volume": "\u003cexternal_volume_name\u003e",
                  "is_transient": false,
                  "log_level": "INFO",
                  "max_data_extension_time_in_days": 20,
                  "name": "${snowflake_database.primary.name}",
                  "provider": "${secondary_account}",
                  "quoted_identifiers_ignore_case": false,
                  "replace_invalid_characters": false,
                  "storage_serialization_policy": "COMPATIBLE",
                  "suspend_task_after_num_failures": 10,
                  "task_auto_retry_attempts": 10,
                  "trace_level": "ALWAYS",
                  "user_task_managed_initial_warehouse_size": "LARGE",
                  "user_task_minimum_trigger_interval_in_seconds": 120,
                  "user_task_timeout_ms": 3600000
                }
              references:
                name: snowflake_database.primary.name
                provider: secondary_account
              dependencies:
                snowflake_database.primary: |-
                    {
                      "name": "database_name",
                      "provider": "${primary_account}",
                      "replication": [
                        {
                          "enable_to_account": [
                            {
                              "account_identifier": "\u003csecondary_account_organization_name\u003e.\u003csecondary_account_name\u003e",
                              "with_failover": true
                            }
                          ],
                          "ignore_edition_check": true
                        }
                      ]
                    }
                snowflake_database.tasks: |-
                    {
                      "name": "database_for_tasks"
                    }
                snowflake_schema.tasks: |-
                    {
                      "database": "${snowflake_database.tasks.name}",
                      "name": "schema_for_tasks"
                    }
                snowflake_task.refresh_secondary_database: |-
                    {
                      "database": "${snowflake_database.tasks.name}",
                      "name": "refresh_secondary_database",
                      "schedule": "10 minute",
                      "schema": "${snowflake_schema.tasks.name}",
                      "sql_statement": "ALTER DATABASE ${snowflake_secondary_database.test.name} REFRESH"
                    }
        argumentDocs:
            as_replica_of: (String) A fully qualified path to a database to create a replica from. A fully qualified path follows the format of "<organization_name>"."<account_name>"."<database_name>". For more information about this resource, see docs.
            catalog: (String) The database parameter that specifies the default catalog to use for Iceberg tables. For more information, see CATALOG.
            comment: (String) Specifies a comment for the database.
            create: (String)
            data_retention_time_in_days: (Number) Specifies the number of days for which Time Travel actions (CLONE and UNDROP) can be performed on the database, as well as specifying the default Time Travel retention time for all schemas created in the database. For more details, see Understanding & Using Time Travel.
            default_ddl_collation: (String) Specifies a default collation specification for all schemas and tables added to the database. It can be overridden on schema or table level. For more information, see collation specification.
            delete: (String)
            enable_console_output: (Boolean) If true, enables stdout/stderr fast path logging for anonymous stored procedures.
            external_volume: (String) The database parameter that specifies the default external volume to use for Iceberg tables. For more information, see EXTERNAL_VOLUME.
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            is_transient: (Boolean) Specifies the database as transient. Transient databases do not have a Fail-safe period so they do not incur additional storage costs once they leave Time Travel; however, this means they are also not protected by Fail-safe in the event of a data loss.
            log_level: '(String) Specifies the severity level of messages that should be ingested and made available in the active event table. Valid options are: [TRACE DEBUG INFO WARN ERROR FATAL OFF]. Messages at the specified level (and at more severe levels) are ingested. For more information, see LOG_LEVEL.'
            max_data_extension_time_in_days: (Number) Object parameter that specifies the maximum number of days for which Snowflake can extend the data retention period for tables in the database to prevent streams on the tables from becoming stale. For a detailed description of this parameter, see MAX_DATA_EXTENSION_TIME_IN_DAYS.
            name: '(String) Specifies the identifier for the database; must be unique for your account. As a best practice for Database Replication and Failover, it is recommended to give each secondary database the same name as its primary database. This practice supports referencing fully-qualified objects (i.e. ''..'') by other objects in the same database, such as querying a fully-qualified table name in a view. If a secondary database has a different name from the primary database, then these object references would break in the secondary database. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            quoted_identifiers_ignore_case: (Boolean) If true, the case of quoted identifiers is ignored. For more information, see QUOTED_IDENTIFIERS_IGNORE_CASE.
            read: (String)
            replace_invalid_characters: (Boolean) Specifies whether to replace invalid UTF-8 characters with the Unicode replacement character (�) in query results for an Iceberg table. You can only set this parameter for tables that use an external Iceberg catalog. For more information, see REPLACE_INVALID_CHARACTERS.
            storage_serialization_policy: '(String) The storage serialization policy for Iceberg tables that use Snowflake as the catalog. Valid options are: [COMPATIBLE OPTIMIZED]. COMPATIBLE: Snowflake performs encoding and compression of data files that ensures interoperability with third-party compute engines. OPTIMIZED: Snowflake performs encoding and compression of data files that ensures the best table performance within Snowflake. For more information, see STORAGE_SERIALIZATION_POLICY.'
            suspend_task_after_num_failures: (Number) How many times a task must fail in a row before it is automatically suspended. 0 disables auto-suspending. For more information, see SUSPEND_TASK_AFTER_NUM_FAILURES.
            task_auto_retry_attempts: (Number) Maximum automatic retries allowed for a user task. For more information, see TASK_AUTO_RETRY_ATTEMPTS.
            timeouts: (Block, Optional) (see below for nested schema)
            trace_level: '(String) Controls how trace events are ingested into the event table. Valid options are: ALWAYS | ON_EVENT | PROPAGATE | OFF. For information about levels, see TRACE_LEVEL.'
            update: (String)
            user_task_managed_initial_warehouse_size: (String) The initial size of warehouse to use for managed warehouses in the absence of history. For more information, see USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE.
            user_task_minimum_trigger_interval_in_seconds: (Number) Minimum amount of time between Triggered Task executions in seconds.
            user_task_timeout_ms: (Number) User task execution timeout in milliseconds. For more information, see USER_TASK_TIMEOUT_MS.
        importStatements:
            - terraform import snowflake_secondary_database.example '"<secondary_database_name>"'
    snowflake_secret_with_authorization_code_grant:
        subCategory: Stable
        description: Resource used to manage secret objects with OAuth Authorization Code Grant. For more information, check secret documentation https://docs.snowflake.com/en/sql-reference/sql/create-secret.
        name: snowflake_secret_with_authorization_code_grant
        title: snowflake_secret_with_authorization_code_grant Resource - terraform-provider-snowflake
        examples:
            - name: test
              manifest: |-
                {
                  "api_authentication": "${snowflake_api_authentication_integration_with_authorization_code_grant.example.fully_qualified_name}",
                  "database": "EXAMPLE_DB",
                  "name": "EXAMPLE_SECRET",
                  "oauth_refresh_token": "${var.oauth_refresh_token}",
                  "oauth_refresh_token_expiry_time": "2025-01-02 15:04:01",
                  "schema": "EXAMPLE_SCHEMA"
                }
              references:
                api_authentication: snowflake_api_authentication_integration_with_authorization_code_grant.example.fully_qualified_name
                oauth_refresh_token: var.oauth_refresh_token
            - name: test
              manifest: |-
                {
                  "api_authentication": "${snowflake_api_authentication_integration_with_authorization_code_grant.example.fully_qualified_name}",
                  "comment": "EXAMPLE_COMMENT",
                  "database": "EXAMPLE_DB",
                  "name": "EXAMPLE_SECRET",
                  "oauth_refresh_token": "${var.oauth_refresh_token}",
                  "oauth_refresh_token_expiry_time": "2025-01-02 15:04:01",
                  "schema": "EXAMPLE_SCHEMA"
                }
              references:
                api_authentication: snowflake_api_authentication_integration_with_authorization_code_grant.example.fully_qualified_name
                oauth_refresh_token: var.oauth_refresh_token
        argumentDocs:
            api_authentication: (String) Specifies the name value of the Snowflake security integration that connects Snowflake to an external service. For more information about this resource, see docs.
            comment: (String) Specifies a comment for the secret.
            create: (String)
            created_on: (String)
            database: '(String) The database in which to create the secret Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            database_name: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE SECRET for the given secret. (see below for nested schema)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            integration_name: (String)
            name: '(String) String that specifies the identifier (i.e. name) for the secret, must be unique in your schema. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            oauth_access_token_expiry_time: (String)
            oauth_refresh_token: (String, Sensitive) Specifies the token as a string that is used to obtain a new access token from the OAuth authorization server when the access token expires. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            oauth_refresh_token_expiry_time: '(String) Specifies the timestamp as a string when the OAuth refresh token expires. Accepted string formats: YYYY-MM-DD, YYYY-MM-DD HH:MI, YYYY-MM-DD HH:MI:SS, YYYY-MM-DD HH:MI'
            oauth_scopes: (Set of String)
            owner: (String)
            owner_role_type: (String)
            read: (String)
            schema: '(String) The schema in which to create the secret. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            secret_type: (String) Specifies a type for the secret. This field is used for checking external changes and recreating the resources if needed.
            show_output: (List of Object) Outputs the result of SHOW SECRETS for the given secret. (see below for nested schema)
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
            username: (String)
        importStatements:
            - terraform import snowflake_secret_with_authorization_code_grant.example '"<database_name>"."<schema_name>"."<secret_name>"'
    snowflake_secret_with_basic_authentication:
        subCategory: Stable
        description: Resource used to manage secret objects with Basic Authentication. For more information, check secret documentation https://docs.snowflake.com/en/sql-reference/sql/create-secret.
        name: snowflake_secret_with_basic_authentication
        title: snowflake_secret_with_basic_authentication Resource - terraform-provider-snowflake
        examples:
            - name: test
              manifest: |-
                {
                  "database": "EXAMPLE_DB",
                  "name": "EXAMPLE_SECRET",
                  "password": "${var.password}",
                  "schema": "EXAMPLE_SCHEMA",
                  "username": "${var.username}"
                }
              references:
                password: var.password
                username: var.username
            - name: test
              manifest: |-
                {
                  "comment": "EXAMPLE_COMMENT",
                  "database": "EXAMPLE_DB",
                  "name": "EXAMPLE_SECRET",
                  "password": "${var.password}",
                  "schema": "EXAMPLE_SCHEMA",
                  "username": "${var.username}"
                }
              references:
                password: var.password
                username: var.username
        argumentDocs:
            comment: (String) Specifies a comment for the secret.
            create: (String)
            created_on: (String)
            database: '(String) The database in which to create the secret Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            database_name: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE SECRET for the given secret. (see below for nested schema)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            integration_name: (String)
            name: '(String) String that specifies the identifier (i.e. name) for the secret, must be unique in your schema. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            oauth_access_token_expiry_time: (String)
            oauth_refresh_token_expiry_time: (String)
            oauth_scopes: (Set of String)
            owner: (String)
            owner_role_type: (String)
            password: (String, Sensitive) Specifies the password value to store in the secret. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            read: (String)
            schema: '(String) The schema in which to create the secret. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            secret_type: (String) Specifies a type for the secret. This field is used for checking external changes and recreating the resources if needed.
            show_output: (List of Object) Outputs the result of SHOW SECRETS for the given secret. (see below for nested schema)
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
            username: (String, Sensitive) Specifies the username value to store in the secret.
        importStatements:
            - terraform import snowflake_secret_with_basic_authentication.example '"<database_name>"."<schema_name>"."<secret_name>"'
    snowflake_secret_with_client_credentials:
        subCategory: Stable
        description: Resource used to manage secret objects with OAuth Client Credentials. For more information, check secret documentation https://docs.snowflake.com/en/sql-reference/sql/create-secret.
        name: snowflake_secret_with_client_credentials
        title: snowflake_secret_with_client_credentials Resource - terraform-provider-snowflake
        examples:
            - name: test
              manifest: |-
                {
                  "api_authentication": "${snowflake_api_authentication_integration_with_client_credentials.example.fully_qualified_name}",
                  "database": "EXAMPLE_DB",
                  "name": "EXAMPLE_SECRET",
                  "oauth_scopes": [
                    "useraccount",
                    "testscope"
                  ],
                  "schema": "EXAMPLE_SCHEMA"
                }
              references:
                api_authentication: snowflake_api_authentication_integration_with_client_credentials.example.fully_qualified_name
            - name: test
              manifest: |-
                {
                  "api_authentication": "${snowflake_api_authentication_integration_with_client_credentials.example.fully_qualified_name}",
                  "comment": "EXAMPLE_COMMENT",
                  "database": "EXAMPLE_DB",
                  "name": "EXAMPLE_SECRET",
                  "oauth_scopes": [
                    "useraccount",
                    "testscope"
                  ],
                  "schema": "EXAMPLE_SCHEMA"
                }
              references:
                api_authentication: snowflake_api_authentication_integration_with_client_credentials.example.fully_qualified_name
        argumentDocs:
            api_authentication: (String) Specifies the name value of the Snowflake security integration that connects Snowflake to an external service. For more information about this resource, see docs.
            comment: (String) Specifies a comment for the secret.
            create: (String)
            created_on: (String)
            database: '(String) The database in which to create the secret Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            database_name: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE SECRET for the given secret. (see below for nested schema)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            integration_name: (String)
            name: '(String) String that specifies the identifier (i.e. name) for the secret, must be unique in your schema. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            oauth_access_token_expiry_time: (String)
            oauth_refresh_token_expiry_time: (String)
            oauth_scopes: (Set of String) Specifies a list of scopes to use when making a request from the OAuth server by a role with USAGE on the integration during the OAuth client credentials flow.
            owner: (String)
            owner_role_type: (String)
            read: (String)
            schema: '(String) The schema in which to create the secret. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            secret_type: (String) Specifies a type for the secret. This field is used for checking external changes and recreating the resources if needed.
            show_output: (List of Object) Outputs the result of SHOW SECRETS for the given secret. (see below for nested schema)
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
            username: (String)
        importStatements:
            - terraform import snowflake_secret_with_client_credentials.example '"<database_name>"."<schema_name>"."<secret_name>"'
    snowflake_secret_with_generic_string:
        subCategory: Stable
        description: Resource used to manage secret objects with Generic String. For more information, check secret documentation https://docs.snowflake.com/en/sql-reference/sql/create-secret.
        name: snowflake_secret_with_generic_string
        title: snowflake_secret_with_generic_string Resource - terraform-provider-snowflake
        examples:
            - name: test
              manifest: |-
                {
                  "database": "EXAMPLE_DB",
                  "name": "EXAMPLE_SECRET",
                  "schema": "EXAMPLE_SCHEMA",
                  "secret_string": "${var.secret_string}"
                }
              references:
                secret_string: var.secret_string
            - name: test
              manifest: |-
                {
                  "comment": "EXAMPLE_COMMENT",
                  "database": "EXAMPLE_DB",
                  "name": "EXAMPLE_SECRET",
                  "schema": "EXAMPLE_SCHEMA",
                  "secret_string": "${var.secret_string}"
                }
              references:
                secret_string: var.secret_string
        argumentDocs:
            comment: (String) Specifies a comment for the secret.
            create: (String)
            created_on: (String)
            database: '(String) The database in which to create the secret Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            database_name: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE SECRET for the given secret. (see below for nested schema)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            integration_name: (String)
            name: '(String) String that specifies the identifier (i.e. name) for the secret, must be unique in your schema. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            oauth_access_token_expiry_time: (String)
            oauth_refresh_token_expiry_time: (String)
            oauth_scopes: (Set of String)
            owner: (String)
            owner_role_type: (String)
            read: (String)
            schema: '(String) The schema in which to create the secret. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            secret_string: (String, Sensitive) Specifies the string to store in the secret. The string can be an API token or a string of sensitive value that can be used in the handler code of a UDF or stored procedure. For details, see Creating and using an external access integration. You should not use this property to store any kind of OAuth token; use one of the other secret types for your OAuth use cases. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            secret_type: (String) Specifies a type for the secret. This field is used for checking external changes and recreating the resources if needed.
            show_output: (List of Object) Outputs the result of SHOW SECRETS for the given secret. (see below for nested schema)
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
            username: (String)
        importStatements:
            - terraform import snowflake_secret_with_generic_string.example '"<database_name>"."<schema_name>"."<secret_name>"'
    snowflake_sequence:
        subCategory: Preview
        name: snowflake_sequence
        title: snowflake_sequence Resource - terraform-provider-snowflake
        examples:
            - name: test_sequence
              manifest: |-
                {
                  "database": "${snowflake_database.test.name}",
                  "name": "thing_counter",
                  "schema": "${snowflake_schema.test_schema.name}"
                }
              references:
                database: snowflake_database.test.name
                schema: snowflake_schema.test_schema.name
              dependencies:
                snowflake_database.test: |-
                    {
                      "name": "things"
                    }
                snowflake_schema.test_schema: |-
                    {
                      "database": "${snowflake_database.test.name}",
                      "name": "things"
                    }
        argumentDocs:
            comment: '(String) (Default: ``) Specifies a comment for the sequence.'
            create: (String)
            database: (String) The database in which to create the sequence. Don't use the | character.
            delete: (String)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            increment: '(Number) (Default: 1) The amount the sequence will increase by each time it is used'
            name: (String) Specifies the name for the sequence.
            next_value: (Number) The increment sequence interval.
            ordering: '(String) (Default: ORDER) The ordering of the sequence. Either ORDER or NOORDER. Default is ORDER.'
            read: (String)
            schema: (String) The schema in which to create the sequence. Don't use the | character.
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
        importStatements:
            - |-
              # format is database name | schema name | sequence name
              terraform import snowflake_sequence.example 'dbName|schemaName|sequenceName'
    snowflake_service:
        subCategory: Preview
        description: Resource used to manage services. For more information, check services documentation https://docs.snowflake.com/en/sql-reference/sql/create-service. A long-running service is like a web service that does not end automatically. After you create a service, Snowflake manages the running service. For example, if a service container stops, for whatever reason, Snowflake restarts that container so the service runs uninterrupted. See Working with services https://docs.snowflake.com/en/developer-guide/snowpark-container-services/working-with-services developer guide for more details.
        name: snowflake_service
        title: snowflake_service Resource - terraform-provider-snowflake
        examples:
            - name: basic
              manifest: |-
                {
                  "database": "${snowflake_database.test.name}",
                  "from_specification": [
                    {
                      "file": "spec.yaml",
                      "stage": "${snowflake_stage.basic.fully_qualified_name}"
                    }
                  ],
                  "in_compute_pool": "${snowflake_compute_pool.test.name}",
                  "name": "SERVICE",
                  "schema": "${snowflake_schema.test.name}"
                }
              references:
                database: snowflake_database.test.name
                from_specification.stage: snowflake_stage.basic.fully_qualified_name
                in_compute_pool: snowflake_compute_pool.test.name
                schema: snowflake_schema.test.name
              dependencies:
                snowflake_compute_pool.complete: |-
                    {
                      "auto_resume": true,
                      "auto_suspend_secs": 1200,
                      "comment": "A service.",
                      "database": "${snowflake_database.test.name}",
                      "external_access_integrations": [
                        "INTEGRATION"
                      ],
                      "from_specification": [
                        {
                          "file": "spec.yaml",
                          "path": "path/to/spec",
                          "stage": "${snowflake_stage.complete.fully_qualified_name}"
                        }
                      ],
                      "in_compute_pool": "${snowflake_compute_pool.test.name}",
                      "max_instances": 2,
                      "min_instances": 1,
                      "min_ready_instances": 1,
                      "name": "SERVICE",
                      "query_warehouse": "${snowflake_warehouse.test.name}",
                      "schema": "${snowflake_schema.test.name}"
                    }
            - name: basic
              manifest: |-
                {
                  "database": "${snowflake_database.test.name}",
                  "from_specification": [
                    {
                      "text": "spec:\n  containers:\n  - name: example-container\n    image: /database/schema/image_repository/exampleimage:latest\n"
                    }
                  ],
                  "in_compute_pool": "${snowflake_compute_pool.test.name}",
                  "name": "SERVICE",
                  "schema": "${snowflake_schema.test.name}"
                }
              references:
                database: snowflake_database.test.name
                in_compute_pool: snowflake_compute_pool.test.name
                schema: snowflake_schema.test.name
              dependencies:
                snowflake_compute_pool.complete: |-
                    {
                      "auto_resume": true,
                      "auto_suspend_secs": 1200,
                      "comment": "A service.",
                      "database": "${snowflake_database.test.name}",
                      "external_access_integrations": [
                        "INTEGRATION"
                      ],
                      "from_specification": [
                        {
                          "file": "spec.yaml",
                          "path": "path/to/spec",
                          "stage": "${snowflake_stage.complete.fully_qualified_name}"
                        }
                      ],
                      "in_compute_pool": "${snowflake_compute_pool.test.name}",
                      "max_instances": 2,
                      "min_instances": 1,
                      "min_ready_instances": 1,
                      "name": "SERVICE",
                      "query_warehouse": "${snowflake_warehouse.test.name}",
                      "schema": "${snowflake_schema.test.name}"
                    }
            - name: basic
              manifest: |-
                {
                  "database": "${snowflake_database.test.name}",
                  "from_specification_template": [
                    {
                      "file": "spec.yaml",
                      "path": "path/to/spec",
                      "stage": "${snowflake_stage.test.fully_qualified_name}",
                      "using": [
                        {
                          "key": "tag",
                          "value": "latest"
                        }
                      ]
                    }
                  ],
                  "in_compute_pool": "${snowflake_compute_pool.test.name}",
                  "name": "SERVICE",
                  "schema": "${snowflake_schema.test.name}"
                }
              references:
                database: snowflake_database.test.name
                from_specification_template.stage: snowflake_stage.test.fully_qualified_name
                in_compute_pool: snowflake_compute_pool.test.name
                schema: snowflake_schema.test.name
              dependencies:
                snowflake_compute_pool.complete: |-
                    {
                      "auto_resume": true,
                      "auto_suspend_secs": 1200,
                      "comment": "A service.",
                      "database": "${snowflake_database.test.name}",
                      "external_access_integrations": [
                        "INTEGRATION"
                      ],
                      "from_specification": [
                        {
                          "file": "spec.yaml",
                          "path": "path/to/spec",
                          "stage": "${snowflake_stage.complete.fully_qualified_name}"
                        }
                      ],
                      "in_compute_pool": "${snowflake_compute_pool.test.name}",
                      "max_instances": 2,
                      "min_instances": 1,
                      "min_ready_instances": 1,
                      "name": "SERVICE",
                      "query_warehouse": "${snowflake_warehouse.test.name}",
                      "schema": "${snowflake_schema.test.name}"
                    }
            - name: basic
              manifest: |-
                {
                  "database": "${snowflake_database.test.name}",
                  "from_specification": [
                    {
                      "text": "spec:\n containers:\n - name: {{ tag }}\n   image: /database/schema/image_repository/exampleimage:latest\n",
                      "using": [
                        {
                          "key": "tag",
                          "value": "latest"
                        }
                      ]
                    }
                  ],
                  "in_compute_pool": "${snowflake_compute_pool.test.name}",
                  "name": "SERVICE",
                  "schema": "${snowflake_schema.test.name}"
                }
              references:
                database: snowflake_database.test.name
                in_compute_pool: snowflake_compute_pool.test.name
                schema: snowflake_schema.test.name
              dependencies:
                snowflake_compute_pool.complete: |-
                    {
                      "auto_resume": true,
                      "auto_suspend_secs": 1200,
                      "comment": "A service.",
                      "database": "${snowflake_database.test.name}",
                      "external_access_integrations": [
                        "INTEGRATION"
                      ],
                      "from_specification": [
                        {
                          "file": "spec.yaml",
                          "path": "path/to/spec",
                          "stage": "${snowflake_stage.complete.fully_qualified_name}"
                        }
                      ],
                      "in_compute_pool": "${snowflake_compute_pool.test.name}",
                      "max_instances": 2,
                      "min_instances": 1,
                      "min_ready_instances": 1,
                      "name": "SERVICE",
                      "query_warehouse": "${snowflake_warehouse.test.name}",
                      "schema": "${snowflake_schema.test.name}"
                    }
        argumentDocs:
            auto_resume: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies whether to automatically resume a service. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            auto_suspend_secs: '(Number) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (-1)) Specifies the number of seconds of inactivity (service is idle) after which Snowflake automatically suspends the service.'
            comment: (String) Specifies a comment for the service.
            compute_pool: '(String) Specifies the name of the compute pool in your account on which to run the service. Identifiers with special or lower-case characters are not supported. This limitation in the provider follows the limitation in Snowflake (see docs). Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            create: (String)
            created_on: (String)
            current_instances: (Number)
            database: '(String) The database in which to create the service. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            database_name: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE SERVICE for the given service. (see below for nested schema)
            dns_name: (String)
            external_access_integrations: (Set of String) Specifies the names of the external access integrations that allow your service to access external sites.
            file: '(String) The file name of the service specification. Example: spec.yaml.'
            from_specification: '(Block List, Max: 1) Specifies the service specification to use for the service. Note that external changes on this field and nested fields are not detected. Use correctly formatted YAML files. Watch out for the space/tabs indentation. See service specification for more information. (see below for nested schema)'
            from_specification_template: '(Block List, Max: 1) Specifies the service specification template to use for the service. Note that external changes on this field and nested fields are not detected. Use correctly formatted YAML files. Watch out for the space/tabs indentation. See service specification for more information. (see below for nested schema)'
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            is_async_job: (Boolean)
            is_job: (Boolean)
            is_upgrading: (Boolean)
            key: (String) The name of the template variable. The provider wraps it in double quotes by default, so be aware of that while referencing the argument in the spec definition.
            managing_object_domain: (String)
            managing_object_name: (String)
            max_instances: (Number) Specifies the maximum number of service instances to run.
            min_instances: (Number) Specifies the minimum number of service instances to run.
            min_ready_instances: (Number) Indicates the minimum service instances that must be ready for Snowflake to consider the service is ready to process requests.
            name: '(String) Specifies the identifier for the service; must be unique for the schema in which the service is created. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            owner: (String)
            owner_role_type: (String)
            path: '(String) The path to the service specification file on the given stage. When the path is specified, the / character is automatically added as a path prefix. Example: path/to/spec.'
            query_warehouse: '(String) Warehouse to use if a service container connects to Snowflake to execute a query but does not explicitly specify a warehouse to use. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            read: (String)
            resumed_on: (String)
            schema: '(String) The schema in which to create the service. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            service_type: (String) Specifies a type for the service. This field is used for checking external changes and recreating the resources if needed.
            show_output: (List of Object) Outputs the result of SHOW SERVICES for the given service. (see below for nested schema)
            spec: (String)
            spec_digest: (String)
            stage: '(String) The fully qualified name of the stage containing the service specification file. At symbol (@) is added automatically. Example: "\"<db_name>\".\"<schema_name>\".\"<stage_name>\"". For more information about this resource, see docs.'
            status: (String)
            suspended_on: (String)
            target_instances: (Number)
            text: (String) The embedded text of the service specification.
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
            updated_on: (String)
            using: '(Block List, Min: 1) List of the specified template variables and the values of those variables. (see below for nested schema)'
            value: (String) The value to assign to the variable in the template. The provider wraps it in $$ by default, so be aware of that while referencing the argument in the spec definition. The value must either be alphanumeric or valid JSON.
        importStatements:
            - terraform import snowflake_service.example '"<database_name>"."<schema_name>"."<service_name>"'
    snowflake_service_user:
        subCategory: Stable
        description: Resource used to manage service user objects. For more information, check user documentation https://docs.snowflake.com/en/sql-reference/commands-user-role#user-management.
        name: snowflake_service_user
        title: snowflake_service_user Resource - terraform-provider-snowflake
        examples:
            - name: minimal
              manifest: |-
                {
                  "name": "Snowflake Service User - minimal"
                }
            - name: service_user
              manifest: |-
                {
                  "comment": "A service user of snowflake.",
                  "days_to_expiry": 8,
                  "default_namespace": "some.namespace",
                  "default_role": "role1",
                  "default_secondary_roles_option": "ALL",
                  "default_warehouse": "warehouse",
                  "disabled": "false",
                  "display_name": "Snowflake Service User",
                  "email": "${var.email}",
                  "login_name": "${var.login_name}",
                  "mins_to_unlock": 9,
                  "name": "Snowflake Service User",
                  "rsa_public_key": "...",
                  "rsa_public_key_2": "..."
                }
              references:
                email: var.email
                login_name: var.login_name
            - name: u
              manifest: |-
                {
                  "abort_detached_query": true,
                  "autocommit": false,
                  "binary_input_format": "UTF8",
                  "binary_output_format": "BASE64",
                  "client_memory_limit": 1024,
                  "client_metadata_request_use_connection_ctx": true,
                  "client_prefetch_threads": 2,
                  "client_result_chunk_size": 48,
                  "client_result_column_case_insensitive": true,
                  "client_session_keep_alive": true,
                  "client_session_keep_alive_heartbeat_frequency": 2400,
                  "client_timestamp_type_mapping": "TIMESTAMP_NTZ",
                  "date_input_format": "YYYY-MM-DD",
                  "date_output_format": "YY-MM-DD",
                  "enable_unload_physical_type_optimization": false,
                  "enable_unredacted_query_syntax_error": true,
                  "error_on_nondeterministic_merge": false,
                  "error_on_nondeterministic_update": true,
                  "geography_output_format": "WKB",
                  "geometry_output_format": "WKB",
                  "jdbc_treat_decimal_as_int": false,
                  "jdbc_treat_timestamp_ntz_as_utc": true,
                  "jdbc_use_session_timezone": false,
                  "json_indent": 4,
                  "lock_timeout": 21222,
                  "log_level": "ERROR",
                  "multi_statement_count": 0,
                  "name": "Snowflake Service User with all parameters",
                  "network_policy": "BVYDGRAT_0D5E3DD1_F644_03DE_318A_1179886518A7",
                  "noorder_sequence_as_default": false,
                  "odbc_treat_decimal_as_int": true,
                  "prevent_unload_to_internal_stages": true,
                  "query_tag": "some_tag",
                  "quoted_identifiers_ignore_case": true,
                  "rows_per_resultset": 2,
                  "s3_stage_vpce_dns_name": "vpce-id.s3.region.vpce.amazonaws.com",
                  "search_path": "$public, $current",
                  "simulated_data_sharing_consumer": "some_consumer",
                  "statement_queued_timeout_in_seconds": 10,
                  "statement_timeout_in_seconds": 10,
                  "strict_json_output": true,
                  "time_input_format": "HH24:MI",
                  "time_output_format": "HH24:MI",
                  "timestamp_day_is_always_24h": true,
                  "timestamp_input_format": "YYYY-MM-DD",
                  "timestamp_ltz_output_format": "YYYY-MM-DD HH24:MI:SS",
                  "timestamp_ntz_output_format": "YYYY-MM-DD HH24:MI:SS",
                  "timestamp_output_format": "YYYY-MM-DD HH24:MI:SS",
                  "timestamp_type_mapping": "TIMESTAMP_LTZ",
                  "timestamp_tz_output_format": "YYYY-MM-DD HH24:MI:SS",
                  "timezone": "Europe/Warsaw",
                  "trace_level": "PROPAGATE",
                  "transaction_abort_on_error": true,
                  "transaction_default_isolation_level": "READ COMMITTED",
                  "two_digit_century_start": 1980,
                  "unsupported_ddl_action": "FAIL",
                  "use_cached_result": false,
                  "week_of_year_policy": 1,
                  "week_start": 1
                }
        argumentDocs:
            abort_detached_query: (Boolean) Specifies the action that Snowflake performs for in-progress queries if connectivity is lost due to abrupt termination of a session (e.g. network outage, browser termination, service interruption). For more information, check ABORT_DETACHED_QUERY docs.
            autocommit: (Boolean) Specifies whether autocommit is enabled for the session. Autocommit determines whether a DML statement, when executed without an active transaction, is automatically committed after the statement successfully completes. For more information, see Transactions. For more information, check AUTOCOMMIT docs.
            binary_input_format: (String) The format of VARCHAR values passed as input to VARCHAR-to-BINARY conversion functions. For more information, see Binary input and output. For more information, check BINARY_INPUT_FORMAT docs.
            binary_output_format: (String) The format for VARCHAR values returned as output by BINARY-to-VARCHAR conversion functions. For more information, see Binary input and output. For more information, check BINARY_OUTPUT_FORMAT docs.
            client_memory_limit: (Number) Parameter that specifies the maximum amount of memory the JDBC driver or ODBC driver should use for the result set from queries (in MB). For more information, check CLIENT_MEMORY_LIMIT docs.
            client_metadata_request_use_connection_ctx: (Boolean) For specific ODBC functions and JDBC methods, this parameter can change the default search scope from all databases/schemas to the current database/schema. The narrower search typically returns fewer rows and executes more quickly. For more information, check CLIENT_METADATA_REQUEST_USE_CONNECTION_CTX docs.
            client_prefetch_threads: (Number) Parameter that specifies the number of threads used by the client to pre-fetch large result sets. The driver will attempt to honor the parameter value, but defines the minimum and maximum values (depending on your system’s resources) to improve performance. For more information, check CLIENT_PREFETCH_THREADS docs.
            client_result_chunk_size: (Number) Parameter that specifies the maximum size of each set (or chunk) of query results to download (in MB). The JDBC driver downloads query results in chunks. For more information, check CLIENT_RESULT_CHUNK_SIZE docs.
            client_result_column_case_insensitive: (Boolean) Parameter that indicates whether to match column name case-insensitively in ResultSet.get* methods in JDBC. For more information, check CLIENT_RESULT_COLUMN_CASE_INSENSITIVE docs.
            client_session_keep_alive: (Boolean) Parameter that indicates whether to force a user to log in again after a period of inactivity in the session. For more information, check CLIENT_SESSION_KEEP_ALIVE docs.
            client_session_keep_alive_heartbeat_frequency: (Number) Number of seconds in-between client attempts to update the token for the session. For more information, check CLIENT_SESSION_KEEP_ALIVE_HEARTBEAT_FREQUENCY docs.
            client_timestamp_type_mapping: (String) Specifies the TIMESTAMP_* variation to use when binding timestamp variables for JDBC or ODBC applications that use the bind API to load data. For more information, check CLIENT_TIMESTAMP_TYPE_MAPPING docs.
            comment: (String) Specifies a comment for the user.
            created_on: (String)
            date_input_format: (String) Specifies the input format for the DATE data type. For more information, see Date and time input and output formats. For more information, check DATE_INPUT_FORMAT docs.
            date_output_format: (String) Specifies the display format for the DATE data type. For more information, see Date and time input and output formats. For more information, check DATE_OUTPUT_FORMAT docs.
            days_to_expiry: (Number) Specifies the number of days after which the user status is set to Expired and the user is no longer allowed to log in. This is useful for defining temporary users (i.e. users who should only have access to Snowflake for a limited time period). In general, you should not set this property for account administrators (i.e. users with the ACCOUNTADMIN role) because Snowflake locks them out when they become Expired. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            default: (String)
            default_namespace: (String) Specifies the namespace (database only or database and schema) that is active by default for the user’s session upon login. Note that the CREATE USER operation does not verify that the namespace exists.
            default_role: (String) Specifies the role that is active by default for the user’s session upon login. Note that specifying a default role for a user does not grant the role to the user. The role must be granted explicitly to the user using the GRANT ROLE command. In addition, the CREATE USER operation does not verify that the role exists. For more information about this resource, see docs.
            default_secondary_roles: (String)
            default_secondary_roles_option: '(String) (Default: DEFAULT) Specifies the secondary roles that are active for the user’s session upon login. Valid values are (case-insensitive): DEFAULT | NONE | ALL. More information can be found in doc.'
            default_warehouse: (String) Specifies the virtual warehouse that is active by default for the user’s session upon login. Note that the CREATE USER operation does not verify that the warehouse exists. For more information about this resource, see docs.
            description: (String)
            disabled: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies whether the user is disabled, which prevents logging in and aborts all the currently-running queries for the user. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            display_name: (String) Name displayed for the user in the Snowflake web interface.
            email: (String, Sensitive) Email address for the user.
            enable_unload_physical_type_optimization: (Boolean) Specifies whether to set the schema for unloaded Parquet files based on the logical column data types (i.e. the types in the unload SQL query or source table) or on the unloaded column values (i.e. the smallest data types and precision that support the values in the output columns of the unload SQL statement or source table). For more information, check ENABLE_UNLOAD_PHYSICAL_TYPE_OPTIMIZATION docs.
            enable_unredacted_query_syntax_error: (Boolean) Controls whether query text is redacted if a SQL query fails due to a syntax or parsing error. If FALSE, the content of a failed query is redacted in the views, pages, and functions that provide a query history. Only users with a role that is granted or inherits the AUDIT privilege can set the ENABLE_UNREDACTED_QUERY_SYNTAX_ERROR parameter. When using the ALTER USER command to set the parameter to TRUE for a particular user, modify the user that you want to see the query text, not the user who executed the query (if those are different users). For more information, check ENABLE_UNREDACTED_QUERY_SYNTAX_ERROR docs.
            error_on_nondeterministic_merge: (Boolean) Specifies whether to return an error when the MERGE command is used to update or delete a target row that joins multiple source rows and the system cannot determine the action to perform on the target row. For more information, check ERROR_ON_NONDETERMINISTIC_MERGE docs.
            error_on_nondeterministic_update: (Boolean) Specifies whether to return an error when the UPDATE command is used to update a target row that joins multiple source rows and the system cannot determine the action to perform on the target row. For more information, check ERROR_ON_NONDETERMINISTIC_UPDATE docs.
            expires_at_time: (String)
            ext_authn_duo: (Boolean)
            ext_authn_uid: (String)
            first_name: (String)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            geography_output_format: (String) Display format for GEOGRAPHY values. For more information, check GEOGRAPHY_OUTPUT_FORMAT docs.
            geometry_output_format: (String) Display format for GEOMETRY values. For more information, check GEOMETRY_OUTPUT_FORMAT docs.
            has_mfa: (Boolean)
            has_password: (Boolean)
            has_rsa_public_key: (Boolean)
            id: (String) The ID of this resource.
            jdbc_treat_decimal_as_int: (Boolean) Specifies how JDBC processes columns that have a scale of zero (0). For more information, check JDBC_TREAT_DECIMAL_AS_INT docs.
            jdbc_treat_timestamp_ntz_as_utc: (Boolean) Specifies how JDBC processes TIMESTAMP_NTZ values. For more information, check JDBC_TREAT_TIMESTAMP_NTZ_AS_UTC docs.
            jdbc_use_session_timezone: (Boolean) Specifies whether the JDBC Driver uses the time zone of the JVM or the time zone of the session (specified by the TIMEZONE parameter) for the getDate(), getTime(), and getTimestamp() methods of the ResultSet class. For more information, check JDBC_USE_SESSION_TIMEZONE docs.
            json_indent: (Number) Specifies the number of blank spaces to indent each new element in JSON output in the session. Also specifies whether to insert newline characters after each element. For more information, check JSON_INDENT docs.
            key: (String)
            last_name: (String)
            last_success_login: (String)
            level: (String)
            lock_timeout: (Number) Number of seconds to wait while trying to lock a resource, before timing out and aborting the statement. For more information, check LOCK_TIMEOUT docs.
            locked_until_time: (String)
            log_level: (String) Specifies the severity level of messages that should be ingested and made available in the active event table. Messages at the specified level (and at more severe levels) are ingested. For more information about log levels, see Setting log level. For more information, check LOG_LEVEL docs.
            login_name: (String, Sensitive) The name users use to log in. If not supplied, snowflake will use name instead. Login names are always case-insensitive.
            mins_to_bypass_mfa: (String)
            mins_to_unlock: '(Number) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (-1)) Specifies the number of minutes until the temporary lock on the user login is cleared. To protect against unauthorized user login, Snowflake places a temporary lock on a user after five consecutive unsuccessful login attempts. When creating a user, this property can be set to prevent them from logging in until the specified amount of time passes. To remove a lock immediately for a user, specify a value of 0 for this parameter. Note because this value changes continuously after setting it, the provider is currently NOT handling the external changes to it. External changes for this field won''t be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".'
            multi_statement_count: (Number) Number of statements to execute when using the multi-statement capability. For more information, check MULTI_STATEMENT_COUNT docs.
            must_change_password: (Boolean)
            name: '(String) Name of the user. Note that if you do not supply login_name this will be used as login_name. Check the docs. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            network_policy: (String) Specifies the network policy to enforce for your account. Network policies enable restricting access to your account based on users’ IP address. For more details, see Controlling network traffic with network policies. Any existing network policy (created using CREATE NETWORK POLICY). For more information, check NETWORK_POLICY docs.
            noorder_sequence_as_default: (Boolean) Specifies whether the ORDER or NOORDER property is set by default when you create a new sequence or add a new table column. The ORDER and NOORDER properties determine whether or not the values are generated for the sequence or auto-incremented column in increasing or decreasing order. For more information, check NOORDER_SEQUENCE_AS_DEFAULT docs.
            odbc_treat_decimal_as_int: (Boolean) Specifies how ODBC processes columns that have a scale of zero (0). For more information, check ODBC_TREAT_DECIMAL_AS_INT docs.
            owner: (String)
            parameters: (List of Object) Outputs the result of SHOW PARAMETERS IN USER for the given user. (see below for nested schema)
            prevent_unload_to_internal_stages: (Boolean) Specifies whether to prevent data unload operations to internal (Snowflake) stages using COPY INTO  statements. For more information, check PREVENT_UNLOAD_TO_INTERNAL_STAGES docs.
            query_tag: (String) Optional string that can be used to tag queries and other SQL statements executed within a session. The tags are displayed in the output of the QUERY_HISTORY, QUERY_HISTORY_BY_* functions. For more information, check QUERY_TAG docs.
            quoted_identifiers_ignore_case: (Boolean) Specifies whether letters in double-quoted object identifiers are stored and resolved as uppercase letters. By default, Snowflake preserves the case of alphabetic characters when storing and resolving double-quoted identifiers (see Identifier resolution). You can use this parameter in situations in which third-party applications always use double quotes around identifiers. For more information, check QUOTED_IDENTIFIERS_IGNORE_CASE docs.
            rows_per_resultset: (Number) Specifies the maximum number of rows returned in a result set. A value of 0 specifies no maximum. For more information, check ROWS_PER_RESULTSET docs.
            rsa_public_key: (String) Specifies the user’s RSA public key; used for key-pair authentication. Must be on 1 line without header and trailer.
            rsa_public_key_2: (String) Specifies the user’s second RSA public key; used to rotate the public and private keys for key-pair authentication based on an expiration schedule set by your organization. Must be on 1 line without header and trailer.
            s3_stage_vpce_dns_name: (String) Specifies the DNS name of an Amazon S3 interface endpoint. Requests sent to the internal stage of an account via AWS PrivateLink for Amazon S3 use this endpoint to connect. For more information, see Accessing Internal stages with dedicated interface endpoints. For more information, check S3_STAGE_VPCE_DNS_NAME docs.
            search_path: (String) Specifies the path to search to resolve unqualified object names in queries. For more information, see Name resolution in queries. Comma-separated list of identifiers. An identifier can be a fully or partially qualified schema name. For more information, check SEARCH_PATH docs.
            show_output: (List of Object) Outputs the result of SHOW USER for the given user. (see below for nested schema)
            simulated_data_sharing_consumer: (String) Specifies the name of a consumer account to simulate for testing/validating shared data, particularly shared secure views. When this parameter is set in a session, shared views return rows as if executed in the specified consumer account rather than the provider account. For more information, see Introduction to Secure Data Sharing and Working with shares. For more information, check SIMULATED_DATA_SHARING_CONSUMER docs.
            snowflake_lock: (Boolean)
            statement_queued_timeout_in_seconds: (Number) Amount of time, in seconds, a SQL statement (query, DDL, DML, etc.) remains queued for a warehouse before it is canceled by the system. This parameter can be used in conjunction with the MAX_CONCURRENCY_LEVEL parameter to ensure a warehouse is never backlogged. For more information, check STATEMENT_QUEUED_TIMEOUT_IN_SECONDS docs.
            statement_timeout_in_seconds: (Number) Amount of time, in seconds, after which a running SQL statement (query, DDL, DML, etc.) is canceled by the system. For more information, check STATEMENT_TIMEOUT_IN_SECONDS docs.
            strict_json_output: (Boolean) This parameter specifies whether JSON output in a session is compatible with the general standard (as described by http://json.org). By design, Snowflake allows JSON input that contains non-standard values; however, these non-standard values might result in Snowflake outputting JSON that is incompatible with other platforms and languages. This parameter, when enabled, ensures that Snowflake outputs valid/compatible JSON. For more information, check STRICT_JSON_OUTPUT docs.
            time_input_format: (String) Specifies the input format for the TIME data type. For more information, see Date and time input and output formats. Any valid, supported time format or AUTO (AUTO specifies that Snowflake attempts to automatically detect the format of times stored in the system during the session). For more information, check TIME_INPUT_FORMAT docs.
            time_output_format: (String) Specifies the display format for the TIME data type. For more information, see Date and time input and output formats. For more information, check TIME_OUTPUT_FORMAT docs.
            timestamp_day_is_always_24h: (Boolean) Specifies whether the DATEADD function (and its aliases) always consider a day to be exactly 24 hours for expressions that span multiple days. For more information, check TIMESTAMP_DAY_IS_ALWAYS_24H docs.
            timestamp_input_format: (String) Specifies the input format for the TIMESTAMP data type alias. For more information, see Date and time input and output formats. Any valid, supported timestamp format or AUTO (AUTO specifies that Snowflake attempts to automatically detect the format of timestamps stored in the system during the session). For more information, check TIMESTAMP_INPUT_FORMAT docs.
            timestamp_ltz_output_format: (String) Specifies the display format for the TIMESTAMP_LTZ data type. If no format is specified, defaults to TIMESTAMP_OUTPUT_FORMAT. For more information, see Date and time input and output formats. For more information, check TIMESTAMP_LTZ_OUTPUT_FORMAT docs.
            timestamp_ntz_output_format: (String) Specifies the display format for the TIMESTAMP_NTZ data type. For more information, check TIMESTAMP_NTZ_OUTPUT_FORMAT docs.
            timestamp_output_format: (String) Specifies the display format for the TIMESTAMP data type alias. For more information, see Date and time input and output formats. For more information, check TIMESTAMP_OUTPUT_FORMAT docs.
            timestamp_type_mapping: (String) Specifies the TIMESTAMP_* variation that the TIMESTAMP data type alias maps to. For more information, check TIMESTAMP_TYPE_MAPPING docs.
            timestamp_tz_output_format: (String) Specifies the display format for the TIMESTAMP_TZ data type. If no format is specified, defaults to TIMESTAMP_OUTPUT_FORMAT. For more information, see Date and time input and output formats. For more information, check TIMESTAMP_TZ_OUTPUT_FORMAT docs.
            timezone: (String) Specifies the time zone for the session. You can specify a time zone name or a link name from release 2021a of the IANA Time Zone Database (e.g. America/Los_Angeles, Europe/London, UTC, Etc/GMT, etc.). For more information, check TIMEZONE docs.
            trace_level: (String) Controls how trace events are ingested into the event table. For more information about trace levels, see Setting trace level. For more information, check TRACE_LEVEL docs.
            transaction_abort_on_error: (Boolean) Specifies the action to perform when a statement issued within a non-autocommit transaction returns with an error. For more information, check TRANSACTION_ABORT_ON_ERROR docs.
            transaction_default_isolation_level: (String) Specifies the isolation level for transactions in the user session. For more information, check TRANSACTION_DEFAULT_ISOLATION_LEVEL docs.
            two_digit_century_start: (Number) Specifies the “century start” year for 2-digit years (i.e. the earliest year such dates can represent). This parameter prevents ambiguous dates when importing or converting data with the YY date format component (i.e. years represented as 2 digits). For more information, check TWO_DIGIT_CENTURY_START docs.
            type: (String)
            unsupported_ddl_action: (String) Determines if an unsupported (i.e. non-default) value specified for a constraint property returns an error. For more information, check UNSUPPORTED_DDL_ACTION docs.
            use_cached_result: (Boolean) Specifies whether to reuse persisted query results, if available, when a matching query is submitted. For more information, check USE_CACHED_RESULT docs.
            user_type: (String) Specifies a type for the user.
            value: (String)
            week_of_year_policy: '(Number) Specifies how the weeks in a given year are computed. 0: The semantics used are equivalent to the ISO semantics, in which a week belongs to a given year if at least 4 days of that week are in that year. 1: January 1 is included in the first week of the year and December 31 is included in the last week of the year. For more information, check WEEK_OF_YEAR_POLICY docs.'
            week_start: '(Number) Specifies the first day of the week (used by week-related date functions). 0: Legacy Snowflake behavior is used (i.e. ISO-like semantics). 1 (Monday) to 7 (Sunday): All the week-related functions use weeks that start on the specified day of the week. For more information, check WEEK_START docs.'
        importStatements:
            - terraform import snowflake_service_user.example '"<user_name>"'
    snowflake_share:
        subCategory: Preview
        name: snowflake_share
        title: snowflake_share Resource - terraform-provider-snowflake
        examples:
            - name: test
              manifest: |-
                {
                  "accounts": [
                    "organizationName.accountName"
                  ],
                  "comment": "cool comment",
                  "name": "share_name"
                }
              dependencies:
                snowflake_database.example: |-
                    {
                      "depends_on": [
                        "${snowflake_share.test}"
                      ],
                      "name": "test"
                    }
        argumentDocs:
            accounts: (List of String) A list of accounts to be added to the share. Values should not be the account locator, but in the form of 'organization_name.account_name
            comment: (String) Specifies a comment for the managed account.
            create: (String)
            delete: (String)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            name: (String) Specifies the identifier for the share; must be unique for the account in which the share is created.
            read: (String)
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
        importStatements:
            - terraform import snowflake_share.example name
    snowflake_shared_database:
        subCategory: Stable
        description: A shared database creates a database from a share provided by another Snowflake account. For more information about shares, see Introduction to Secure Data Sharing https://docs.snowflake.com/en/user-guide/data-sharing-intro.
        name: snowflake_shared_database
        title: snowflake_shared_database Resource - terraform-provider-snowflake
        examples:
            - name: test
              manifest: |-
                {
                  "depends_on": [
                    "${snowflake_grant_privileges_to_share.test}"
                  ],
                  "from_share": "\u003cprimary_account_organization_name\u003e.\u003cprimary_account_name\u003e.${snowflake_share.test.name}",
                  "name": "${snowflake_database.test.name}",
                  "provider": "${secondary_account}"
                }
              references:
                name: snowflake_database.test.name
                provider: secondary_account
              dependencies:
                snowflake_database.test: |-
                    {
                      "name": "shared_database",
                      "provider": "${primary_account}"
                    }
                snowflake_grant_privileges_to_share.test: |-
                    {
                      "on_database": "${snowflake_database.test.name}",
                      "privileges": [
                        "USAGE"
                      ],
                      "provider": "${primary_account}",
                      "to_share": "${snowflake_share.test.name}"
                    }
                snowflake_share.test: |-
                    {
                      "accounts": [
                        "\u003csecondary_account_organization_name\u003e.\u003csecondary_account_name\u003e"
                      ],
                      "name": "share_name",
                      "provider": "${primary_account}"
                    }
            - name: test
              manifest: |-
                {
                  "catalog": "\u003ccatalog_name\u003e",
                  "comment": "A shared database",
                  "default_ddl_collation": "en_US",
                  "depends_on": [
                    "${snowflake_grant_privileges_to_share.test}"
                  ],
                  "enable_console_output": false,
                  "external_volume": "\u003cexternal_volume_name\u003e",
                  "from_share": "\u003cprimary_account_organization_name\u003e.\u003cprimary_account_name\u003e.${snowflake_share.test.name}",
                  "log_level": "INFO",
                  "name": "${snowflake_database.test.name}",
                  "provider": "${secondary_account}",
                  "quoted_identifiers_ignore_case": false,
                  "replace_invalid_characters": false,
                  "storage_serialization_policy": "COMPATIBLE",
                  "suspend_task_after_num_failures": 10,
                  "task_auto_retry_attempts": 10,
                  "trace_level": "ALWAYS",
                  "user_task_managed_initial_warehouse_size": "LARGE",
                  "user_task_minimum_trigger_interval_in_seconds": 120,
                  "user_task_timeout_ms": 3600000
                }
              references:
                name: snowflake_database.test.name
                provider: secondary_account
              dependencies:
                snowflake_database.test: |-
                    {
                      "name": "shared_database",
                      "provider": "${primary_account}"
                    }
                snowflake_grant_privileges_to_share.test: |-
                    {
                      "on_database": "${snowflake_database.test.name}",
                      "privileges": [
                        "USAGE"
                      ],
                      "provider": "${primary_account}",
                      "to_share": "${snowflake_share.test.name}"
                    }
                snowflake_share.test: |-
                    {
                      "accounts": [
                        "\u003csecondary_account_organization_name\u003e.\u003csecondary_account_name\u003e"
                      ],
                      "name": "share_name",
                      "provider": "${primary_account}"
                    }
        argumentDocs:
            catalog: (String) The database parameter that specifies the default catalog to use for Iceberg tables. For more information, see CATALOG.
            comment: (String) Specifies a comment for the database.
            create: (String)
            default_ddl_collation: (String) Specifies a default collation specification for all schemas and tables added to the database. It can be overridden on schema or table level. For more information, see collation specification.
            delete: (String)
            enable_console_output: (Boolean) If true, enables stdout/stderr fast path logging for anonymous stored procedures.
            external_volume: (String) The database parameter that specifies the default external volume to use for Iceberg tables. For more information, see EXTERNAL_VOLUME.
            from_share: (String) A fully qualified path to a share from which the database will be created. A fully qualified path follows the format of "<organization_name>"."<account_name>"."<share_name>". For more information about this resource, see docs.
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            log_level: '(String) Specifies the severity level of messages that should be ingested and made available in the active event table. Valid options are: [TRACE DEBUG INFO WARN ERROR FATAL OFF]. Messages at the specified level (and at more severe levels) are ingested. For more information, see LOG_LEVEL.'
            name: '(String) Specifies the identifier for the database; must be unique for your account. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            quoted_identifiers_ignore_case: (Boolean) If true, the case of quoted identifiers is ignored. For more information, see QUOTED_IDENTIFIERS_IGNORE_CASE.
            read: (String)
            replace_invalid_characters: (Boolean) Specifies whether to replace invalid UTF-8 characters with the Unicode replacement character (�) in query results for an Iceberg table. You can only set this parameter for tables that use an external Iceberg catalog. For more information, see REPLACE_INVALID_CHARACTERS.
            storage_serialization_policy: '(String) The storage serialization policy for Iceberg tables that use Snowflake as the catalog. Valid options are: [COMPATIBLE OPTIMIZED]. COMPATIBLE: Snowflake performs encoding and compression of data files that ensures interoperability with third-party compute engines. OPTIMIZED: Snowflake performs encoding and compression of data files that ensures the best table performance within Snowflake. For more information, see STORAGE_SERIALIZATION_POLICY.'
            suspend_task_after_num_failures: (Number) How many times a task must fail in a row before it is automatically suspended. 0 disables auto-suspending. For more information, see SUSPEND_TASK_AFTER_NUM_FAILURES.
            task_auto_retry_attempts: (Number) Maximum automatic retries allowed for a user task. For more information, see TASK_AUTO_RETRY_ATTEMPTS.
            timeouts: (Block, Optional) (see below for nested schema)
            trace_level: '(String) Controls how trace events are ingested into the event table. Valid options are: ALWAYS | ON_EVENT | PROPAGATE | OFF. For information about levels, see TRACE_LEVEL.'
            update: (String)
            user_task_managed_initial_warehouse_size: (String) The initial size of warehouse to use for managed warehouses in the absence of history. For more information, see USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE.
            user_task_minimum_trigger_interval_in_seconds: (Number) Minimum amount of time between Triggered Task executions in seconds.
            user_task_timeout_ms: (Number) User task execution timeout in milliseconds. For more information, see USER_TASK_TIMEOUT_MS.
        importStatements:
            - terraform import snowflake_shared_database.example '"<shared_database_name>"'
    snowflake_stage:
        subCategory: Preview
        name: snowflake_stage
        title: snowflake_stage Resource - terraform-provider-snowflake
        examples:
            - name: example_stage
              manifest: |-
                {
                  "credentials": "AWS_KEY_ID='${var.example_aws_key_id}' AWS_SECRET_KEY='${var.example_aws_secret_key}'",
                  "database": "EXAMPLE_DB",
                  "name": "EXAMPLE_STAGE",
                  "schema": "EXAMPLE_SCHEMA",
                  "url": "s3://com.example.bucket/prefix"
                }
            - name: example_stage_with_file_format
              manifest: |-
                {
                  "credentials": "AWS_KEY_ID='${var.example_aws_key_id}' AWS_SECRET_KEY='${var.example_aws_secret_key}'",
                  "database": "EXAMPLE_DB",
                  "file_format": "FORMAT_NAME = DB.SCHEMA.FORMATNAME",
                  "name": "EXAMPLE_STAGE",
                  "schema": "EXAMPLE_SCHEMA",
                  "url": "s3://com.example.bucket/prefix"
                }
        argumentDocs:
            aws_external_id: '(String) A unique ID assigned to the specific stage. The ID has the following format: <snowflakeAccount>SFCRole=<snowflakeRoleId><randomId>'
            comment: (String) Specifies a comment for the stage.
            copy_options: (String) Specifies the copy options for the stage.
            create: (String)
            credentials: (String, Sensitive) Specifies the credentials for the stage.
            database: (String) The database in which to create the stage.
            delete: (String)
            directory: (String) Specifies the directory settings for the stage.
            encryption: (String) Specifies the encryption settings for the stage.
            file_format: '(String) Specifies the file format for the stage. Specifying the default Snowflake value (e.g. TYPE = CSV) will currently result in a permadiff (check #2679). For now, omit the default values; it will be fixed in the upcoming provider versions. Examples of usage: 1. with hardcoding value: file_format="FORMAT_NAME = DB.SCHEMA.FORMATNAME" 2. from dynamic value: file_format = "FORMAT_NAME = ${snowflake_file_format.myfileformat.fully_qualified_name}" 3. from expression: file_format = format("FORMAT_NAME =%s.%s.MYFILEFORMAT", var.db_name, each.value.schema_name). Reference: #265'
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            name: (String) Specifies the identifier for the stage; must be unique for the database and schema in which the stage is created.
            read: (String)
            schema: (String) The schema in which to create the stage.
            snowflake_iam_user: (String) An AWS IAM user created for your Snowflake account. This user is the same for every external S3 stage created in your account.
            storage_integration: (String) Specifies the name of the storage integration used to delegate authentication responsibility for external cloud storage to a Snowflake identity and access management (IAM) entity.
            tag: (Block List, Deprecated) Definitions of a tag to associate with the resource. (see below for nested schema)
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
            url: (String) Specifies the URL for the stage.
            value: (String) Tag value, e.g. marketing_info.
        importStatements:
            - |-
              # format is database name | schema name | stage name
              terraform import snowflake_stage.example 'dbName|schemaName|stageName'
    snowflake_storage_integration:
        subCategory: Preview
        name: snowflake_storage_integration
        title: snowflake_storage_integration Resource - terraform-provider-snowflake
        examples:
            - name: integration
              manifest: |-
                {
                  "comment": "A storage integration.",
                  "enabled": true,
                  "name": "storage",
                  "storage_aws_external_id": "ABC12345_DEFRole=2_123ABC459AWQmtAdRqwe/A==",
                  "storage_aws_iam_user_arn": "...",
                  "storage_aws_role_arn": "...",
                  "storage_provider": "S3",
                  "type": "EXTERNAL_STAGE"
                }
        argumentDocs:
            azure_consent_url: (String, Sensitive) The consent URL that is used to create an Azure Snowflake service principle inside your tenant.
            azure_multi_tenant_app_name: (String) This is the name of the Snowflake client application created for your account.
            azure_tenant_id: '(String) (Default: ``)'
            comment: '(String) (Default: ``)'
            create: (String)
            created_on: (String) Date and time when the storage integration was created.
            default: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE STORAGE INTEGRATION for the given storage integration. (see below for nested schema)
            enabled: '(Boolean) (Default: true)'
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            name: (String)
            read: (String)
            storage_allowed_locations: (List of String) Explicitly limits external stages that use the integration to reference one or more storage locations.
            storage_aws_external_id: (String) The external ID that Snowflake will use when assuming the AWS role.
            storage_aws_iam_user_arn: (String) The Snowflake user that will attempt to assume the AWS role.
            storage_aws_object_acl: (String) "bucket-owner-full-control" Enables support for AWS access control lists (ACLs) to grant the bucket owner full control.
            storage_aws_role_arn: '(String) (Default: ``)'
            storage_blocked_locations: (List of String) Explicitly prohibits external stages that use the integration from referencing one or more storage locations.
            storage_gcp_service_account: (String) This is the name of the Snowflake Google Service Account created for your account.
            storage_provider: '(String) Specifies the storage provider for the integration. Valid options are: S3 | S3GOV | S3CHINA | GCS | AZURE'
            timeouts: (Block, Optional) (see below for nested schema)
            type: '(String) (Default: EXTERNAL_STAGE)'
            update: (String)
            use_privatelink_endpoint: (List of Object) (see below for nested schema)
            value: (String)
        importStatements:
            - terraform import snowflake_storage_integration.example name
    snowflake_stream_on_directory_table:
        subCategory: Stable
        description: Resource used to manage streams on directory tables. For more information, check stream documentation https://docs.snowflake.com/en/sql-reference/sql/create-stream.
        name: snowflake_stream_on_directory_table
        title: snowflake_stream_on_directory_table Resource - terraform-provider-snowflake
        examples:
            - name: stream
              manifest: |-
                {
                  "database": "database",
                  "name": "stream",
                  "schema": "schema",
                  "stage": "${snowflake_stage.example.fully_qualified_name}"
                }
              references:
                stage: snowflake_stage.example.fully_qualified_name
            - name: stream
              manifest: |-
                {
                  "comment": "A stream.",
                  "copy_grants": true,
                  "database": "database",
                  "name": "stream",
                  "schema": "schema",
                  "stage": "${snowflake_stage.example.fully_qualified_name}"
                }
              references:
                stage: snowflake_stage.example.fully_qualified_name
        argumentDocs:
            base_tables: (List of String)
            comment: (String) Specifies a comment for the stream.
            copy_grants: '(Boolean) (Default: false) Retains the access permissions from the original stream when a stream is recreated using the OR REPLACE clause. This is used when the provider detects changes for fields that can not be changed by ALTER. This value will not have any effect during creating a new object with Terraform.'
            create: (String)
            created_on: (String)
            database: '(String) The database in which to create the stream. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            database_name: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE STREAM for the given stream. (see below for nested schema)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            invalid_reason: (String)
            mode: (String)
            name: '(String) Specifies the identifier for the stream; must be unique for the database and schema in which the stream is created. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            owner: (String)
            owner_role_type: (String)
            read: (String)
            schema: '(String) The schema in which to create the stream. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            show_output: (List of Object) Outputs the result of SHOW STREAMS for the given stream. (see below for nested schema)
            source_type: (String)
            stage: '(String) Specifies an identifier for the stage the stream will monitor. Due to Snowflake limitations, the provider can not read the stage''s database and schema. For stages, Snowflake returns only partially qualified name instead of fully qualified name. Please use stages located in the same schema as the stream. Due to technical limitations (read more here), avoid using the following characters: |, ., ". For more information about this resource, see docs.'
            stale: (Boolean) Indicated if the stream is stale. When Terraform detects that the stream is stale, the stream is recreated with CREATE OR REPLACE. Read more on stream staleness in Snowflake docs.
            stale_after: (String)
            stream_type: (String) Specifies a type for the stream. This field is used for checking external changes and recreating the resources if needed.
            table_name: (String)
            timeouts: (Block, Optional) (see below for nested schema)
            type: (String)
            update: (String)
        importStatements:
            - terraform import snowflake_stream_on_directory_table.example '"<database_name>"."<schema_name>"."<stream_name>"'
    snowflake_stream_on_external_table:
        subCategory: Stable
        description: Resource used to manage streams on external tables. For more information, check stream documentation https://docs.snowflake.com/en/sql-reference/sql/create-stream.
        name: snowflake_stream_on_external_table
        title: snowflake_stream_on_external_table Resource - terraform-provider-snowflake
        examples:
            - name: stream
              manifest: |-
                {
                  "database": "database",
                  "external_table": "${snowflake_external_table.example.fully_qualified_name}",
                  "name": "stream",
                  "schema": "schema"
                }
              references:
                external_table: snowflake_external_table.example.fully_qualified_name
            - name: stream
              manifest: |-
                {
                  "at": [
                    {
                      "statement": "8e5d0ca9-005e-44e6-b858-a8f5b37c5726"
                    }
                  ],
                  "comment": "A stream.",
                  "copy_grants": true,
                  "database": "database",
                  "external_table": "${snowflake_external_table.example.fully_qualified_name}",
                  "insert_only": "true",
                  "name": "stream",
                  "schema": "schema"
                }
              references:
                external_table: snowflake_external_table.example.fully_qualified_name
        argumentDocs:
            at: '(Block List, Max: 1) This field specifies that the request is inclusive of any changes made by a statement or transaction with a timestamp equal to the specified parameter. Due to Snowflake limitations, the provider does not detect external changes on this field. External changes for this field won''t be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint". (see below for nested schema)'
            base_tables: (List of String)
            before: '(Block List, Max: 1) This field specifies that the request refers to a point immediately preceding the specified parameter. This point in time is just before the statement, identified by its query ID, is completed.  Due to Snowflake limitations, the provider does not detect external changes on this field. External changes for this field won''t be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint". (see below for nested schema)'
            comment: (String) Specifies a comment for the stream.
            copy_grants: '(Boolean) (Default: false) Retains the access permissions from the original stream when a stream is recreated using the OR REPLACE clause. This is used when the provider detects changes for fields that can not be changed by ALTER. This value will not have any effect during creating a new object with Terraform.'
            create: (String)
            created_on: (String)
            database: '(String) The database in which to create the stream. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            database_name: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE STREAM for the given stream. (see below for nested schema)
            external_table: '(String) Specifies an identifier for the external table the stream will monitor. Due to technical limitations (read more here), avoid using the following characters: |, ., ". For more information about this resource, see docs.'
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            insert_only: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies whether this is an insert-only stream. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            invalid_reason: (String)
            mode: (String)
            name: '(String) Specifies the identifier for the stream; must be unique for the database and schema in which the stream is created. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            offset: (String) Specifies the difference in seconds from the current time to use for Time Travel, in the form -N where N can be an integer or arithmetic expression (e.g. -120 is 120 seconds, -30*60 is 1800 seconds or 30 minutes).
            owner: (String)
            owner_role_type: (String)
            read: (String)
            schema: '(String) The schema in which to create the stream. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            show_output: (List of Object) Outputs the result of SHOW STREAMS for the given stream. (see below for nested schema)
            source_type: (String)
            stale: (Boolean) Indicated if the stream is stale. When Terraform detects that the stream is stale, the stream is recreated with CREATE OR REPLACE. Read more on stream staleness in Snowflake docs.
            stale_after: (String)
            statement: '(String) Specifies the query ID of a statement to use as the reference point for Time Travel. This parameter supports any statement of one of the following types: DML (e.g. INSERT, UPDATE, DELETE), TCL (BEGIN, COMMIT transaction), SELECT.'
            stream: (String) Specifies the identifier (i.e. name) for an existing stream on the queried table or view. The current offset in the stream is used as the AT point in time for returning change data for the source object.
            stream_type: (String) Specifies a type for the stream. This field is used for checking external changes and recreating the resources if needed.
            table_name: (String)
            timeouts: (Block, Optional) (see below for nested schema)
            timestamp: (String) Specifies an exact date and time to use for Time Travel. The value must be explicitly cast to a TIMESTAMP, TIMESTAMP_LTZ, TIMESTAMP_NTZ, or TIMESTAMP_TZ data type.
            type: (String)
            update: (String)
        importStatements:
            - terraform import snowflake_stream_on_external_table.example '"<database_name>"."<schema_name>"."<stream_name>"'
    snowflake_stream_on_table:
        subCategory: Stable
        description: Resource used to manage streams on tables. For more information, check stream documentation https://docs.snowflake.com/en/sql-reference/sql/create-stream.
        name: snowflake_stream_on_table
        title: snowflake_stream_on_table Resource - terraform-provider-snowflake
        examples:
            - name: stream
              manifest: |-
                {
                  "database": "database",
                  "name": "stream",
                  "schema": "schema",
                  "table": "${snowflake_table.example.fully_qualified_name}"
                }
              references:
                table: snowflake_table.example.fully_qualified_name
            - name: stream
              manifest: |-
                {
                  "append_only": "true",
                  "at": [
                    {
                      "statement": "8e5d0ca9-005e-44e6-b858-a8f5b37c5726"
                    }
                  ],
                  "comment": "A stream.",
                  "copy_grants": true,
                  "database": "database",
                  "name": "stream",
                  "schema": "schema",
                  "show_initial_rows": "true",
                  "table": "${snowflake_table.example.fully_qualified_name}"
                }
              references:
                table: snowflake_table.example.fully_qualified_name
        argumentDocs:
            append_only: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies whether this is an append-only stream. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            at: '(Block List, Max: 1) This field specifies that the request is inclusive of any changes made by a statement or transaction with a timestamp equal to the specified parameter. Due to Snowflake limitations, the provider does not detect external changes on this field. External changes for this field won''t be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint". (see below for nested schema)'
            base_tables: (List of String)
            before: '(Block List, Max: 1) This field specifies that the request refers to a point immediately preceding the specified parameter. This point in time is just before the statement, identified by its query ID, is completed.  Due to Snowflake limitations, the provider does not detect external changes on this field. External changes for this field won''t be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint". (see below for nested schema)'
            comment: (String) Specifies a comment for the stream.
            copy_grants: '(Boolean) (Default: false) Retains the access permissions from the original stream when a stream is recreated using the OR REPLACE clause. This is used when the provider detects changes for fields that can not be changed by ALTER. This value will not have any effect during creating a new object with Terraform.'
            create: (String)
            created_on: (String)
            database: '(String) The database in which to create the stream. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            database_name: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE STREAM for the given stream. (see below for nested schema)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            invalid_reason: (String)
            mode: (String)
            name: '(String) Specifies the identifier for the stream; must be unique for the database and schema in which the stream is created. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            offset: (String) Specifies the difference in seconds from the current time to use for Time Travel, in the form -N where N can be an integer or arithmetic expression (e.g. -120 is 120 seconds, -30*60 is 1800 seconds or 30 minutes).
            owner: (String)
            owner_role_type: (String)
            read: (String)
            schema: '(String) The schema in which to create the stream. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            show_initial_rows: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies whether to return all existing rows in the source table as row inserts the first time the stream is consumed. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value. External changes for this field won''t be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".'
            show_output: (List of Object) Outputs the result of SHOW STREAMS for the given stream. (see below for nested schema)
            source_type: (String)
            stale: (Boolean) Indicated if the stream is stale. When Terraform detects that the stream is stale, the stream is recreated with CREATE OR REPLACE. Read more on stream staleness in Snowflake docs.
            stale_after: (String)
            statement: '(String) Specifies the query ID of a statement to use as the reference point for Time Travel. This parameter supports any statement of one of the following types: DML (e.g. INSERT, UPDATE, DELETE), TCL (BEGIN, COMMIT transaction), SELECT.'
            stream: (String) Specifies the identifier (i.e. name) for an existing stream on the queried table or view. The current offset in the stream is used as the AT point in time for returning change data for the source object.
            stream_type: (String) Specifies a type for the stream. This field is used for checking external changes and recreating the resources if needed.
            table: '(String) Specifies an identifier for the table the stream will monitor. Due to technical limitations (read more here), avoid using the following characters: |, ., ". For more information about this resource, see docs.'
            table_name: (String)
            timeouts: (Block, Optional) (see below for nested schema)
            timestamp: (String) Specifies an exact date and time to use for Time Travel. The value must be explicitly cast to a TIMESTAMP, TIMESTAMP_LTZ, TIMESTAMP_NTZ, or TIMESTAMP_TZ data type.
            type: (String)
            update: (String)
        importStatements:
            - terraform import snowflake_stream_on_table.example '"<database_name>"."<schema_name>"."<stream_name>"'
    snowflake_stream_on_view:
        subCategory: Stable
        description: Resource used to manage streams on views. For more information, check stream documentation https://docs.snowflake.com/en/sql-reference/sql/create-stream.
        name: snowflake_stream_on_view
        title: snowflake_stream_on_view Resource - terraform-provider-snowflake
        examples:
            - name: stream
              manifest: |-
                {
                  "database": "database",
                  "name": "stream",
                  "schema": "schema",
                  "view": "${snowflake_view.example.fully_qualified_name}"
                }
              references:
                view: snowflake_view.example.fully_qualified_name
            - name: stream
              manifest: |-
                {
                  "append_only": "true",
                  "at": [
                    {
                      "statement": "8e5d0ca9-005e-44e6-b858-a8f5b37c5726"
                    }
                  ],
                  "comment": "A stream.",
                  "copy_grants": true,
                  "database": "database",
                  "name": "stream",
                  "schema": "schema",
                  "show_initial_rows": "true",
                  "view": "${snowflake_view.example.fully_qualified_name}"
                }
              references:
                view: snowflake_view.example.fully_qualified_name
        argumentDocs:
            append_only: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies whether this is an append-only stream. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            at: '(Block List, Max: 1) This field specifies that the request is inclusive of any changes made by a statement or transaction with a timestamp equal to the specified parameter. Due to Snowflake limitations, the provider does not detect external changes on this field. External changes for this field won''t be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint". (see below for nested schema)'
            base_tables: (List of String)
            before: '(Block List, Max: 1) This field specifies that the request refers to a point immediately preceding the specified parameter. This point in time is just before the statement, identified by its query ID, is completed.  Due to Snowflake limitations, the provider does not detect external changes on this field. External changes for this field won''t be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint". (see below for nested schema)'
            comment: (String) Specifies a comment for the stream.
            copy_grants: '(Boolean) (Default: false) Retains the access permissions from the original stream when a stream is recreated using the OR REPLACE clause. This is used when the provider detects changes for fields that can not be changed by ALTER. This value will not have any effect during creating a new object with Terraform.'
            create: (String)
            created_on: (String)
            database: '(String) The database in which to create the stream. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            database_name: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE STREAM for the given stream. (see below for nested schema)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            invalid_reason: (String)
            mode: (String)
            name: '(String) Specifies the identifier for the stream; must be unique for the database and schema in which the stream is created. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            offset: (String) Specifies the difference in seconds from the current time to use for Time Travel, in the form -N where N can be an integer or arithmetic expression (e.g. -120 is 120 seconds, -30*60 is 1800 seconds or 30 minutes).
            owner: (String)
            owner_role_type: (String)
            read: (String)
            schema: '(String) The schema in which to create the stream. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            show_initial_rows: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies whether to return all existing rows in the source table as row inserts the first time the stream is consumed. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value. External changes for this field won''t be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".'
            show_output: (List of Object) Outputs the result of SHOW STREAMS for the given stream. (see below for nested schema)
            source_type: (String)
            stale: (Boolean) Indicated if the stream is stale. When Terraform detects that the stream is stale, the stream is recreated with CREATE OR REPLACE. Read more on stream staleness in Snowflake docs.
            stale_after: (String)
            statement: '(String) Specifies the query ID of a statement to use as the reference point for Time Travel. This parameter supports any statement of one of the following types: DML (e.g. INSERT, UPDATE, DELETE), TCL (BEGIN, COMMIT transaction), SELECT.'
            stream: (String) Specifies the identifier (i.e. name) for an existing stream on the queried table or view. The current offset in the stream is used as the AT point in time for returning change data for the source object.
            stream_type: (String) Specifies a type for the stream. This field is used for checking external changes and recreating the resources if needed.
            table_name: (String)
            timeouts: (Block, Optional) (see below for nested schema)
            timestamp: (String) Specifies an exact date and time to use for Time Travel. The value must be explicitly cast to a TIMESTAMP, TIMESTAMP_LTZ, TIMESTAMP_NTZ, or TIMESTAMP_TZ data type.
            type: (String)
            update: (String)
            view: '(String) Specifies an identifier for the view the stream will monitor. Due to technical limitations (read more here), avoid using the following characters: |, ., ". For more information about this resource, see docs.'
        importStatements:
            - terraform import snowflake_stream_on_view.example '"<database_name>"."<schema_name>"."<stream_name>"'
    snowflake_streamlit:
        subCategory: Stable
        description: Resource used to manage streamlits objects. For more information, check streamlit documentation https://docs.snowflake.com/en/sql-reference/commands-streamlit.
        name: snowflake_streamlit
        title: snowflake_streamlit Resource - terraform-provider-snowflake
        examples:
            - name: streamlit
              manifest: |-
                {
                  "database": "database",
                  "main_file": "/streamlit_main.py",
                  "name": "streamlit",
                  "schema": "schema",
                  "stage": "${snowflake_stage.example.fully_qualified_name}"
                }
              references:
                stage: snowflake_stage.example.fully_qualified_name
            - name: streamlit
              manifest: |-
                {
                  "comment": "comment",
                  "database": "database",
                  "directory_location": "src",
                  "external_access_integrations": [
                    "integration_id"
                  ],
                  "main_file": "streamlit_main.py",
                  "name": "streamlit",
                  "query_warehouse": "${snowflake_warehouse.example.fully_qualified_name}",
                  "schema": "schema",
                  "stage": "${snowflake_stage.example.fully_qualified_name}",
                  "title": "title"
                }
              references:
                query_warehouse: snowflake_warehouse.example.fully_qualified_name
                stage: snowflake_stage.example.fully_qualified_name
        argumentDocs:
            comment: (String) Specifies a comment for the streamlit.
            create: (String)
            created_on: (String)
            database: '(String) The database in which to create the streamlit Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            database_name: (String)
            default_packages: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE STREAMLIT for the given streamlit. (see below for nested schema)
            directory_location: (String) Specifies the full path to the named stage containing the Streamlit Python files, media files, and the environment.yml file.
            external_access_integrations: (Set of String) External access integrations connected to the Streamlit.
            external_access_secrets: (String)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            import_urls: (Set of String)
            main_file: (String) Specifies the filename of the Streamlit Python application. This filename is relative to the value of directory_location
            name: '(String) String that specifies the identifier (i.e. name) for the streamlit; must be unique in your account. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            owner: (String)
            owner_role_type: (String)
            query_warehouse: (String) Specifies the warehouse where SQL queries issued by the Streamlit application are run. Due to Snowflake limitations warehouse identifier can consist of only upper-cased letters. For more information about this resource, see docs.
            read: (String)
            root_location: (String)
            schema: '(String) The schema in which to create the streamlit. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            show_output: (List of Object) Outputs the result of SHOW STREAMLIT for the given streamlit. (see below for nested schema)
            stage: (String) The stage in which streamlit files are located. For more information about this resource, see docs.
            timeouts: (Block, Optional) (see below for nested schema)
            title: (String) Specifies a title for the Streamlit app to display in Snowsight.
            update: (String)
            url_id: (String)
            user_packages: (Set of String)
        importStatements:
            - terraform import snowflake_schema.example '"<database_name>"."<schema_name>"."<streamlit_name>"'
    snowflake_table:
        subCategory: Preview
        name: snowflake_table
        title: snowflake_table Resource - terraform-provider-snowflake
        examples:
            - name: table
              manifest: |-
                {
                  "change_tracking": false,
                  "cluster_by": [
                    "to_date(DATE)"
                  ],
                  "column": [
                    {
                      "default": [
                        {
                          "sequence": "${snowflake_sequence.sequence.fully_qualified_name}"
                        }
                      ],
                      "name": "id",
                      "nullable": true,
                      "type": "int"
                    },
                    {
                      "identity": [
                        {
                          "start_num": 1,
                          "step_num": 3
                        }
                      ],
                      "name": "identity",
                      "nullable": true,
                      "type": "NUMBER(38,0)"
                    },
                    {
                      "collate": "en-ci",
                      "name": "data",
                      "nullable": false,
                      "type": "text"
                    },
                    {
                      "name": "DATE",
                      "type": "TIMESTAMP_NTZ(9)"
                    },
                    {
                      "comment": "extra data",
                      "name": "extra",
                      "type": "VARIANT"
                    }
                  ],
                  "comment": "A table.",
                  "data_retention_time_in_days": "${snowflake_schema.schema.data_retention_time_in_days}",
                  "database": "${snowflake_schema.schema.database}",
                  "name": "table",
                  "primary_key": [
                    {
                      "keys": [
                        "data"
                      ],
                      "name": "my_key"
                    }
                  ],
                  "schema": "${snowflake_schema.schema.name}"
                }
              references:
                column.default.sequence: snowflake_sequence.sequence.fully_qualified_name
                data_retention_time_in_days: snowflake_schema.schema.data_retention_time_in_days
                database: snowflake_schema.schema.database
                schema: snowflake_schema.schema.name
              dependencies:
                snowflake_schema.schema: |-
                    {
                      "data_retention_days": 1,
                      "database": "database",
                      "name": "schema"
                    }
                snowflake_sequence.sequence: |-
                    {
                      "database": "${snowflake_schema.schema.database}",
                      "name": "sequence",
                      "schema": "${snowflake_schema.schema.name}"
                    }
        argumentDocs:
            change_tracking: '(Boolean) (Default: false) Specifies whether to enable change tracking on the table. Default false.'
            cluster_by: (List of String) A list of one or more table columns/expressions to be used as clustering key(s) for the table
            collate: '(String) (Default: ``) Column collation, e.g. utf8'
            column: '(Block List, Min: 1) Definitions of a column to create in the table. Minimum one required. (see below for nested schema)'
            comment: (String) Specifies a comment for the table.
            constant: (String) The default constant value for the column
            create: (String)
            data_retention_time_in_days: '(Number) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (-1)) Specifies the retention period for the table so that Time Travel actions (SELECT, CLONE, UNDROP) can be performed on historical data in the table. If you wish to inherit the parent schema setting then pass in the schema attribute to this argument or do not fill this parameter at all; the default value for this field is -1, which is a fallback to use Snowflake default - in this case the schema value'
            database: (String) The database in which to create the table.
            default: '(Block List, Max: 1) Defines the column default value; note due to limitations of Snowflake''s ALTER TABLE ADD/MODIFY COLUMN updates to default will not be applied (see below for nested schema)'
            delete: (String)
            expression: (String) The default expression value for the column
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            identity: '(Block List, Max: 1) Defines the identity start/step values for a column. Note Identity/default are mutually exclusive. (see below for nested schema)'
            keys: (List of String) Columns to use in primary key
            masking_policy: '(String) (Default: ``) Masking policy to apply on column. It has to be a fully qualified name.'
            name: (String) Specifies the identifier for the table; must be unique for the database and schema in which the table is created.
            nullable: '(Boolean) (Default: true) Whether this column can contain null values. Note: Depending on your Snowflake version, the default value will not suffice if this column is used in a primary key constraint.'
            owner: (String) Name of the role that owns the table.
            primary_key: '(Block List, Max: 1, Deprecated) Definitions of primary key constraint to create on table (see below for nested schema)'
            read: (String)
            schema: (String) The schema in which to create the table.
            schema_evolution_record: (String) Record of schema evolution.
            sequence: (String) The default sequence to use for the column
            start_num: '(Number) (Default: 1) The number to start incrementing at.'
            step_num: '(Number) (Default: 1) Step size to increment by.'
            tag: (Block List, Deprecated) Definitions of a tag to associate with the resource. (see below for nested schema)
            timeouts: (Block, Optional) (see below for nested schema)
            type: (String) Column type, e.g. VARIANT. For a full list of column types, see Summary of Data Types.
            update: (String)
            value: (String) Tag value, e.g. marketing_info.
        importStatements:
            - |-
              # format is database name | schema name | table name
              terraform import snowflake_table.example 'databaseName|schemaName|tableName'
    snowflake_table_column_masking_policy_application:
        subCategory: Preview
        description: Applies a masking policy to a table column.
        name: snowflake_table_column_masking_policy_application
        title: snowflake_table_column_masking_policy_application Resource - terraform-provider-snowflake
        examples:
            - name: application
              manifest: |-
                {
                  "column": "secret",
                  "masking_policy": "${snowflake_masking_policy.policy.fully_qualified_name}",
                  "provider": "${snowflake.masking}",
                  "table": "${snowflake_table.table.fully_qualified_name}"
                }
              references:
                masking_policy: snowflake_masking_policy.policy.fully_qualified_name
                provider: snowflake.masking
                table: snowflake_table.table.fully_qualified_name
              dependencies:
                snowflake_masking_policy.policy: |-
                    {
                      "database": "EXAMPLE_DB",
                      "masking_expression": "case when current_role() in ('ANALYST') then val else sha2(val, 512) end",
                      "name": "EXAMPLE_MASKING_POLICY",
                      "provider": "${snowflake.masking}",
                      "return_data_type": "VARCHAR",
                      "schema": "EXAMPLE_SCHEMA",
                      "value_data_type": "VARCHAR"
                    }
                snowflake_table.table: |-
                    {
                      "column": [
                        {
                          "name": "secret",
                          "type": "VARCHAR(16777216)"
                        }
                      ],
                      "database": "EXAMPLE_DB",
                      "lifecycle": [
                        {
                          "ignore_changes": [
                            "${column[0].masking_policy}"
                          ]
                        }
                      ],
                      "name": "table",
                      "schema": "EXAMPLE_SCHEMA"
                    }
        argumentDocs:
            column: (String) The column to apply the masking policy to.
            create: (String)
            delete: (String)
            id: (String) The ID of this resource.
            masking_policy: (String) Fully qualified name (database.schema.policyname) of the policy to apply.
            read: (String)
            table: (String) The fully qualified name (database.schema.table) of the table to apply the masking policy to.
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
        importStatements: []
    snowflake_table_constraint:
        subCategory: Preview
        name: snowflake_table_constraint
        title: snowflake_table_constraint Resource - terraform-provider-snowflake
        examples:
            - name: primary_key
              manifest: |-
                {
                  "columns": [
                    "col1"
                  ],
                  "comment": "hello world",
                  "name": "myconstraint",
                  "table_id": "${snowflake_table.t.fully_qualified_name}",
                  "type": "PRIMARY KEY"
                }
              references:
                table_id: snowflake_table.t.fully_qualified_name
              dependencies:
                snowflake_database.d: |-
                    {
                      "name": "some_db"
                    }
                snowflake_schema.s: |-
                    {
                      "database": "${snowflake_database.d.name}",
                      "name": "some_schema"
                    }
                snowflake_table.fk_t: |-
                    {
                      "column": [
                        {
                          "name": "fk_col1",
                          "nullable": false,
                          "type": "text"
                        },
                        {
                          "name": "fk_col2",
                          "nullable": false,
                          "type": "text"
                        }
                      ],
                      "database": "${snowflake_database.d.name}",
                      "name": "fk_table",
                      "schema": "${snowflake_schema.s.name}"
                    }
                snowflake_table.t: |-
                    {
                      "column": [
                        {
                          "name": "col1",
                          "nullable": false,
                          "type": "text"
                        },
                        {
                          "name": "col2",
                          "nullable": false,
                          "type": "text"
                        },
                        {
                          "name": "col3",
                          "nullable": false,
                          "type": "text"
                        }
                      ],
                      "database": "${snowflake_database.d.name}",
                      "name": "some_table",
                      "schema": "${snowflake_schema.s.name}"
                    }
            - name: foreign_key
              manifest: |-
                {
                  "columns": [
                    "col2"
                  ],
                  "comment": "hello fk",
                  "deferrable": false,
                  "enforced": false,
                  "foreign_key_properties": [
                    {
                      "references": [
                        {
                          "columns": [
                            "fk_col1"
                          ],
                          "table_id": "${snowflake_table.fk_t.fully_qualified_name}"
                        }
                      ]
                    }
                  ],
                  "initially": "IMMEDIATE",
                  "name": "myconstraintfk",
                  "table_id": "${snowflake_table.t.fully_qualified_name}",
                  "type": "FOREIGN KEY"
                }
              references:
                foreign_key_properties.references.table_id: snowflake_table.fk_t.fully_qualified_name
                table_id: snowflake_table.t.fully_qualified_name
              dependencies:
                snowflake_database.d: |-
                    {
                      "name": "some_db"
                    }
                snowflake_schema.s: |-
                    {
                      "database": "${snowflake_database.d.name}",
                      "name": "some_schema"
                    }
                snowflake_table.fk_t: |-
                    {
                      "column": [
                        {
                          "name": "fk_col1",
                          "nullable": false,
                          "type": "text"
                        },
                        {
                          "name": "fk_col2",
                          "nullable": false,
                          "type": "text"
                        }
                      ],
                      "database": "${snowflake_database.d.name}",
                      "name": "fk_table",
                      "schema": "${snowflake_schema.s.name}"
                    }
                snowflake_table.t: |-
                    {
                      "column": [
                        {
                          "name": "col1",
                          "nullable": false,
                          "type": "text"
                        },
                        {
                          "name": "col2",
                          "nullable": false,
                          "type": "text"
                        },
                        {
                          "name": "col3",
                          "nullable": false,
                          "type": "text"
                        }
                      ],
                      "database": "${snowflake_database.d.name}",
                      "name": "some_table",
                      "schema": "${snowflake_schema.s.name}"
                    }
            - name: unique
              manifest: |-
                {
                  "columns": [
                    "col3"
                  ],
                  "comment": "hello unique",
                  "name": "unique",
                  "table_id": "${snowflake_table.t.fully_qualified_name}",
                  "type": "UNIQUE"
                }
              references:
                table_id: snowflake_table.t.fully_qualified_name
              dependencies:
                snowflake_database.d: |-
                    {
                      "name": "some_db"
                    }
                snowflake_schema.s: |-
                    {
                      "database": "${snowflake_database.d.name}",
                      "name": "some_schema"
                    }
                snowflake_table.fk_t: |-
                    {
                      "column": [
                        {
                          "name": "fk_col1",
                          "nullable": false,
                          "type": "text"
                        },
                        {
                          "name": "fk_col2",
                          "nullable": false,
                          "type": "text"
                        }
                      ],
                      "database": "${snowflake_database.d.name}",
                      "name": "fk_table",
                      "schema": "${snowflake_schema.s.name}"
                    }
                snowflake_table.t: |-
                    {
                      "column": [
                        {
                          "name": "col1",
                          "nullable": false,
                          "type": "text"
                        },
                        {
                          "name": "col2",
                          "nullable": false,
                          "type": "text"
                        },
                        {
                          "name": "col3",
                          "nullable": false,
                          "type": "text"
                        }
                      ],
                      "database": "${snowflake_database.d.name}",
                      "name": "some_table",
                      "schema": "${snowflake_schema.s.name}"
                    }
        argumentDocs:
            columns: (List of String) Columns to use in constraint key
            comment: (String, Deprecated) Comment for the table constraint
            create: (String)
            deferrable: '(Boolean) (Default: true) Whether the constraint is deferrable'
            delete: (String)
            enable: '(Boolean) (Default: true) Specifies whether the constraint is enabled or disabled. These properties are provided for compatibility with Oracle.'
            enforced: '(Boolean) (Default: false) Whether the constraint is enforced'
            foreign_key_properties: '(Block List, Max: 1) Additional properties when type is set to foreign key. Not applicable for primary/unique keys (see below for nested schema)'
            id: (String) The ID of this resource.
            initially: '(String) (Default: DEFERRED) Whether the constraint is initially deferred or immediate'
            match: '(String) (Default: FULL) The match type for the foreign key. Not applicable for primary/unique keys'
            name: (String) Name of constraint
            on_delete: '(String) (Default: NO ACTION) Specifies the action performed when the primary/unique key for the foreign key is deleted. Not applicable for primary/unique keys'
            on_update: '(String) (Default: NO ACTION) Specifies the action performed when the primary/unique key for the foreign key is updated. Not applicable for primary/unique keys'
            read: (String)
            references: '(Block List, Min: 1, Max: 1) The table and columns that the foreign key references. (see below for nested schema)'
            rely: '(Boolean) (Default: true) Specifies whether a constraint in NOVALIDATE mode is taken into account during query rewrite.'
            table_id: '(String) Identifier for table to create constraint on. Format must follow: ""<db_name>"."<schema_name>"."<table_name>"" or "<db_name>.<schema_name>.<table_name>" (snowflake_table.my_table.id)'
            timeouts: (Block, Optional) (see below for nested schema)
            type: (String) Type of constraint, one of 'UNIQUE', 'PRIMARY KEY', or 'FOREIGN KEY'
            update: (String)
            validate: '(Boolean) (Default: false) Specifies whether to validate existing data on the table when a constraint is created. Only used in conjunction with the ENABLE property.'
        importStatements:
            - terraform import snowflake_table_constraint.example 'myconstraintfk❄️FOREIGN KEY❄️databaseName|schemaName|tableName'
    snowflake_tag:
        subCategory: Stable
        description: Resource used to manage tags. For more information, check tag documentation https://docs.snowflake.com/en/sql-reference/sql/create-tag. For assigning tags to Snowflake objects, see tag_association resource ./tag_association.
        name: snowflake_tag
        title: snowflake_tag Resource - terraform-provider-snowflake
        examples:
            - name: tag
              manifest: |-
                {
                  "database": "database",
                  "name": "tag",
                  "schema": "schema"
                }
            - name: tag
              manifest: |-
                {
                  "allowed_values": [
                    "finance",
                    "engineering",
                    ""
                  ],
                  "comment": "comment",
                  "database": "database",
                  "masking_policies": [
                    "${snowflake_masking_policy.example.fully_qualified_name}"
                  ],
                  "name": "tag",
                  "schema": "schema"
                }
        argumentDocs:
            allowed_values: (Set of String) Set of allowed values for the tag.
            comment: (String) Specifies a comment for the tag.
            create: (String)
            created_on: (String)
            database: '(String) The database in which to create the tag. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            database_name: (String)
            delete: (String)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            id: (String) The ID of this resource.
            masking_policies: (Set of String) Set of masking policies for the tag. A tag can support one masking policy for each data type. If masking policies are assigned to the tag, before dropping the tag, the provider automatically unassigns them. For more information about this resource, see docs.
            name: '(String) Specifies the identifier for the tag; must be unique for the database in which the tag is created. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            owner: (String)
            owner_role_type: (String)
            read: (String)
            schema: '(String) The schema in which to create the tag. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            show_output: (List of Object) Outputs the result of SHOW TAGS for the given tag. (see below for nested schema)
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
        importStatements:
            - terraform import snowflake_tag.example '"<database_name>"."<schema_name>"."<tag_name>"'
    snowflake_tag_association:
        subCategory: Stable
        description: Resource used to manage tag associations. For more information, check object tagging documentation https://docs.snowflake.com/en/user-guide/object-tagging.
        name: snowflake_tag_association
        title: snowflake_tag_association Resource - terraform-provider-snowflake
        examples:
            - name: db_association
              manifest: |-
                {
                  "object_identifiers": [
                    "${snowflake_database.test.fully_qualified_name}"
                  ],
                  "object_type": "DATABASE",
                  "tag_id": "${snowflake_tag.test.fully_qualified_name}",
                  "tag_value": "finance"
                }
              references:
                tag_id: snowflake_tag.test.fully_qualified_name
              dependencies:
                snowflake_database.test: |-
                    {
                      "name": "database"
                    }
                snowflake_schema.test: |-
                    {
                      "database": "${snowflake_database.test.name}",
                      "name": "schema"
                    }
                snowflake_table.test: |-
                    {
                      "column": [
                        {
                          "name": "column1",
                          "type": "VARIANT"
                        },
                        {
                          "name": "column2",
                          "type": "VARCHAR(16)"
                        }
                      ],
                      "comment": "Terraform example table",
                      "database": "${snowflake_database.test.name}",
                      "name": "TABLE_NAME",
                      "schema": "${snowflake_schema.test.name}"
                    }
                snowflake_tag.test: |-
                    {
                      "allowed_values": [
                        "finance",
                        "engineering"
                      ],
                      "database": "${snowflake_database.test.name}",
                      "name": "cost_center",
                      "schema": "${snowflake_schema.test.name}"
                    }
            - name: table_association
              manifest: |-
                {
                  "object_identifiers": [
                    "${snowflake_table.test.fully_qualified_name}"
                  ],
                  "object_type": "TABLE",
                  "tag_id": "${snowflake_tag.test.fully_qualified_name}",
                  "tag_value": "engineering"
                }
              references:
                tag_id: snowflake_tag.test.fully_qualified_name
              dependencies:
                snowflake_database.test: |-
                    {
                      "name": "database"
                    }
                snowflake_schema.test: |-
                    {
                      "database": "${snowflake_database.test.name}",
                      "name": "schema"
                    }
                snowflake_table.test: |-
                    {
                      "column": [
                        {
                          "name": "column1",
                          "type": "VARIANT"
                        },
                        {
                          "name": "column2",
                          "type": "VARCHAR(16)"
                        }
                      ],
                      "comment": "Terraform example table",
                      "database": "${snowflake_database.test.name}",
                      "name": "TABLE_NAME",
                      "schema": "${snowflake_schema.test.name}"
                    }
                snowflake_tag.test: |-
                    {
                      "allowed_values": [
                        "finance",
                        "engineering"
                      ],
                      "database": "${snowflake_database.test.name}",
                      "name": "cost_center",
                      "schema": "${snowflake_schema.test.name}"
                    }
            - name: column_association
              manifest: |-
                {
                  "object_identifiers": [
                    "${format(\"%s.\\\"column1\\\"\", snowflake_table.test.fully_qualified_name)}"
                  ],
                  "object_type": "COLUMN",
                  "tag_id": "${snowflake_tag.test.fully_qualified_name}",
                  "tag_value": "engineering"
                }
              references:
                tag_id: snowflake_tag.test.fully_qualified_name
              dependencies:
                snowflake_database.test: |-
                    {
                      "name": "database"
                    }
                snowflake_schema.test: |-
                    {
                      "database": "${snowflake_database.test.name}",
                      "name": "schema"
                    }
                snowflake_table.test: |-
                    {
                      "column": [
                        {
                          "name": "column1",
                          "type": "VARIANT"
                        },
                        {
                          "name": "column2",
                          "type": "VARCHAR(16)"
                        }
                      ],
                      "comment": "Terraform example table",
                      "database": "${snowflake_database.test.name}",
                      "name": "TABLE_NAME",
                      "schema": "${snowflake_schema.test.name}"
                    }
                snowflake_tag.test: |-
                    {
                      "allowed_values": [
                        "finance",
                        "engineering"
                      ],
                      "database": "${snowflake_database.test.name}",
                      "name": "cost_center",
                      "schema": "${snowflake_schema.test.name}"
                    }
            - name: account_association
              manifest: |-
                {
                  "object_identifiers": [
                    "\"ORGANIZATION_NAME\".\"ACCOUNT_NAME\""
                  ],
                  "object_type": "ACCOUNT",
                  "tag_id": "${snowflake_tag.test.fully_qualified_name}",
                  "tag_value": "engineering"
                }
              references:
                tag_id: snowflake_tag.test.fully_qualified_name
              dependencies:
                snowflake_database.test: |-
                    {
                      "name": "database"
                    }
                snowflake_schema.test: |-
                    {
                      "database": "${snowflake_database.test.name}",
                      "name": "schema"
                    }
                snowflake_table.test: |-
                    {
                      "column": [
                        {
                          "name": "column1",
                          "type": "VARIANT"
                        },
                        {
                          "name": "column2",
                          "type": "VARCHAR(16)"
                        }
                      ],
                      "comment": "Terraform example table",
                      "database": "${snowflake_database.test.name}",
                      "name": "TABLE_NAME",
                      "schema": "${snowflake_schema.test.name}"
                    }
                snowflake_tag.test: |-
                    {
                      "allowed_values": [
                        "finance",
                        "engineering"
                      ],
                      "database": "${snowflake_database.test.name}",
                      "name": "cost_center",
                      "schema": "${snowflake_schema.test.name}"
                    }
        argumentDocs:
            create: (String)
            delete: (String)
            id: (String) The ID of this resource.
            object_identifiers: (Set of String) Specifies the object identifiers for the tag association.
            object_type: '(String) Specifies the type of object to add a tag. Allowed object types: ACCOUNT | APPLICATION | APPLICATION PACKAGE | COMPUTE POOL | DATABASE | FAILOVER GROUP | INTEGRATION | NETWORK POLICY | REPLICATION GROUP | ROLE | SHARE | USER | WAREHOUSE | DATABASE ROLE | SCHEMA | ALERT | SNOWFLAKE.CORE.BUDGET | SNOWFLAKE.ML.CLASSIFICATION | EXTERNAL FUNCTION | EXTERNAL TABLE | FUNCTION | IMAGE REPOSITORY | GIT REPOSITORY | ICEBERG TABLE | MATERIALIZED VIEW | PIPE | MASKING POLICY | PASSWORD POLICY | ROW ACCESS POLICY | SESSION POLICY | PRIVACY POLICY | PROCEDURE | SERVICE | STAGE | STREAM | TABLE | TASK | VIEW | COLUMN | EVENT TABLE.'
            read: (String)
            skip_validation: '(Boolean) (Default: true) If true, skips validation of the tag association.'
            tag_id: (String) Specifies the identifier for the tag.
            tag_value: (String) Specifies the value of the tag, (e.g. 'finance' or 'engineering')
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
        importStatements:
            - terraform import snowflake_tag_association.example '"TAG_DATABASE"."TAG_SCHEMA"."TAG_NAME"|TAG_VALUE|OBJECT_TYPE'
    snowflake_task:
        subCategory: Stable
        description: Resource used to manage task objects. For more information, check task documentation https://docs.snowflake.com/en/user-guide/tasks-intro.
        name: snowflake_task
        title: snowflake_task Resource - terraform-provider-snowflake
        examples:
            - name: task
              manifest: |-
                {
                  "database": "database",
                  "name": "task",
                  "schedule": [
                    {
                      "minutes": 5
                    }
                  ],
                  "schema": "schema",
                  "sql_statement": "select 1",
                  "started": true,
                  "warehouse": "warehouse"
                }
            - name: serverless_task
              manifest: |-
                {
                  "database": "database",
                  "name": "task",
                  "schedule": [
                    {
                      "minutes": 5
                    }
                  ],
                  "schema": "schema",
                  "sql_statement": "select 1",
                  "started": true,
                  "user_task_managed_initial_warehouse_size": "XSMALL"
                }
            - name: child_task
              manifest: |-
                {
                  "after": [
                    "${snowflake_task.root_task.fully_qualified_name}",
                    "\u003cdatabase_name\u003e.\u003cschema_name\u003e.\u003croot_task_name\u003e"
                  ],
                  "database": "database",
                  "name": "task",
                  "schema": "schema",
                  "sql_statement": "select 1",
                  "started": true,
                  "warehouse": "warehouse"
                }
            - name: child_task
              manifest: |-
                {
                  "database": "database",
                  "finalize": "${snowflake_task.root_task.fully_qualified_name}",
                  "name": "task",
                  "schema": "schema",
                  "sql_statement": "select 1",
                  "started": true,
                  "warehouse": "warehouse"
                }
              references:
                finalize: snowflake_task.root_task.fully_qualified_name
            - name: test
              manifest: |-
                {
                  "abort_detached_query": false,
                  "allow_overlapping_execution": true,
                  "autocommit": true,
                  "binary_input_format": "HEX",
                  "binary_output_format": "HEX",
                  "client_memory_limit": 1536,
                  "client_metadata_request_use_connection_ctx": false,
                  "client_prefetch_threads": 4,
                  "client_result_chunk_size": 160,
                  "client_result_column_case_insensitive": false,
                  "client_session_keep_alive": false,
                  "client_session_keep_alive_heartbeat_frequency": 3600,
                  "client_timestamp_type_mapping": "TIMESTAMP_LTZ",
                  "comment": "complete task",
                  "config": "{\"key\":\"value\"}",
                  "database": "database",
                  "date_input_format": "AUTO",
                  "date_output_format": "YYYY-MM-DD",
                  "enable_unload_physical_type_optimization": true,
                  "error_integration": "${snowflake_notification_integration.example.fully_qualified_name}",
                  "error_on_nondeterministic_merge": true,
                  "error_on_nondeterministic_update": false,
                  "geography_output_format": "GeoJSON",
                  "geometry_output_format": "GeoJSON",
                  "jdbc_use_session_timezone": true,
                  "json_indent": 2,
                  "lock_timeout": 43200,
                  "log_level": "OFF",
                  "multi_statement_count": 1,
                  "name": "task",
                  "noorder_sequence_as_default": true,
                  "odbc_treat_decimal_as_int": false,
                  "query_tag": "",
                  "quoted_identifiers_ignore_case": false,
                  "rows_per_resultset": 0,
                  "s3_stage_vpce_dns_name": "",
                  "schedule": [
                    {
                      "minutes": 10
                    }
                  ],
                  "schema": "schema",
                  "search_path": "$current, $public",
                  "sql_statement": "select 1",
                  "started": true,
                  "statement_queued_timeout_in_seconds": 0,
                  "statement_timeout_in_seconds": 172800,
                  "strict_json_output": false,
                  "suspend_task_after_num_failures": 10,
                  "task_auto_retry_attempts": 0,
                  "time_input_format": "AUTO",
                  "time_output_format": "HH24:MI:SS",
                  "timestamp_day_is_always_24h": false,
                  "timestamp_input_format": "AUTO",
                  "timestamp_ltz_output_format": "",
                  "timestamp_ntz_output_format": "YYYY-MM-DD HH24:MI:SS.FF3",
                  "timestamp_output_format": "YYYY-MM-DD HH24:MI:SS.FF3 TZHTZM",
                  "timestamp_type_mapping": "TIMESTAMP_NTZ",
                  "timestamp_tz_output_format": "",
                  "timezone": "America/Los_Angeles",
                  "trace_level": "OFF",
                  "transaction_abort_on_error": false,
                  "transaction_default_isolation_level": "READ COMMITTED",
                  "two_digit_century_start": 1970,
                  "unsupported_ddl_action": "ignore",
                  "use_cached_result": true,
                  "user_task_managed_initial_warehouse_size": "Medium",
                  "user_task_minimum_trigger_interval_in_seconds": 30,
                  "user_task_timeout_ms": 3600000,
                  "warehouse": "${snowflake_warehouse.example.fully_qualified_name}",
                  "week_of_year_policy": 0,
                  "week_start": 0,
                  "when": "SYSTEM$STREAM_HAS_DATA('\u003cstream_name\u003e')"
                }
              references:
                error_integration: snowflake_notification_integration.example.fully_qualified_name
                warehouse: snowflake_warehouse.example.fully_qualified_name
        argumentDocs:
            abort_detached_query: (Boolean) Specifies the action that Snowflake performs for in-progress queries if connectivity is lost due to abrupt termination of a session (e.g. network outage, browser termination, service interruption). For more information, check ABORT_DETACHED_QUERY docs.
            after: '(Set of String) Specifies one or more predecessor tasks for the current task. Use this option to create a DAG of tasks or add this task to an existing DAG. A DAG is a series of tasks that starts with a scheduled root task and is linked together by dependencies. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            allow_overlapping_execution: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) By default, Snowflake ensures that only one instance of a particular DAG is allowed to run at a time, setting the parameter value to TRUE permits DAG runs to overlap. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            autocommit: (Boolean) Specifies whether autocommit is enabled for the session. Autocommit determines whether a DML statement, when executed without an active transaction, is automatically committed after the statement successfully completes. For more information, see Transactions. For more information, check AUTOCOMMIT docs.
            binary_input_format: (String) The format of VARCHAR values passed as input to VARCHAR-to-BINARY conversion functions. For more information, see Binary input and output. For more information, check BINARY_INPUT_FORMAT docs.
            binary_output_format: (String) The format for VARCHAR values returned as output by BINARY-to-VARCHAR conversion functions. For more information, see Binary input and output. For more information, check BINARY_OUTPUT_FORMAT docs.
            budget: (String)
            client_memory_limit: (Number) Parameter that specifies the maximum amount of memory the JDBC driver or ODBC driver should use for the result set from queries (in MB). For more information, check CLIENT_MEMORY_LIMIT docs.
            client_metadata_request_use_connection_ctx: (Boolean) For specific ODBC functions and JDBC methods, this parameter can change the default search scope from all databases/schemas to the current database/schema. The narrower search typically returns fewer rows and executes more quickly. For more information, check CLIENT_METADATA_REQUEST_USE_CONNECTION_CTX docs.
            client_prefetch_threads: (Number) Parameter that specifies the number of threads used by the client to pre-fetch large result sets. The driver will attempt to honor the parameter value, but defines the minimum and maximum values (depending on your system’s resources) to improve performance. For more information, check CLIENT_PREFETCH_THREADS docs.
            client_result_chunk_size: (Number) Parameter that specifies the maximum size of each set (or chunk) of query results to download (in MB). The JDBC driver downloads query results in chunks. For more information, check CLIENT_RESULT_CHUNK_SIZE docs.
            client_result_column_case_insensitive: (Boolean) Parameter that indicates whether to match column name case-insensitively in ResultSet.get* methods in JDBC. For more information, check CLIENT_RESULT_COLUMN_CASE_INSENSITIVE docs.
            client_session_keep_alive: (Boolean) Parameter that indicates whether to force a user to log in again after a period of inactivity in the session. For more information, check CLIENT_SESSION_KEEP_ALIVE docs.
            client_session_keep_alive_heartbeat_frequency: (Number) Number of seconds in-between client attempts to update the token for the session. For more information, check CLIENT_SESSION_KEEP_ALIVE_HEARTBEAT_FREQUENCY docs.
            client_timestamp_type_mapping: (String) Specifies the TIMESTAMP_* variation to use when binding timestamp variables for JDBC or ODBC applications that use the bind API to load data. For more information, check CLIENT_TIMESTAMP_TYPE_MAPPING docs.
            comment: (String) Specifies a comment for the task.
            condition: (String)
            config: (String) Specifies a string representation of key value pairs that can be accessed by all tasks in the task graph. Must be in JSON format.
            create: (String)
            created_on: (String)
            database: '(String) The database in which to create the task. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            database_name: (String)
            date_input_format: (String) Specifies the input format for the DATE data type. For more information, see Date and time input and output formats. For more information, check DATE_INPUT_FORMAT docs.
            date_output_format: (String) Specifies the display format for the DATE data type. For more information, see Date and time input and output formats. For more information, check DATE_OUTPUT_FORMAT docs.
            default: (String)
            definition: (String)
            delete: (String)
            description: (String)
            enable_unload_physical_type_optimization: (Boolean) Specifies whether to set the schema for unloaded Parquet files based on the logical column data types (i.e. the types in the unload SQL query or source table) or on the unloaded column values (i.e. the smallest data types and precision that support the values in the output columns of the unload SQL statement or source table). For more information, check ENABLE_UNLOAD_PHYSICAL_TYPE_OPTIMIZATION docs.
            error_integration: '(String) Specifies the name of the notification integration used for error notifications. Due to technical limitations (read more here), avoid using the following characters: |, ., ". For more information about this resource, see docs.'
            error_on_nondeterministic_merge: (Boolean) Specifies whether to return an error when the MERGE command is used to update or delete a target row that joins multiple source rows and the system cannot determine the action to perform on the target row. For more information, check ERROR_ON_NONDETERMINISTIC_MERGE docs.
            error_on_nondeterministic_update: (Boolean) Specifies whether to return an error when the UPDATE command is used to update a target row that joins multiple source rows and the system cannot determine the action to perform on the target row. For more information, check ERROR_ON_NONDETERMINISTIC_UPDATE docs.
            finalize: '(String) Specifies the name of a root task that the finalizer task is associated with. Finalizer tasks run after all other tasks in the task graph run to completion. You can define the SQL of a finalizer task to handle notifications and the release and cleanup of resources that a task graph uses. For more information, see Release and cleanup of task graphs. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            finalized_root_task: (String)
            finalizer: (String)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            geography_output_format: (String) Display format for GEOGRAPHY values. For more information, check GEOGRAPHY_OUTPUT_FORMAT docs.
            geometry_output_format: (String) Display format for GEOMETRY values. For more information, check GEOMETRY_OUTPUT_FORMAT docs.
            id: (String) The ID of this resource.
            jdbc_treat_timestamp_ntz_as_utc: (Boolean) Specifies how JDBC processes TIMESTAMP_NTZ values. For more information, check JDBC_TREAT_TIMESTAMP_NTZ_AS_UTC docs.
            jdbc_use_session_timezone: (Boolean) Specifies whether the JDBC Driver uses the time zone of the JVM or the time zone of the session (specified by the TIMEZONE parameter) for the getDate(), getTime(), and getTimestamp() methods of the ResultSet class. For more information, check JDBC_USE_SESSION_TIMEZONE docs.
            json_indent: (Number) Specifies the number of blank spaces to indent each new element in JSON output in the session. Also specifies whether to insert newline characters after each element. For more information, check JSON_INDENT docs.
            key: (String)
            last_committed_on: (String)
            last_suspended_on: (String)
            last_suspended_reason: (String)
            level: (String)
            lock_timeout: (Number) Number of seconds to wait while trying to lock a resource, before timing out and aborting the statement. For more information, check LOCK_TIMEOUT docs.
            log_level: (String) Specifies the severity level of messages that should be ingested and made available in the active event table. Messages at the specified level (and at more severe levels) are ingested. For more information about log levels, see Setting log level. For more information, check LOG_LEVEL docs.
            minutes: (Number) Specifies an interval (in minutes) of wait time inserted between runs of the task. Accepts positive integers only. (conflicts with using_cron)
            multi_statement_count: (Number) Number of statements to execute when using the multi-statement capability. For more information, check MULTI_STATEMENT_COUNT docs.
            name: '(String) Specifies the identifier for the task; must be unique for the database and schema in which the task is created. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            noorder_sequence_as_default: (Boolean) Specifies whether the ORDER or NOORDER property is set by default when you create a new sequence or add a new table column. The ORDER and NOORDER properties determine whether or not the values are generated for the sequence or auto-incremented column in increasing or decreasing order. For more information, check NOORDER_SEQUENCE_AS_DEFAULT docs.
            odbc_treat_decimal_as_int: (Boolean) Specifies how ODBC processes columns that have a scale of zero (0). For more information, check ODBC_TREAT_DECIMAL_AS_INT docs.
            owner: (String)
            owner_role_type: (String)
            parameters: (List of Object) Outputs the result of SHOW PARAMETERS IN TASK for the given task. (see below for nested schema)
            predecessors: (Set of String)
            query_tag: (String) Optional string that can be used to tag queries and other SQL statements executed within a session. The tags are displayed in the output of the QUERY_HISTORY, QUERY_HISTORY_BY_* functions. For more information, check QUERY_TAG docs.
            quoted_identifiers_ignore_case: (Boolean) Specifies whether letters in double-quoted object identifiers are stored and resolved as uppercase letters. By default, Snowflake preserves the case of alphabetic characters when storing and resolving double-quoted identifiers (see Identifier resolution). You can use this parameter in situations in which third-party applications always use double quotes around identifiers. For more information, check QUOTED_IDENTIFIERS_IGNORE_CASE docs.
            read: (String)
            rows_per_resultset: (Number) Specifies the maximum number of rows returned in a result set. A value of 0 specifies no maximum. For more information, check ROWS_PER_RESULTSET docs.
            s3_stage_vpce_dns_name: (String) Specifies the DNS name of an Amazon S3 interface endpoint. Requests sent to the internal stage of an account via AWS PrivateLink for Amazon S3 use this endpoint to connect. For more information, see Accessing Internal stages with dedicated interface endpoints. For more information, check S3_STAGE_VPCE_DNS_NAME docs.
            schedule: '(Block List, Max: 1) The schedule for periodically running the task. This can be a cron or interval in minutes. (Conflicts with finalize and after; when set, one of the sub-fields minutes or using_cron should be set) (see below for nested schema)'
            schema: '(String) The schema in which to create the task. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            search_path: (String) Specifies the path to search to resolve unqualified object names in queries. For more information, see Name resolution in queries. Comma-separated list of identifiers. An identifier can be a fully or partially qualified schema name. For more information, check SEARCH_PATH docs.
            show_output: (List of Object) Outputs the result of SHOW TASKS for the given task. (see below for nested schema)
            sql_statement: (String) Any single SQL statement, or a call to a stored procedure, executed when the task runs.
            started: (Boolean) Specifies if the task should be started or suspended.
            state: (String)
            statement_queued_timeout_in_seconds: (Number) Amount of time, in seconds, a SQL statement (query, DDL, DML, etc.) remains queued for a warehouse before it is canceled by the system. This parameter can be used in conjunction with the MAX_CONCURRENCY_LEVEL parameter to ensure a warehouse is never backlogged. For more information, check STATEMENT_QUEUED_TIMEOUT_IN_SECONDS docs.
            statement_timeout_in_seconds: (Number) Amount of time, in seconds, after which a running SQL statement (query, DDL, DML, etc.) is canceled by the system. For more information, check STATEMENT_TIMEOUT_IN_SECONDS docs.
            strict_json_output: (Boolean) This parameter specifies whether JSON output in a session is compatible with the general standard (as described by http://json.org). By design, Snowflake allows JSON input that contains non-standard values; however, these non-standard values might result in Snowflake outputting JSON that is incompatible with other platforms and languages. This parameter, when enabled, ensures that Snowflake outputs valid/compatible JSON. For more information, check STRICT_JSON_OUTPUT docs.
            suspend_task_after_num_failures: (Number) Specifies the number of consecutive failed task runs after which the current task is suspended automatically. The default is 0 (no automatic suspension). For more information, check SUSPEND_TASK_AFTER_NUM_FAILURES docs.
            task_auto_retry_attempts: (Number) Specifies the number of automatic task graph retry attempts. If any task graphs complete in a FAILED state, Snowflake can automatically retry the task graphs from the last task in the graph that failed. For more information, check TASK_AUTO_RETRY_ATTEMPTS docs.
            task_relations: (List of Object) (see below for nested schema)
            time_input_format: (String) Specifies the input format for the TIME data type. For more information, see Date and time input and output formats. Any valid, supported time format or AUTO (AUTO specifies that Snowflake attempts to automatically detect the format of times stored in the system during the session). For more information, check TIME_INPUT_FORMAT docs.
            time_output_format: (String) Specifies the display format for the TIME data type. For more information, see Date and time input and output formats. For more information, check TIME_OUTPUT_FORMAT docs.
            timeouts: (Block, Optional) (see below for nested schema)
            timestamp_day_is_always_24h: (Boolean) Specifies whether the DATEADD function (and its aliases) always consider a day to be exactly 24 hours for expressions that span multiple days. For more information, check TIMESTAMP_DAY_IS_ALWAYS_24H docs.
            timestamp_input_format: (String) Specifies the input format for the TIMESTAMP data type alias. For more information, see Date and time input and output formats. Any valid, supported timestamp format or AUTO (AUTO specifies that Snowflake attempts to automatically detect the format of timestamps stored in the system during the session). For more information, check TIMESTAMP_INPUT_FORMAT docs.
            timestamp_ltz_output_format: (String) Specifies the display format for the TIMESTAMP_LTZ data type. If no format is specified, defaults to TIMESTAMP_OUTPUT_FORMAT. For more information, see Date and time input and output formats. For more information, check TIMESTAMP_LTZ_OUTPUT_FORMAT docs.
            timestamp_ntz_output_format: (String) Specifies the display format for the TIMESTAMP_NTZ data type. For more information, check TIMESTAMP_NTZ_OUTPUT_FORMAT docs.
            timestamp_output_format: (String) Specifies the display format for the TIMESTAMP data type alias. For more information, see Date and time input and output formats. For more information, check TIMESTAMP_OUTPUT_FORMAT docs.
            timestamp_type_mapping: (String) Specifies the TIMESTAMP_* variation that the TIMESTAMP data type alias maps to. For more information, check TIMESTAMP_TYPE_MAPPING docs.
            timestamp_tz_output_format: (String) Specifies the display format for the TIMESTAMP_TZ data type. If no format is specified, defaults to TIMESTAMP_OUTPUT_FORMAT. For more information, see Date and time input and output formats. For more information, check TIMESTAMP_TZ_OUTPUT_FORMAT docs.
            timezone: (String) Specifies the time zone for the session. You can specify a time zone name or a link name from release 2021a of the IANA Time Zone Database (e.g. America/Los_Angeles, Europe/London, UTC, Etc/GMT, etc.). For more information, check TIMEZONE docs.
            trace_level: (String) Controls how trace events are ingested into the event table. For more information about trace levels, see Setting trace level. For more information, check TRACE_LEVEL docs.
            transaction_abort_on_error: (Boolean) Specifies the action to perform when a statement issued within a non-autocommit transaction returns with an error. For more information, check TRANSACTION_ABORT_ON_ERROR docs.
            transaction_default_isolation_level: (String) Specifies the isolation level for transactions in the user session. For more information, check TRANSACTION_DEFAULT_ISOLATION_LEVEL docs.
            two_digit_century_start: (Number) Specifies the “century start” year for 2-digit years (i.e. the earliest year such dates can represent). This parameter prevents ambiguous dates when importing or converting data with the YY date format component (i.e. years represented as 2 digits). For more information, check TWO_DIGIT_CENTURY_START docs.
            unsupported_ddl_action: (String) Determines if an unsupported (i.e. non-default) value specified for a constraint property returns an error. For more information, check UNSUPPORTED_DDL_ACTION docs.
            update: (String)
            use_cached_result: (Boolean) Specifies whether to reuse persisted query results, if available, when a matching query is submitted. For more information, check USE_CACHED_RESULT docs.
            user_task_managed_initial_warehouse_size: '(String) Specifies the size of the compute resources to provision for the first run of the task, before a task history is available for Snowflake to determine an ideal size. Once a task has successfully completed a few runs, Snowflake ignores this parameter setting. Valid values are (case-insensitive): %s. (Conflicts with warehouse). For more information about warehouses, see docs. For more information, check USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE docs.'
            user_task_minimum_trigger_interval_in_seconds: (Number) Minimum amount of time between Triggered Task executions in seconds For more information, check USER_TASK_MINIMUM_TRIGGER_INTERVAL_IN_SECONDS docs.
            user_task_timeout_ms: (Number) Specifies the time limit on a single run of the task before it times out (in milliseconds). For more information, check USER_TASK_TIMEOUT_MS docs.
            using_cron: (String) Specifies a cron expression and time zone for periodically running the task. Supports a subset of standard cron utility syntax. (conflicts with minutes)
            value: (String)
            warehouse: (String) The warehouse the task will use. Omit this parameter to use Snowflake-managed compute resources for runs of this task. Due to Snowflake limitations warehouse identifier can consist of only upper-cased letters. (Conflicts with user_task_managed_initial_warehouse_size) For more information about this resource, see docs.
            week_of_year_policy: '(Number) Specifies how the weeks in a given year are computed. 0: The semantics used are equivalent to the ISO semantics, in which a week belongs to a given year if at least 4 days of that week are in that year. 1: January 1 is included in the first week of the year and December 31 is included in the last week of the year. For more information, check WEEK_OF_YEAR_POLICY docs.'
            week_start: '(Number) Specifies the first day of the week (used by week-related date functions). 0: Legacy Snowflake behavior is used (i.e. ISO-like semantics). 1 (Monday) to 7 (Sunday): All the week-related functions use weeks that start on the specified day of the week. For more information, check WEEK_START docs.'
            when: (String) Specifies a Boolean SQL expression; multiple conditions joined with AND/OR are supported. When a task is triggered (based on its SCHEDULE or AFTER setting), it validates the conditions of the expression to determine whether to execute. If the conditions of the expression are not met, then the task skips the current run. Any tasks that identify this task as a predecessor also don’t run.
        importStatements:
            - terraform import snowflake_task.example '"<database_name>"."<schema_name>"."<task_name>"'
    snowflake_user:
        subCategory: Stable
        description: Resource used to manage user objects. For more information, check user documentation https://docs.snowflake.com/en/sql-reference/commands-user-role#user-management.
        name: snowflake_user
        title: snowflake_user Resource - terraform-provider-snowflake
        examples:
            - name: minimal
              manifest: |-
                {
                  "name": "Snowflake User - minimal"
                }
            - name: user
              manifest: |-
                {
                  "comment": "User of snowflake.",
                  "days_to_expiry": 8,
                  "default_namespace": "some.namespace",
                  "default_role": "${snowflake_role.example.fully_qualified_name}",
                  "default_secondary_roles_option": "ALL",
                  "default_warehouse": "${snowflake_warehouse.example.fully_qualified_name}",
                  "disable_mfa": "false",
                  "disabled": "false",
                  "display_name": "Snowflake User display name",
                  "email": "${var.email}",
                  "first_name": "${var.first_name}",
                  "last_name": "${var.last_name}",
                  "login_name": "${var.login_name}",
                  "middle_name": "${var.middle_name}",
                  "mins_to_bypass_mfa": 10,
                  "mins_to_unlock": 9,
                  "must_change_password": "true",
                  "name": "Snowflake User",
                  "password": "${var.password}",
                  "rsa_public_key": "...",
                  "rsa_public_key_2": "..."
                }
              references:
                default_role: snowflake_role.example.fully_qualified_name
                default_warehouse: snowflake_warehouse.example.fully_qualified_name
                email: var.email
                first_name: var.first_name
                last_name: var.last_name
                login_name: var.login_name
                middle_name: var.middle_name
                password: var.password
            - name: u
              manifest: |-
                {
                  "abort_detached_query": true,
                  "autocommit": false,
                  "binary_input_format": "UTF8",
                  "binary_output_format": "BASE64",
                  "client_memory_limit": 1024,
                  "client_metadata_request_use_connection_ctx": true,
                  "client_prefetch_threads": 2,
                  "client_result_chunk_size": 48,
                  "client_result_column_case_insensitive": true,
                  "client_session_keep_alive": true,
                  "client_session_keep_alive_heartbeat_frequency": 2400,
                  "client_timestamp_type_mapping": "TIMESTAMP_NTZ",
                  "date_input_format": "YYYY-MM-DD",
                  "date_output_format": "YY-MM-DD",
                  "enable_unload_physical_type_optimization": false,
                  "enable_unredacted_query_syntax_error": true,
                  "error_on_nondeterministic_merge": false,
                  "error_on_nondeterministic_update": true,
                  "geography_output_format": "WKB",
                  "geometry_output_format": "WKB",
                  "jdbc_treat_decimal_as_int": false,
                  "jdbc_treat_timestamp_ntz_as_utc": true,
                  "jdbc_use_session_timezone": false,
                  "json_indent": 4,
                  "lock_timeout": 21222,
                  "log_level": "ERROR",
                  "multi_statement_count": 0,
                  "name": "Snowflake User with all parameters",
                  "network_policy": "BVYDGRAT_0D5E3DD1_F644_03DE_318A_1179886518A7",
                  "noorder_sequence_as_default": false,
                  "odbc_treat_decimal_as_int": true,
                  "prevent_unload_to_internal_stages": true,
                  "query_tag": "some_tag",
                  "quoted_identifiers_ignore_case": true,
                  "rows_per_resultset": 2,
                  "s3_stage_vpce_dns_name": "vpce-id.s3.region.vpce.amazonaws.com",
                  "search_path": "$public, $current",
                  "simulated_data_sharing_consumer": "some_consumer",
                  "statement_queued_timeout_in_seconds": 10,
                  "statement_timeout_in_seconds": 10,
                  "strict_json_output": true,
                  "time_input_format": "HH24:MI",
                  "time_output_format": "HH24:MI",
                  "timestamp_day_is_always_24h": true,
                  "timestamp_input_format": "YYYY-MM-DD",
                  "timestamp_ltz_output_format": "YYYY-MM-DD HH24:MI:SS",
                  "timestamp_ntz_output_format": "YYYY-MM-DD HH24:MI:SS",
                  "timestamp_output_format": "YYYY-MM-DD HH24:MI:SS",
                  "timestamp_type_mapping": "TIMESTAMP_LTZ",
                  "timestamp_tz_output_format": "YYYY-MM-DD HH24:MI:SS",
                  "timezone": "Europe/Warsaw",
                  "trace_level": "PROPAGATE",
                  "transaction_abort_on_error": true,
                  "transaction_default_isolation_level": "READ COMMITTED",
                  "two_digit_century_start": 1980,
                  "unsupported_ddl_action": "FAIL",
                  "use_cached_result": false,
                  "week_of_year_policy": 1,
                  "week_start": 1
                }
        argumentDocs:
            abort_detached_query: (Boolean) Specifies the action that Snowflake performs for in-progress queries if connectivity is lost due to abrupt termination of a session (e.g. network outage, browser termination, service interruption). For more information, check ABORT_DETACHED_QUERY docs.
            autocommit: (Boolean) Specifies whether autocommit is enabled for the session. Autocommit determines whether a DML statement, when executed without an active transaction, is automatically committed after the statement successfully completes. For more information, see Transactions. For more information, check AUTOCOMMIT docs.
            binary_input_format: (String) The format of VARCHAR values passed as input to VARCHAR-to-BINARY conversion functions. For more information, see Binary input and output. For more information, check BINARY_INPUT_FORMAT docs.
            binary_output_format: (String) The format for VARCHAR values returned as output by BINARY-to-VARCHAR conversion functions. For more information, see Binary input and output. For more information, check BINARY_OUTPUT_FORMAT docs.
            client_memory_limit: (Number) Parameter that specifies the maximum amount of memory the JDBC driver or ODBC driver should use for the result set from queries (in MB). For more information, check CLIENT_MEMORY_LIMIT docs.
            client_metadata_request_use_connection_ctx: (Boolean) For specific ODBC functions and JDBC methods, this parameter can change the default search scope from all databases/schemas to the current database/schema. The narrower search typically returns fewer rows and executes more quickly. For more information, check CLIENT_METADATA_REQUEST_USE_CONNECTION_CTX docs.
            client_prefetch_threads: (Number) Parameter that specifies the number of threads used by the client to pre-fetch large result sets. The driver will attempt to honor the parameter value, but defines the minimum and maximum values (depending on your system’s resources) to improve performance. For more information, check CLIENT_PREFETCH_THREADS docs.
            client_result_chunk_size: (Number) Parameter that specifies the maximum size of each set (or chunk) of query results to download (in MB). The JDBC driver downloads query results in chunks. For more information, check CLIENT_RESULT_CHUNK_SIZE docs.
            client_result_column_case_insensitive: (Boolean) Parameter that indicates whether to match column name case-insensitively in ResultSet.get* methods in JDBC. For more information, check CLIENT_RESULT_COLUMN_CASE_INSENSITIVE docs.
            client_session_keep_alive: (Boolean) Parameter that indicates whether to force a user to log in again after a period of inactivity in the session. For more information, check CLIENT_SESSION_KEEP_ALIVE docs.
            client_session_keep_alive_heartbeat_frequency: (Number) Number of seconds in-between client attempts to update the token for the session. For more information, check CLIENT_SESSION_KEEP_ALIVE_HEARTBEAT_FREQUENCY docs.
            client_timestamp_type_mapping: (String) Specifies the TIMESTAMP_* variation to use when binding timestamp variables for JDBC or ODBC applications that use the bind API to load data. For more information, check CLIENT_TIMESTAMP_TYPE_MAPPING docs.
            comment: (String) Specifies a comment for the user.
            create: (String)
            created_on: (String)
            date_input_format: (String) Specifies the input format for the DATE data type. For more information, see Date and time input and output formats. For more information, check DATE_INPUT_FORMAT docs.
            date_output_format: (String) Specifies the display format for the DATE data type. For more information, see Date and time input and output formats. For more information, check DATE_OUTPUT_FORMAT docs.
            days_to_expiry: (Number) Specifies the number of days after which the user status is set to Expired and the user is no longer allowed to log in. This is useful for defining temporary users (i.e. users who should only have access to Snowflake for a limited time period). In general, you should not set this property for account administrators (i.e. users with the ACCOUNTADMIN role) because Snowflake locks them out when they become Expired. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            default: (String)
            default_namespace: (String) Specifies the namespace (database only or database and schema) that is active by default for the user’s session upon login. Note that the CREATE USER operation does not verify that the namespace exists.
            default_role: (String) Specifies the role that is active by default for the user’s session upon login. Note that specifying a default role for a user does not grant the role to the user. The role must be granted explicitly to the user using the GRANT ROLE command. In addition, the CREATE USER operation does not verify that the role exists. For more information about this resource, see docs.
            default_secondary_roles: (String)
            default_secondary_roles_option: '(String) (Default: DEFAULT) Specifies the secondary roles that are active for the user’s session upon login. Valid values are (case-insensitive): DEFAULT | NONE | ALL. More information can be found in doc.'
            default_warehouse: (String) Specifies the virtual warehouse that is active by default for the user’s session upon login. Note that the CREATE USER operation does not verify that the warehouse exists. For more information about this resource, see docs.
            delete: (String)
            description: (String)
            disable_mfa: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Allows enabling or disabling multi-factor authentication. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value. External changes for this field won''t be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".'
            disabled: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies whether the user is disabled, which prevents logging in and aborts all the currently-running queries for the user. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            display_name: (String) Name displayed for the user in the Snowflake web interface.
            email: (String, Sensitive) Email address for the user.
            enable_unload_physical_type_optimization: (Boolean) Specifies whether to set the schema for unloaded Parquet files based on the logical column data types (i.e. the types in the unload SQL query or source table) or on the unloaded column values (i.e. the smallest data types and precision that support the values in the output columns of the unload SQL statement or source table). For more information, check ENABLE_UNLOAD_PHYSICAL_TYPE_OPTIMIZATION docs.
            enable_unredacted_query_syntax_error: (Boolean) Controls whether query text is redacted if a SQL query fails due to a syntax or parsing error. If FALSE, the content of a failed query is redacted in the views, pages, and functions that provide a query history. Only users with a role that is granted or inherits the AUDIT privilege can set the ENABLE_UNREDACTED_QUERY_SYNTAX_ERROR parameter. When using the ALTER USER command to set the parameter to TRUE for a particular user, modify the user that you want to see the query text, not the user who executed the query (if those are different users). For more information, check ENABLE_UNREDACTED_QUERY_SYNTAX_ERROR docs.
            error_on_nondeterministic_merge: (Boolean) Specifies whether to return an error when the MERGE command is used to update or delete a target row that joins multiple source rows and the system cannot determine the action to perform on the target row. For more information, check ERROR_ON_NONDETERMINISTIC_MERGE docs.
            error_on_nondeterministic_update: (Boolean) Specifies whether to return an error when the UPDATE command is used to update a target row that joins multiple source rows and the system cannot determine the action to perform on the target row. For more information, check ERROR_ON_NONDETERMINISTIC_UPDATE docs.
            expires_at_time: (String)
            ext_authn_duo: (Boolean)
            ext_authn_uid: (String)
            first_name: (String, Sensitive) First name of the user.
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            geography_output_format: (String) Display format for GEOGRAPHY values. For more information, check GEOGRAPHY_OUTPUT_FORMAT docs.
            geometry_output_format: (String) Display format for GEOMETRY values. For more information, check GEOMETRY_OUTPUT_FORMAT docs.
            has_mfa: (Boolean)
            has_password: (Boolean)
            has_rsa_public_key: (Boolean)
            id: (String) The ID of this resource.
            jdbc_treat_decimal_as_int: (Boolean) Specifies how JDBC processes columns that have a scale of zero (0). For more information, check JDBC_TREAT_DECIMAL_AS_INT docs.
            jdbc_treat_timestamp_ntz_as_utc: (Boolean) Specifies how JDBC processes TIMESTAMP_NTZ values. For more information, check JDBC_TREAT_TIMESTAMP_NTZ_AS_UTC docs.
            jdbc_use_session_timezone: (Boolean) Specifies whether the JDBC Driver uses the time zone of the JVM or the time zone of the session (specified by the TIMEZONE parameter) for the getDate(), getTime(), and getTimestamp() methods of the ResultSet class. For more information, check JDBC_USE_SESSION_TIMEZONE docs.
            json_indent: (Number) Specifies the number of blank spaces to indent each new element in JSON output in the session. Also specifies whether to insert newline characters after each element. For more information, check JSON_INDENT docs.
            key: (String)
            last_name: (String, Sensitive) Last name of the user.
            last_success_login: (String)
            level: (String)
            lock_timeout: (Number) Number of seconds to wait while trying to lock a resource, before timing out and aborting the statement. For more information, check LOCK_TIMEOUT docs.
            locked_until_time: (String)
            log_level: (String) Specifies the severity level of messages that should be ingested and made available in the active event table. Messages at the specified level (and at more severe levels) are ingested. For more information about log levels, see Setting log level. For more information, check LOG_LEVEL docs.
            login_name: (String, Sensitive) The name users use to log in. If not supplied, snowflake will use name instead. Login names are always case-insensitive.
            middle_name: (String, Sensitive) Middle name of the user.
            mins_to_bypass_mfa: '(Number) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (-1)) Specifies the number of minutes to temporarily bypass MFA for the user. This property can be used to allow a MFA-enrolled user to temporarily bypass MFA during login in the event that their MFA device is not available. External changes for this field won''t be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".'
            mins_to_unlock: '(Number) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (-1)) Specifies the number of minutes until the temporary lock on the user login is cleared. To protect against unauthorized user login, Snowflake places a temporary lock on a user after five consecutive unsuccessful login attempts. When creating a user, this property can be set to prevent them from logging in until the specified amount of time passes. To remove a lock immediately for a user, specify a value of 0 for this parameter. Note because this value changes continuously after setting it, the provider is currently NOT handling the external changes to it. External changes for this field won''t be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".'
            multi_statement_count: (Number) Number of statements to execute when using the multi-statement capability. For more information, check MULTI_STATEMENT_COUNT docs.
            must_change_password: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies whether the user is forced to change their password on next login (including their first/initial login) into the system. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            name: '(String) Name of the user. Note that if you do not supply login_name this will be used as login_name. Check the docs. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            network_policy: (String) Specifies the network policy to enforce for your account. Network policies enable restricting access to your account based on users’ IP address. For more details, see Controlling network traffic with network policies. Any existing network policy (created using CREATE NETWORK POLICY). For more information, check NETWORK_POLICY docs.
            noorder_sequence_as_default: (Boolean) Specifies whether the ORDER or NOORDER property is set by default when you create a new sequence or add a new table column. The ORDER and NOORDER properties determine whether or not the values are generated for the sequence or auto-incremented column in increasing or decreasing order. For more information, check NOORDER_SEQUENCE_AS_DEFAULT docs.
            odbc_treat_decimal_as_int: (Boolean) Specifies how ODBC processes columns that have a scale of zero (0). For more information, check ODBC_TREAT_DECIMAL_AS_INT docs.
            owner: (String)
            parameters: (List of Object) Outputs the result of SHOW PARAMETERS IN USER for the given user. (see below for nested schema)
            password: '(String, Sensitive) Password for the user. WARNING: this will put the password in the terraform state file. Use carefully. External changes for this field won''t be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".'
            prevent_unload_to_internal_stages: (Boolean) Specifies whether to prevent data unload operations to internal (Snowflake) stages using COPY INTO  statements. For more information, check PREVENT_UNLOAD_TO_INTERNAL_STAGES docs.
            query_tag: (String) Optional string that can be used to tag queries and other SQL statements executed within a session. The tags are displayed in the output of the QUERY_HISTORY, QUERY_HISTORY_BY_* functions. For more information, check QUERY_TAG docs.
            quoted_identifiers_ignore_case: (Boolean) Specifies whether letters in double-quoted object identifiers are stored and resolved as uppercase letters. By default, Snowflake preserves the case of alphabetic characters when storing and resolving double-quoted identifiers (see Identifier resolution). You can use this parameter in situations in which third-party applications always use double quotes around identifiers. For more information, check QUOTED_IDENTIFIERS_IGNORE_CASE docs.
            read: (String)
            rows_per_resultset: (Number) Specifies the maximum number of rows returned in a result set. A value of 0 specifies no maximum. For more information, check ROWS_PER_RESULTSET docs.
            rsa_public_key: (String) Specifies the user’s RSA public key; used for key-pair authentication. Must be on 1 line without header and trailer.
            rsa_public_key_2: (String) Specifies the user’s second RSA public key; used to rotate the public and private keys for key-pair authentication based on an expiration schedule set by your organization. Must be on 1 line without header and trailer.
            s3_stage_vpce_dns_name: (String) Specifies the DNS name of an Amazon S3 interface endpoint. Requests sent to the internal stage of an account via AWS PrivateLink for Amazon S3 use this endpoint to connect. For more information, see Accessing Internal stages with dedicated interface endpoints. For more information, check S3_STAGE_VPCE_DNS_NAME docs.
            search_path: (String) Specifies the path to search to resolve unqualified object names in queries. For more information, see Name resolution in queries. Comma-separated list of identifiers. An identifier can be a fully or partially qualified schema name. For more information, check SEARCH_PATH docs.
            show_output: (List of Object) Outputs the result of SHOW USER for the given user. (see below for nested schema)
            simulated_data_sharing_consumer: (String) Specifies the name of a consumer account to simulate for testing/validating shared data, particularly shared secure views. When this parameter is set in a session, shared views return rows as if executed in the specified consumer account rather than the provider account. For more information, see Introduction to Secure Data Sharing and Working with shares. For more information, check SIMULATED_DATA_SHARING_CONSUMER docs.
            snowflake_lock: (Boolean)
            statement_queued_timeout_in_seconds: (Number) Amount of time, in seconds, a SQL statement (query, DDL, DML, etc.) remains queued for a warehouse before it is canceled by the system. This parameter can be used in conjunction with the MAX_CONCURRENCY_LEVEL parameter to ensure a warehouse is never backlogged. For more information, check STATEMENT_QUEUED_TIMEOUT_IN_SECONDS docs.
            statement_timeout_in_seconds: (Number) Amount of time, in seconds, after which a running SQL statement (query, DDL, DML, etc.) is canceled by the system. For more information, check STATEMENT_TIMEOUT_IN_SECONDS docs.
            strict_json_output: (Boolean) This parameter specifies whether JSON output in a session is compatible with the general standard (as described by http://json.org). By design, Snowflake allows JSON input that contains non-standard values; however, these non-standard values might result in Snowflake outputting JSON that is incompatible with other platforms and languages. This parameter, when enabled, ensures that Snowflake outputs valid/compatible JSON. For more information, check STRICT_JSON_OUTPUT docs.
            time_input_format: (String) Specifies the input format for the TIME data type. For more information, see Date and time input and output formats. Any valid, supported time format or AUTO (AUTO specifies that Snowflake attempts to automatically detect the format of times stored in the system during the session). For more information, check TIME_INPUT_FORMAT docs.
            time_output_format: (String) Specifies the display format for the TIME data type. For more information, see Date and time input and output formats. For more information, check TIME_OUTPUT_FORMAT docs.
            timeouts: (Block, Optional) (see below for nested schema)
            timestamp_day_is_always_24h: (Boolean) Specifies whether the DATEADD function (and its aliases) always consider a day to be exactly 24 hours for expressions that span multiple days. For more information, check TIMESTAMP_DAY_IS_ALWAYS_24H docs.
            timestamp_input_format: (String) Specifies the input format for the TIMESTAMP data type alias. For more information, see Date and time input and output formats. Any valid, supported timestamp format or AUTO (AUTO specifies that Snowflake attempts to automatically detect the format of timestamps stored in the system during the session). For more information, check TIMESTAMP_INPUT_FORMAT docs.
            timestamp_ltz_output_format: (String) Specifies the display format for the TIMESTAMP_LTZ data type. If no format is specified, defaults to TIMESTAMP_OUTPUT_FORMAT. For more information, see Date and time input and output formats. For more information, check TIMESTAMP_LTZ_OUTPUT_FORMAT docs.
            timestamp_ntz_output_format: (String) Specifies the display format for the TIMESTAMP_NTZ data type. For more information, check TIMESTAMP_NTZ_OUTPUT_FORMAT docs.
            timestamp_output_format: (String) Specifies the display format for the TIMESTAMP data type alias. For more information, see Date and time input and output formats. For more information, check TIMESTAMP_OUTPUT_FORMAT docs.
            timestamp_type_mapping: (String) Specifies the TIMESTAMP_* variation that the TIMESTAMP data type alias maps to. For more information, check TIMESTAMP_TYPE_MAPPING docs.
            timestamp_tz_output_format: (String) Specifies the display format for the TIMESTAMP_TZ data type. If no format is specified, defaults to TIMESTAMP_OUTPUT_FORMAT. For more information, see Date and time input and output formats. For more information, check TIMESTAMP_TZ_OUTPUT_FORMAT docs.
            timezone: (String) Specifies the time zone for the session. You can specify a time zone name or a link name from release 2021a of the IANA Time Zone Database (e.g. America/Los_Angeles, Europe/London, UTC, Etc/GMT, etc.). For more information, check TIMEZONE docs.
            trace_level: (String) Controls how trace events are ingested into the event table. For more information about trace levels, see Setting trace level. For more information, check TRACE_LEVEL docs.
            transaction_abort_on_error: (Boolean) Specifies the action to perform when a statement issued within a non-autocommit transaction returns with an error. For more information, check TRANSACTION_ABORT_ON_ERROR docs.
            transaction_default_isolation_level: (String) Specifies the isolation level for transactions in the user session. For more information, check TRANSACTION_DEFAULT_ISOLATION_LEVEL docs.
            two_digit_century_start: (Number) Specifies the “century start” year for 2-digit years (i.e. the earliest year such dates can represent). This parameter prevents ambiguous dates when importing or converting data with the YY date format component (i.e. years represented as 2 digits). For more information, check TWO_DIGIT_CENTURY_START docs.
            type: (String)
            unsupported_ddl_action: (String) Determines if an unsupported (i.e. non-default) value specified for a constraint property returns an error. For more information, check UNSUPPORTED_DDL_ACTION docs.
            update: (String)
            use_cached_result: (Boolean) Specifies whether to reuse persisted query results, if available, when a matching query is submitted. For more information, check USE_CACHED_RESULT docs.
            user_type: (String) Specifies a type for the user.
            value: (String)
            week_of_year_policy: '(Number) Specifies how the weeks in a given year are computed. 0: The semantics used are equivalent to the ISO semantics, in which a week belongs to a given year if at least 4 days of that week are in that year. 1: January 1 is included in the first week of the year and December 31 is included in the last week of the year. For more information, check WEEK_OF_YEAR_POLICY docs.'
            week_start: '(Number) Specifies the first day of the week (used by week-related date functions). 0: Legacy Snowflake behavior is used (i.e. ISO-like semantics). 1 (Monday) to 7 (Sunday): All the week-related functions use weeks that start on the specified day of the week. For more information, check WEEK_START docs.'
        importStatements:
            - terraform import snowflake_user.example '"<user_name>"'
    snowflake_user_authentication_policy_attachment:
        subCategory: Preview
        description: Specifies the authentication policy to use for a certain user.
        name: snowflake_user_authentication_policy_attachment
        title: snowflake_user_authentication_policy_attachment Resource - terraform-provider-snowflake
        examples:
            - name: apa
              manifest: |-
                {
                  "authentication_policy_name": "${snowflake_authentication_policy.ap.fully_qualified_name}",
                  "user_name": "${snowflake_user.user.name}"
                }
              references:
                authentication_policy_name: snowflake_authentication_policy.ap.fully_qualified_name
                user_name: snowflake_user.user.name
              dependencies:
                snowflake_authentication_policy.ap: |-
                    {
                      "database": "prod",
                      "name": "default_policy",
                      "schema": "security"
                    }
                snowflake_user.user: |-
                    {
                      "name": "USER_NAME"
                    }
        argumentDocs:
            authentication_policy_name: (String) Fully qualified name of the authentication policy
            create: (String)
            delete: (String)
            id: (String) The ID of this resource.
            read: (String)
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
            user_name: (String) User name of the user you want to attach the authentication policy to
        importStatements: []
    snowflake_user_password_policy_attachment:
        subCategory: Preview
        description: Specifies the password policy to use for a certain user.
        name: snowflake_user_password_policy_attachment
        title: snowflake_user_password_policy_attachment Resource - terraform-provider-snowflake
        examples:
            - name: ppa
              manifest: |-
                {
                  "password_policy_name": "${snowflake_password_policy.pp.fully_qualified_name}",
                  "user_name": "${snowflake_user.user.name}"
                }
              references:
                password_policy_name: snowflake_password_policy.pp.fully_qualified_name
                user_name: snowflake_user.user.name
              dependencies:
                snowflake_password_policy.pp: |-
                    {
                      "database": "prod",
                      "name": "default_policy",
                      "schema": "security"
                    }
                snowflake_user.user: |-
                    {
                      "name": "USER_NAME"
                    }
        argumentDocs:
            create: (String)
            delete: (String)
            id: (String) The ID of this resource.
            password_policy_name: (String) Fully qualified name of the password policy
            read: (String)
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
            user_name: (String) User name of the user you want to attach the password policy to
        importStatements:
            - terraform import snowflake_user_password_policy_attachment.example "MY_DATABASE|MY_SCHEMA|PASSWORD_POLICY_NAME|USER_NAME"
    snowflake_user_programmatic_access_token:
        subCategory: Preview
        description: Resource used to manage user programmatic access tokens. For more information, check user programmatic access tokens documentation https://docs.snowflake.com/en/sql-reference/sql/alter-user-add-programmatic-access-token. A programmatic access token is a token that can be used to authenticate to an endpoint. See Using programmatic access tokens for authentication https://docs.snowflake.com/en/user-guide/programmatic-access-tokens user guide for more details.
        name: snowflake_user_programmatic_access_token
        title: snowflake_user_programmatic_access_token Resource - terraform-provider-snowflake
        examples:
            - name: basic
              manifest: |-
                {
                  "name": "TOKEN",
                  "user": "USER"
                }
              dependencies:
                snowflake_account_role.role: |-
                    {
                      "name": "ROLE"
                    }
                snowflake_grant_account_role.grant_role_to_user: |-
                    {
                      "role_name": "${snowflake_account_role.role.name}",
                      "user_name": "${snowflake_user.user.name}"
                    }
                snowflake_user.user: |-
                    {
                      "name": "USER"
                    }
                time_rotating.rotation_schedule: |-
                    {
                      "rotation_days": 30
                    }
            - name: complete
              manifest: |-
                {
                  "comment": "COMMENT",
                  "days_to_expiry": 30,
                  "disabled": false,
                  "mins_to_bypass_network_policy_requirement": 10,
                  "name": "TOKEN",
                  "role_restriction": "ROLE",
                  "user": "USER"
                }
              dependencies:
                snowflake_account_role.role: |-
                    {
                      "name": "ROLE"
                    }
                snowflake_grant_account_role.grant_role_to_user: |-
                    {
                      "role_name": "${snowflake_account_role.role.name}",
                      "user_name": "${snowflake_user.user.name}"
                    }
                snowflake_user.user: |-
                    {
                      "name": "USER"
                    }
                time_rotating.rotation_schedule: |-
                    {
                      "rotation_days": 30
                    }
            - name: complete_with_external_references
              manifest: |-
                {
                  "comment": "COMMENT",
                  "days_to_expiry": 30,
                  "disabled": false,
                  "mins_to_bypass_network_policy_requirement": 10,
                  "name": "TOKEN",
                  "role_restriction": "${snowflake_account_role.role.name}",
                  "user": "${snowflake_user.user.name}"
                }
              references:
                role_restriction: snowflake_account_role.role.name
                user: snowflake_user.user.name
              dependencies:
                snowflake_account_role.role: |-
                    {
                      "name": "ROLE"
                    }
                snowflake_grant_account_role.grant_role_to_user: |-
                    {
                      "role_name": "${snowflake_account_role.role.name}",
                      "user_name": "${snowflake_user.user.name}"
                    }
                snowflake_user.user: |-
                    {
                      "name": "USER"
                    }
                time_rotating.rotation_schedule: |-
                    {
                      "rotation_days": 30
                    }
            - name: rotating
              manifest: |-
                {
                  "keeper": "${time_rotating.rotation_schedule.rotation_rfc3339}",
                  "name": "TOKEN",
                  "user": "USER"
                }
              references:
                keeper: time_rotating.rotation_schedule.rotation_rfc3339
              dependencies:
                snowflake_account_role.role: |-
                    {
                      "name": "ROLE"
                    }
                snowflake_grant_account_role.grant_role_to_user: |-
                    {
                      "role_name": "${snowflake_account_role.role.name}",
                      "user_name": "${snowflake_user.user.name}"
                    }
                snowflake_user.user: |-
                    {
                      "name": "USER"
                    }
                time_rotating.rotation_schedule: |-
                    {
                      "rotation_days": 30
                    }
        argumentDocs:
            comment: (String) Descriptive comment about the programmatic access token.
            create: (String)
            created_by: (String)
            created_on: (String)
            days_to_expiry: (Number) The number of days that the programmatic access token can be used for authentication. This field cannot be altered after the token is created. Instead, you must rotate the token with the keeper field. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            delete: (String)
            disabled: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Disables or enables the programmatic access token. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            expire_rotated_token_after_hours: '(Number) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (-1)) This field is only used when the token is rotated by changing the keeper field. Sets the expiration time of the existing token secret to expire after the specified number of hours. You can set this to a value of 0 to expire the current token secret immediately.'
            expires_at: (String)
            id: (String) The ID of this resource.
            keeper: (String) Arbitrary string that, if and only if, changed from a non-empty to a different non-empty value (or known after apply), will trigger a key to be rotated. When you add this field to the configuration, or remove it from the configuration, the rotation is not triggered. When the token is rotated, the token and rotated_token_name fields are marked as computed.
            mins_to_bypass_network_policy_requirement: (Number) The number of minutes during which a user can use this token to access Snowflake without being subject to an active network policy. External changes for this field won't be detected. In case you want to apply external changes, you can re-create the resource manually using "terraform taint".
            name: '(String) Specifies the name for the programmatic access token; must be unique for the user. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            read: (String)
            role_restriction: '(String) The name of the role used for privilege evaluation and object creation. This must be one of the roles that has already been granted to the user. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            rotated_to: (String)
            rotated_token_name: (String) Name of the token that represents the prior secret. This field is updated only when the token is rotated. In this case, the field is marked as computed.
            show_output: (List of Object) Outputs the result of SHOW USER PROGRAMMATIC ACCESS TOKENS for the given user programmatic access token. (see below for nested schema)
            status: (String)
            timeouts: (Block, Optional) (see below for nested schema)
            token: (String, Sensitive) The token itself. Use this to authenticate to an endpoint. The data in this field is updated only when the token is created or rotated. In this case, the field is marked as computed.
            update: (String)
            user: '(String) The name of the user that the token is associated with. A user cannot use another user''s programmatic access token to authenticate. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            user_name: (String)
        importStatements:
            - terraform import snowflake_user_programmatic_access_token.example '"<user_name>"|"<token_name>"'
    snowflake_user_public_keys Resource - terraform-provider-snowflake:
        subCategory: Preview
        name: snowflake_user_public_keys Resource - terraform-provider-snowflake
        title: snowflake_user_public_keys Resource - terraform-provider-snowflake
        argumentDocs:
            create: (String)
            delete: (String)
            id: (String) The ID of this resource.
            name: (String) Name of the user.
            read: (String)
            rsa_public_key: (String) Specifies the user’s RSA public key; used for key-pair authentication. Must be on 1 line without header and trailer.
            rsa_public_key_2: (String) Specifies the user’s second RSA public key; used to rotate the public and Public keys for key-pair authentication based on an expiration schedule set by your organization. Must be on 1 line without header and trailer.
            timeouts: (Block, Optional) (see below for nested schema)
            update: (String)
        importStatements: []
    snowflake_view:
        subCategory: Stable
        description: Resource used to manage view objects. For more information, check view documentation https://docs.snowflake.com/en/sql-reference/sql/create-view.
        name: snowflake_view
        title: snowflake_view Resource - terraform-provider-snowflake
        examples:
            - name: view
              manifest: |-
                {
                  "database": "database",
                  "name": "view",
                  "schema": "schema",
                  "statement": "select * from foo;\n"
                }
            - name: view
              manifest: |-
                {
                  "database": "database",
                  "is_recursive": "true",
                  "name": "view",
                  "schema": "schema",
                  "statement": "select * from foo;\n"
                }
            - name: test
              manifest: |-
                {
                  "aggregation_policy": [
                    {
                      "entity_key": [
                        "id"
                      ],
                      "policy_name": "aggregation_policy"
                    }
                  ],
                  "change_tracking": "true",
                  "column": [
                    {
                      "column_name": "id",
                      "comment": "column comment"
                    },
                    {
                      "column_name": "address",
                      "masking_policy": [
                        {
                          "policy_name": "${snowflake_masking_policy.example.fully_qualified_name}",
                          "using": [
                            "address"
                          ]
                        }
                      ],
                      "projection_policy": [
                        {
                          "policy_name": "projection_policy"
                        }
                      ]
                    }
                  ],
                  "comment": "comment",
                  "data_metric_function": [
                    {
                      "function_name": "data_metric_function",
                      "on": [
                        "id"
                      ],
                      "schedule_status": "STARTED"
                    }
                  ],
                  "data_metric_schedule": [
                    {
                      "using_cron": "15 * * * * UTC"
                    }
                  ],
                  "database": "database",
                  "is_secure": "true",
                  "is_temporary": "true",
                  "name": "view",
                  "row_access_policy": [
                    {
                      "on": [
                        "id"
                      ],
                      "policy_name": "${snowflake_row_access_policy.example.fully_qualified_name}"
                    }
                  ],
                  "schema": "schema",
                  "statement": "SELECT id, address FROM TABLE;\n"
                }
              references:
                column.masking_policy.policy_name: snowflake_masking_policy.example.fully_qualified_name
                row_access_policy.policy_name: snowflake_row_access_policy.example.fully_qualified_name
        argumentDocs:
            aggregation_policy: '(Block List, Max: 1) Specifies the aggregation policy to set on a view. (see below for nested schema)'
            change_tracking: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies to enable or disable change tracking on the table. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            check: (String)
            column: (Block List) If you want to change the name of a column or add a comment to a column in the new view, include a column list that specifies the column names and (if needed) comments about the columns. You do not need to specify the data types of the columns. If this field is not specified, columns are inferred from the statement field by Snowflake. (see below for nested schema)
            column_name: (String) Specifies affected column name.
            comment: (String) Specifies a comment for the view.
            copy_grants: '(Boolean) (Default: false) Retains the access permissions from the original view when a view is recreated using the OR REPLACE clause. This is used when the provider detects changes for fields that can not be changed by ALTER. This value will not have any effect during creating a new object with Terraform.'
            create: (String)
            created_on: (String)
            data_metric_function: (Block Set) Data metric functions used for the view. (see below for nested schema)
            data_metric_schedule: '(Block List, Max: 1) Specifies the schedule to run the data metric functions periodically. (see below for nested schema)'
            database: '(String) The database in which to create the view. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            database_name: (String)
            default: (String)
            delete: (String)
            describe_output: (List of Object) Outputs the result of DESCRIBE VIEW for the given view. (see below for nested schema)
            entity_key: (Set of String) Defines which columns uniquely identify an entity within the view.
            expression: (String)
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            function_name: (String) Identifier of the data metric function to add to the table or view or drop from the table or view. This function identifier must be provided without arguments in parenthesis.
            id: (String) The ID of this resource.
            is_materialized: (Boolean)
            is_nullable: (Boolean)
            is_primary: (Boolean)
            is_recursive: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies that the view can refer to itself using recursive syntax without necessarily using a CTE (common table expression). Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            is_secure: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies that the view is secure. By design, the Snowflake''s SHOW VIEWS command does not provide information about secure views (consult view usage notes) which is essential to manage/import view with Terraform. Use the role owning the view while managing secure views. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            is_temporary: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies that the view persists only for the duration of the session that you created it in. A temporary view and all its contents are dropped at the end of the session. In context of this provider, it means that it''s dropped after a Terraform operation. This results in a permanent plan with object creation. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            is_unique: (Boolean)
            kind: (String)
            masking_policy: '(Block List, Max: 1) (see below for nested schema)'
            minutes: '(Number) Specifies an interval (in minutes) of wait time inserted between runs of the data metric function. Conflicts with using_cron. Valid values are: 5 | 15 | 30 | 60 | 720 | 1440. Due to Snowflake limitations, changes in this field are not managed by the provider. Please consider using taint command, using_cron field, or replace_triggered_by metadata argument.'
            name: '(String) Specifies the identifier for the view; must be unique for the schema in which the view is created. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            "on": (Set of String) The table or view columns on which to associate the data metric function. The data types of the columns must match the data types of the columns specified in the data metric function definition.
            owner: (String)
            owner_role_type: (String)
            policy_name: (String) Aggregation policy name.
            privacy_domain: (String)
            projection_policy: '(Block List, Max: 1) (see below for nested schema)'
            read: (String)
            reserved: (String)
            row_access_policy: '(Block List, Max: 1) Specifies the row access policy to set on a view. (see below for nested schema)'
            schedule_status: '(String) The status of the metrics association. Valid values are: STARTED | SUSPENDED. When status of a data metric function is changed, it is being reassigned with DROP DATA METRIC FUNCTION and ADD DATA METRIC FUNCTION, and then its status is changed by MODIFY DATA METRIC FUNCTION'
            schema: '(String) The schema in which to create the view. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            schema_name: (String)
            show_output: (List of Object) Outputs the result of SHOW VIEW for the given view. (see below for nested schema)
            statement: (String) Specifies the query used to create the view. To mitigate permadiff on this field, the provider replaces blank characters with a space. This can lead to false positives in cases where a change in case or run of whitespace is semantically significant.
            text: (String)
            timeouts: (Block, Optional) (see below for nested schema)
            type: (String)
            update: (String)
            using: (List of String) Specifies the arguments to pass into the conditional masking policy SQL expression. The first column in the list specifies the column for the policy conditions to mask or tokenize the data and must match the column to which the masking policy is set. The additional columns specify the columns to evaluate to determine whether to mask or tokenize the data in each row of the query result when a query is made on the first column. If the USING clause is omitted, Snowflake treats the conditional masking policy as a normal masking policy.
            using_cron: (String) Specifies a cron expression and time zone for periodically running the data metric function. Supports a subset of standard cron utility syntax. Conflicts with minutes.
        importStatements:
            - terraform import snowflake_view.example '"<database_name>"."<schema_name>"."<view_name>"'
    snowflake_warehouse:
        subCategory: Stable
        description: Resource used to manage warehouse objects. For more information, check warehouse documentation https://docs.snowflake.com/en/sql-reference/commands-warehouse.
        name: snowflake_warehouse
        title: snowflake_warehouse Resource - terraform-provider-snowflake
        examples:
            - name: warehouse
              manifest: |-
                {
                  "name": "WAREHOUSE"
                }
            - name: warehouse
              manifest: |-
                {
                  "auto_resume": false,
                  "auto_suspend": 1200,
                  "comment": "An example warehouse.",
                  "enable_query_acceleration": true,
                  "initially_suspended": false,
                  "max_cluster_count": 4,
                  "max_concurrency_level": 4,
                  "min_cluster_count": 2,
                  "name": "WAREHOUSE",
                  "query_acceleration_max_scale_factor": 4,
                  "resource_constraint": "MEMORY_16X",
                  "resource_monitor": "${snowflake_resource_monitor.monitor.fully_qualified_name}",
                  "scaling_policy": "ECONOMY",
                  "statement_queued_timeout_in_seconds": 5,
                  "statement_timeout_in_seconds": 86400,
                  "warehouse_size": "MEDIUM",
                  "warehouse_type": "SNOWPARK-OPTIMIZED"
                }
              references:
                resource_monitor: snowflake_resource_monitor.monitor.fully_qualified_name
            - name: warehouse
              manifest: |-
                {
                  "generation": "2",
                  "name": "WAREHOUSE",
                  "warehouse_size": "MEDIUM",
                  "warehouse_type": "STANDARD"
                }
        argumentDocs:
            auto_resume: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies whether to automatically resume a warehouse when a SQL statement (e.g. query) is submitted to it. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            auto_suspend: '(Number) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (-1)) Specifies the number of seconds of inactivity after which a warehouse is automatically suspended.'
            available: (Number)
            comment: (String) Specifies a comment for the warehouse.
            create: (String)
            created_on: (String)
            default: (String)
            delete: (String)
            description: (String)
            enable_query_acceleration: '(String) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (default)) Specifies whether to enable the query acceleration service for queries that rely on this warehouse for compute resources. Available options are: "true" or "false". When the value is not set in the configuration the provider will put "default" there which means to use the Snowflake default for this value.'
            fully_qualified_name: (String) Fully qualified name of the resource. For more information, see object name resolution.
            generation: '(String) Specifies the generation for the warehouse. Only available for standard warehouses. Valid values are (case-insensitive): 1 | 2.'
            id: (String) The ID of this resource.
            initially_suspended: (Boolean) Specifies whether the warehouse is created initially in the ‘Suspended’ state.
            is_current: (Boolean)
            is_default: (Boolean)
            key: (String)
            level: (String)
            max_cluster_count: (Number) Specifies the maximum number of server clusters for the warehouse.
            max_concurrency_level: (Number) Object parameter that specifies the concurrency level for SQL statements (i.e. queries and DML) executed by a warehouse.
            min_cluster_count: (Number) Specifies the minimum number of server clusters for the warehouse (only applies to multi-cluster warehouses).
            name: '(String) Identifier for the virtual warehouse; must be unique for your account. Due to technical limitations (read more here), avoid using the following characters: |, ., ".'
            other: (Number)
            owner: (String)
            owner_role_type: (String)
            parameters: (List of Object) Outputs the result of SHOW PARAMETERS IN WAREHOUSE for the given warehouse. (see below for nested schema)
            provisioning: (Number)
            query_acceleration_max_scale_factor: '(Number) (Default: fallback to Snowflake default - uses special value that cannot be set in the configuration manually (-1)) Specifies the maximum scale factor for leasing compute resources for query acceleration. The scale factor is used as a multiplier based on warehouse size.'
            queued: (Number)
            quiescing: (Number)
            read: (String)
            resource_constraint: '(String) Specifies the resource constraint for the warehouse. Only available for snowpark-optimized warehouses. For setting generation please use the generation field. Please check Snowflake documentation for required warehouse sizes for each resource constraint. Valid values are (case-insensitive): MEMORY_1X | MEMORY_1X_x86 | MEMORY_16X | MEMORY_16X_x86 | MEMORY_64X | MEMORY_64X_x86.'
            resource_monitor: (String) Specifies the name of a resource monitor that is explicitly assigned to the warehouse. For more information about this resource, see docs.
            resumed_on: (String)
            running: (Number)
            scaling_policy: '(String) Specifies the policy for automatically starting and shutting down clusters in a multi-cluster warehouse running in Auto-scale mode. Valid values are (case-insensitive): STANDARD | ECONOMY.'
            show_output: (List of Object) Outputs the result of SHOW WAREHOUSES for the given warehouse. (see below for nested schema)
            size: (String)
            started_clusters: (Number)
            state: (String)
            statement_queued_timeout_in_seconds: (Number) Object parameter that specifies the time, in seconds, a SQL statement (query, DDL, DML, etc.) can be queued on a warehouse before it is canceled by the system.
            statement_timeout_in_seconds: (Number) Specifies the time, in seconds, after which a running SQL statement (query, DDL, DML, etc.) is canceled by the system
            timeouts: (Block, Optional) (see below for nested schema)
            type: (String)
            update: (String)
            updated_on: (String)
            value: (String)
            warehouse_size: '(String) Specifies the size of the virtual warehouse. Valid values are (case-insensitive): XSMALL | X-SMALL | SMALL | MEDIUM | LARGE | XLARGE | X-LARGE | XXLARGE | X2LARGE | 2X-LARGE | XXXLARGE | X3LARGE | 3X-LARGE | X4LARGE | 4X-LARGE | X5LARGE | 5X-LARGE | X6LARGE | 6X-LARGE. Consult warehouse documentation for the details. Note: removing the size from config will result in the resource recreation.'
            warehouse_type: '(String) Specifies warehouse type. Valid values are (case-insensitive): STANDARD | SNOWPARK-OPTIMIZED. Warehouse needs to be suspended to change its type. Provider will handle automatic suspension and resumption if needed.'
        importStatements:
            - terraform import snowflake_warehouse.example '"<warehouse_name>"'
